[
    {
        "instruction": "Dependencies in Python file: `py2dataset.save_output.py`?",
        "input": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = (\n            100 / column_count\n        )  # Calculate the width for each column based on the number of columns\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []  # List to store each row's HTML content\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))  # Join all parts of the row and append to the list\n\n        # After the loop, join all rows\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef convert_json_to_markdown(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to Markdown format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def escape_markdown(text: str) -> str:\n        \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n        markdown_special_chars = \"\\\\`*_{}[]()#+-.!\"\n        for char in markdown_special_chars:\n            text = text.replace(char, f\"\\\\{char}\")\n        return text\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        markdown_content = \"# Data Report\\n\\n\"\n        # Create Markdown table headers\n        headers = dataset[0].keys()\n        markdown_content += \"| \" + \" | \".join(headers) + \" |\\n\"\n        markdown_content += \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n\n        # Create Markdown table rows\n        for entry in dataset:\n            row = \"| \" + \" | \".join(escape_markdown(str(entry[key])) for key in headers) + \" |\\n\"\n            markdown_content += row\n\n        markdown_file_path = json_file.with_suffix(\".md\")\n        try:\n            with open(markdown_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(markdown_content)\n        except Exception as e:\n            logging.error(f\"Failed to save Markdown file {markdown_file_path}: {e}\")\n            \n\ndef combine_json_files(directory: str, html: bool = False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    def remove_duplicate_dataset_entries(\n        dataset: List[Dict], key1: str, key2: str\n    ) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n\n    instruct_data = []\n    for file_name in [\"instruct.json\"]:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f\"*.{file_name}\"):\n            try:\n                json_file_data = read_file(json_file)\n                combined_data.extend(json_file_data)\n                combined_data = remove_duplicate_dataset_entries(\n                    combined_data, \"instruction\", \"output\"\n                )\n                instruct_data = combined_data.copy()\n                purpose_data = [\n                    item\n                    for item in combined_data\n                    if item[\"instruction\"].startswith(\"1) Describe the Purpose\")\n                ]\n\n                code_output = []\n                for item in purpose_data:\n                    instruction = (\n                        \"Define a Python code file that is described as follows:\\n\"\n                        + item[\"output\"]\n                    )\n                    code_output.append(\n                        {\"instruction\": instruction, \"output\": item[\"input\"]}\n                    )\n                write_file(code_output, Path(directory) / \"training.json\")\n            except Exception as e:\n                logging.info(f\"Error processing: {json_file}: {e}\")\n\n        write_file(combined_data, file_path)\n\n    # Save html / markdown file for each json file in the output directory\n    # convert_json_to_markdown(directory)\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n    \n    return {\"instruct_list\": instruct_data}\n    \n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = \"entire_code_graph\"\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n\n    # Create graphs, add nodes, and add edges\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n    # Draw graphs\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)  # Save the figure\n    plt.close()  # Close the figure\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, base_name: str, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # want output subdir to include the absolute path to the output dir + the directories between the output dir and the file\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n\n    # write instrunct.json files\n    file_names = [f\"{base_name}.instruct.json\", f\"{base_name}.details.yaml\"]\n    contents = [instruct_list, file_details]\n\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f\"Error creating graph for {base_name}: {e}\")\n\n```",
        "output": "Dependencies:logging, yaml, networkx, json, html, pathlib, matplotlib.pyplot, typing\n\nPurpose and Significance:\nThe `read_file` function reads a JSON or YAML file from the provided file path and returns its contents as a dictionary.\r\n\nThe `write_file` function writes a dictionary to a JSON or YAML file.\r\n\nThe `convert_json_to_html` function converts JSON files within a given directory to HTML format, preserving spacing and tabs for the 'input' field in each dataset. It saves each converted file with a .html extension.\r\n\nThe `combine_json_files` function merges all JSON files in the provided directory into one dictionary called `instruct_list`, removes duplicates from it, and writes it to an output file named `instruct.json`. It also converts the merged data to HTML format using the `convert_json_to_html` function.\r\n\nThe `create_code_graph` function generates code graphs based on the provided file details and saves them as PNG images in a specified output directory.\r\n\nThe `save_python_data` function saves details of a Python file, the `instruct_list` datasets generated by the `combine_json_files`, and the corresponding graph in YAML and JSON formats within the same subdirectory specified by `relative_path`. It generates and saves code graphs based on the provided file details. "
    },
    {
        "instruction": "Call code graph in Python file: `py2dataset.save_output.py`?",
        "input": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = (\n            100 / column_count\n        )  # Calculate the width for each column based on the number of columns\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []  # List to store each row's HTML content\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))  # Join all parts of the row and append to the list\n\n        # After the loop, join all rows\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef convert_json_to_markdown(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to Markdown format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def escape_markdown(text: str) -> str:\n        \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n        markdown_special_chars = \"\\\\`*_{}[]()#+-.!\"\n        for char in markdown_special_chars:\n            text = text.replace(char, f\"\\\\{char}\")\n        return text\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        markdown_content = \"# Data Report\\n\\n\"\n        # Create Markdown table headers\n        headers = dataset[0].keys()\n        markdown_content += \"| \" + \" | \".join(headers) + \" |\\n\"\n        markdown_content += \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n\n        # Create Markdown table rows\n        for entry in dataset:\n            row = \"| \" + \" | \".join(escape_markdown(str(entry[key])) for key in headers) + \" |\\n\"\n            markdown_content += row\n\n        markdown_file_path = json_file.with_suffix(\".md\")\n        try:\n            with open(markdown_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(markdown_content)\n        except Exception as e:\n            logging.error(f\"Failed to save Markdown file {markdown_file_path}: {e}\")\n            \n\ndef combine_json_files(directory: str, html: bool = False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    def remove_duplicate_dataset_entries(\n        dataset: List[Dict], key1: str, key2: str\n    ) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n\n    instruct_data = []\n    for file_name in [\"instruct.json\"]:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f\"*.{file_name}\"):\n            try:\n                json_file_data = read_file(json_file)\n                combined_data.extend(json_file_data)\n                combined_data = remove_duplicate_dataset_entries(\n                    combined_data, \"instruction\", \"output\"\n                )\n                instruct_data = combined_data.copy()\n                purpose_data = [\n                    item\n                    for item in combined_data\n                    if item[\"instruction\"].startswith(\"1) Describe the Purpose\")\n                ]\n\n                code_output = []\n                for item in purpose_data:\n                    instruction = (\n                        \"Define a Python code file that is described as follows:\\n\"\n                        + item[\"output\"]\n                    )\n                    code_output.append(\n                        {\"instruction\": instruction, \"output\": item[\"input\"]}\n                    )\n                write_file(code_output, Path(directory) / \"training.json\")\n            except Exception as e:\n                logging.info(f\"Error processing: {json_file}: {e}\")\n\n        write_file(combined_data, file_path)\n\n    # Save html / markdown file for each json file in the output directory\n    # convert_json_to_markdown(directory)\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n    \n    return {\"instruct_list\": instruct_data}\n    \n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = \"entire_code_graph\"\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n\n    # Create graphs, add nodes, and add edges\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n    # Draw graphs\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)  # Save the figure\n    plt.close()  # Close the figure\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, base_name: str, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # want output subdir to include the absolute path to the output dir + the directories between the output dir and the file\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n\n    # write instrunct.json files\n    file_names = [f\"{base_name}.instruct.json\", f\"{base_name}.details.yaml\"]\n    contents = [instruct_list, file_details]\n\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f\"Error creating graph for {base_name}: {e}\")\n\n```",
        "output": "{'nodes': ['read_file', 'write_file', 'convert_json_to_html', 'preserve_spacing', 'convert_json_to_markdown', 'escape_markdown', 'combine_json_files', 'remove_duplicate_dataset_entries', 'create_code_graph', 'save_python_data', 'file_path.open', 'json.load', 'yaml.load', 'json.dump', 'yaml.dump', \"text.replace(' ', '&nbsp;').replace\", 'text.replace', 'Path(directory).rglob', 'Path', 'len', 'dataset[0].keys', 'escape', 'str', 'value.replace', 'row_parts.append', 'html_rows.append', \"''.join\", 'json_file.with_suffix', 'open', 'file.write', 'logging.info', \"' | '.join\", 'logging.error', 'set', 'seen.add', 'result.append', 'combined_data.extend', 'combined_data.copy', \"item['instruction'].startswith\", 'code_output.append', 'nx.DiGraph', 'G.add_nodes_from', 'G.add_edge', 'edge.items', 'plt.figure', 'nx.spring_layout', 'nx.draw', 'G.edges', 'label.append', \"', '.join\", \"'\\\\n'.join\", 'nx.draw_networkx_edge_labels', 'plt.savefig', 'plt.close', 'output_subdir.mkdir', 'zip'], 'edges': [{'source': 'read_file', 'target': 'file_path.open', 'target_inputs': []}, {'source': 'read_file', 'target': 'json.load', 'target_inputs': ['f']}, {'source': 'read_file', 'target': 'yaml.load', 'target_inputs': ['f']}, {'source': 'write_file', 'target': 'file_path.open', 'target_inputs': [\"'w'\"]}, {'source': 'write_file', 'target': 'json.dump', 'target_inputs': ['data', 'f']}, {'source': 'write_file', 'target': 'yaml.dump', 'target_inputs': ['data', 'f']}, {'source': 'convert_json_to_html', 'target': \"text.replace(' ', '&nbsp;').replace\", 'target_inputs': [\"'\\\\t'\", \"'&nbsp;' * tab_width\"]}, {'source': 'convert_json_to_html', 'target': 'text.replace', 'target_inputs': [\"' '\", \"'&nbsp;'\"]}, {'source': 'convert_json_to_html', 'target': 'Path(directory).rglob', 'target_inputs': [\"'*.json'\"]}, {'source': 'convert_json_to_html', 'target': 'Path', 'target_inputs': ['directory']}, {'source': 'convert_json_to_html', 'target': 'read_file', 'target_inputs': ['json_file'], 'target_returns': ['{}', 'json.load(f)', 'yaml.load(f, Loader=yaml.SafeLoader)']}, {'source': 'convert_json_to_html', 'target': 'len', 'target_inputs': ['dataset[0].keys()']}, {'source': 'convert_json_to_html', 'target': 'dataset[0].keys', 'target_inputs': []}, {'source': 'convert_json_to_html', 'target': 'escape', 'target_inputs': ['str(entry[key])']}, {'source': 'convert_json_to_html', 'target': 'str', 'target_inputs': ['entry[key]']}, {'source': 'convert_json_to_html', 'target': 'preserve_spacing', 'target_inputs': ['value'], 'target_returns': [\"text.replace(' ', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)\"]}, {'source': 'convert_json_to_html', 'target': 'value.replace', 'target_inputs': [\"'\\\\n'\", \"'<br/>'\"]}, {'source': 'convert_json_to_html', 'target': 'row_parts.append', 'target_inputs': [\"f'<td>{value}</td>'\", \"'</tr>'\"]}, {'source': 'convert_json_to_html', 'target': 'html_rows.append', 'target_inputs': [\"''.join(row_parts)\"]}, {'source': 'convert_json_to_html', 'target': \"''.join\", 'target_inputs': ['row_parts', 'html_rows']}, {'source': 'convert_json_to_html', 'target': 'json_file.with_suffix', 'target_inputs': [\"'.html'\"]}, {'source': 'convert_json_to_html', 'target': 'open', 'target_inputs': ['html_file_path', \"'w'\"]}, {'source': 'convert_json_to_html', 'target': 'file.write', 'target_inputs': ['html_content']}, {'source': 'convert_json_to_html', 'target': 'logging.info', 'target_inputs': [\"f'Failed saving: {html_file_path}'\"]}, {'source': 'preserve_spacing', 'target': \"text.replace(' ', '&nbsp;').replace\", 'target_inputs': [\"'\\\\t'\", \"'&nbsp;' * tab_width\"]}, {'source': 'preserve_spacing', 'target': 'text.replace', 'target_inputs': [\"' '\", \"'&nbsp;'\"]}, {'source': 'convert_json_to_markdown', 'target': 'text.replace', 'target_inputs': ['char', \"f'\\\\\\\\{char}'\"]}, {'source': 'convert_json_to_markdown', 'target': 'Path(directory).rglob', 'target_inputs': [\"'*.json'\"]}, {'source': 'convert_json_to_markdown', 'target': 'Path', 'target_inputs': ['directory']}, {'source': 'convert_json_to_markdown', 'target': 'read_file', 'target_inputs': ['json_file'], 'target_returns': ['{}', 'json.load(f)', 'yaml.load(f, Loader=yaml.SafeLoader)']}, {'source': 'convert_json_to_markdown', 'target': 'dataset[0].keys', 'target_inputs': []}, {'source': 'convert_json_to_markdown', 'target': \"' | '.join\", 'target_inputs': ['headers', \"['---'] * len(headers)\", '(escape_markdown(str(entry[key])) for key in headers)']}, {'source': 'convert_json_to_markdown', 'target': 'len', 'target_inputs': ['headers']}, {'source': 'convert_json_to_markdown', 'target': 'escape_markdown', 'target_inputs': ['str(entry[key])'], 'target_returns': ['text']}, {'source': 'convert_json_to_markdown', 'target': 'str', 'target_inputs': ['entry[key]']}, {'source': 'convert_json_to_markdown', 'target': 'json_file.with_suffix', 'target_inputs': [\"'.md'\"]}, {'source': 'convert_json_to_markdown', 'target': 'open', 'target_inputs': ['markdown_file_path', \"'w'\"]}, {'source': 'convert_json_to_markdown', 'target': 'file.write', 'target_inputs': ['markdown_content']}, {'source': 'convert_json_to_markdown', 'target': 'logging.error', 'target_inputs': [\"f'Failed to save Markdown file {markdown_file_path}: {e}'\"]}, {'source': 'escape_markdown', 'target': 'text.replace', 'target_inputs': ['char', \"f'\\\\\\\\{char}'\"]}, {'source': 'combine_json_files', 'target': 'logging.info', 'target_inputs': [\"f'Combining JSON files in {directory}'\", \"f'Error processing: {json_file}: {e}'\", \"'Converting JSON files to HTML'\"]}, {'source': 'combine_json_files', 'target': 'set', 'target_inputs': []}, {'source': 'combine_json_files', 'target': 'seen.add', 'target_inputs': ['(item[key1], item[key2])']}, {'source': 'combine_json_files', 'target': 'result.append', 'target_inputs': ['item']}, {'source': 'combine_json_files', 'target': 'Path', 'target_inputs': ['directory', 'directory', 'directory']}, {'source': 'combine_json_files', 'target': 'Path(directory).rglob', 'target_inputs': [\"f'*.{file_name}'\"]}, {'source': 'combine_json_files', 'target': 'read_file', 'target_inputs': ['json_file'], 'target_returns': ['{}', 'json.load(f)', 'yaml.load(f, Loader=yaml.SafeLoader)']}, {'source': 'combine_json_files', 'target': 'combined_data.extend', 'target_inputs': ['json_file_data']}, {'source': 'combine_json_files', 'target': 'remove_duplicate_dataset_entries', 'target_inputs': ['combined_data', \"'instruction'\", \"'output'\"], 'target_returns': ['result']}, {'source': 'combine_json_files', 'target': 'combined_data.copy', 'target_inputs': []}, {'source': 'combine_json_files', 'target': \"item['instruction'].startswith\", 'target_inputs': [\"'1) Describe the Purpose'\"]}, {'source': 'combine_json_files', 'target': 'code_output.append', 'target_inputs': [\"{'instruction': instruction, 'output': item['input']}\"]}, {'source': 'combine_json_files', 'target': 'write_file', 'target_inputs': ['code_output', \"Path(directory) / 'training.json'\", 'combined_data', 'file_path'], 'target_returns': []}, {'source': 'combine_json_files', 'target': 'convert_json_to_html', 'target_inputs': ['directory'], 'target_returns': [\"text.replace(' ', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)\"]}, {'source': 'remove_duplicate_dataset_entries', 'target': 'set', 'target_inputs': []}, {'source': 'remove_duplicate_dataset_entries', 'target': 'seen.add', 'target_inputs': ['(item[key1], item[key2])']}, {'source': 'remove_duplicate_dataset_entries', 'target': 'result.append', 'target_inputs': ['item']}, {'source': 'create_code_graph', 'target': 'nx.DiGraph', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'G.add_nodes_from', 'target_inputs': [\"file_details['file_info'][graph_type]['nodes']\"]}, {'source': 'create_code_graph', 'target': 'G.add_edge', 'target_inputs': ['source', 'target']}, {'source': 'create_code_graph', 'target': 'edge.items', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'plt.figure', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'nx.spring_layout', 'target_inputs': ['G']}, {'source': 'create_code_graph', 'target': 'nx.draw', 'target_inputs': ['G', 'pos']}, {'source': 'create_code_graph', 'target': 'G.edges', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'label.append', 'target_inputs': ['f\"Inputs: {\\', \\'.join(edge[2][\\'target_inputs\\'])}\"', 'f\"\\\\nReturns: {\\', \\'.join(edge[2][\\'target_returns\\'])}\"']}, {'source': 'create_code_graph', 'target': \"', '.join\", 'target_inputs': [\"edge[2]['target_inputs']\", \"edge[2]['target_returns']\"]}, {'source': 'create_code_graph', 'target': \"'\\\\n'.join\", 'target_inputs': ['label']}, {'source': 'create_code_graph', 'target': 'nx.draw_networkx_edge_labels', 'target_inputs': ['G', 'pos']}, {'source': 'create_code_graph', 'target': 'plt.savefig', 'target_inputs': ['output_file']}, {'source': 'create_code_graph', 'target': 'plt.close', 'target_inputs': []}, {'source': 'save_python_data', 'target': 'Path', 'target_inputs': ['output_dir']}, {'source': 'save_python_data', 'target': 'output_subdir.mkdir', 'target_inputs': []}, {'source': 'save_python_data', 'target': 'zip', 'target_inputs': ['file_names', 'contents']}, {'source': 'save_python_data', 'target': 'write_file', 'target_inputs': ['content', 'output_subdir / file_name'], 'target_returns': []}, {'source': 'save_python_data', 'target': 'create_code_graph', 'target_inputs': ['file_details', 'base_name', 'output_subdir'], 'target_returns': []}, {'source': 'save_python_data', 'target': 'logging.info', 'target_inputs': [\"f'Error creating graph for {base_name}: {e}'\"]}]}"
    },
    {
        "instruction": "Functions defined in Python file: `py2dataset.save_output.py`?",
        "input": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = (\n            100 / column_count\n        )  # Calculate the width for each column based on the number of columns\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []  # List to store each row's HTML content\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))  # Join all parts of the row and append to the list\n\n        # After the loop, join all rows\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef convert_json_to_markdown(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to Markdown format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def escape_markdown(text: str) -> str:\n        \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n        markdown_special_chars = \"\\\\`*_{}[]()#+-.!\"\n        for char in markdown_special_chars:\n            text = text.replace(char, f\"\\\\{char}\")\n        return text\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        markdown_content = \"# Data Report\\n\\n\"\n        # Create Markdown table headers\n        headers = dataset[0].keys()\n        markdown_content += \"| \" + \" | \".join(headers) + \" |\\n\"\n        markdown_content += \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n\n        # Create Markdown table rows\n        for entry in dataset:\n            row = \"| \" + \" | \".join(escape_markdown(str(entry[key])) for key in headers) + \" |\\n\"\n            markdown_content += row\n\n        markdown_file_path = json_file.with_suffix(\".md\")\n        try:\n            with open(markdown_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(markdown_content)\n        except Exception as e:\n            logging.error(f\"Failed to save Markdown file {markdown_file_path}: {e}\")\n            \n\ndef combine_json_files(directory: str, html: bool = False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    def remove_duplicate_dataset_entries(\n        dataset: List[Dict], key1: str, key2: str\n    ) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n\n    instruct_data = []\n    for file_name in [\"instruct.json\"]:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f\"*.{file_name}\"):\n            try:\n                json_file_data = read_file(json_file)\n                combined_data.extend(json_file_data)\n                combined_data = remove_duplicate_dataset_entries(\n                    combined_data, \"instruction\", \"output\"\n                )\n                instruct_data = combined_data.copy()\n                purpose_data = [\n                    item\n                    for item in combined_data\n                    if item[\"instruction\"].startswith(\"1) Describe the Purpose\")\n                ]\n\n                code_output = []\n                for item in purpose_data:\n                    instruction = (\n                        \"Define a Python code file that is described as follows:\\n\"\n                        + item[\"output\"]\n                    )\n                    code_output.append(\n                        {\"instruction\": instruction, \"output\": item[\"input\"]}\n                    )\n                write_file(code_output, Path(directory) / \"training.json\")\n            except Exception as e:\n                logging.info(f\"Error processing: {json_file}: {e}\")\n\n        write_file(combined_data, file_path)\n\n    # Save html / markdown file for each json file in the output directory\n    # convert_json_to_markdown(directory)\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n    \n    return {\"instruct_list\": instruct_data}\n    \n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = \"entire_code_graph\"\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n\n    # Create graphs, add nodes, and add edges\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n    # Draw graphs\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)  # Save the figure\n    plt.close()  # Close the figure\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, base_name: str, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # want output subdir to include the absolute path to the output dir + the directories between the output dir and the file\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n\n    # write instrunct.json files\n    file_names = [f\"{base_name}.instruct.json\", f\"{base_name}.details.yaml\"]\n    contents = [instruct_list, file_details]\n\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f\"Error creating graph for {base_name}: {e}\")\n\n```",
        "output": "Functions defined:read_file, write_file, convert_json_to_html, preserve_spacing, convert_json_to_markdown, escape_markdown, combine_json_files, remove_duplicate_dataset_entries, create_code_graph, save_python_data\n\nPurpose and Significance:\nFunctions defined in the context:\r\n1. `read_file` function reads a JSON or YAML file and returns its contents as a dictionary.\r\n2. `write_file` function writes a dictionary to a JSON or YAML file.\r\n3. `convert_json_to_html` function converts JSON files within a given directory to HTML format, preserving the spacing and tabs for 'input' field.\r\n4. `combine_json_files` function merges all JSON files in a directory into a single file called `instruct.json`, removes duplicates, and returns the 'instruct_list' datasets.\r\n5. `create_code_graph` generates code graphs based on provided Python file details and saves them as PNG images in a specified output directory.\r\n6. `save_python_data` saves the details of a Python file as a YAML file, saves instruction data as JSON files, and generates code graphs. "
    },
    {
        "instruction": "Inputs to `read_file` in Python file: `py2dataset.save_output.py`?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}",
        "output": "Inputs to `read_file`:file_path\n\nPurpose and Significance:\nThe `read_file` function takes a file path as an argument and reads its contents based on the extension (json or yaml) and returns it as a dictionary. The `write_file` function writes a dictionary to a file in either JSON or YAML format, depending on the file type of the provided file path. The `convert_json_to_html` function converts all json files within a given directory to HTML format, preserving spacing and tabs for the 'input' field. The `combine_json_files` function merges all JSON files in the specified directory, removes duplicates from the combined data, and saves it as 'instruct.json'. It also converts merged JSON files to HTML format using `convert_json_to_html`. The `create_code_graph` function generates code graphs based on provided file details, and `save_python_data` function saves Python file details as a YAML file, the instruction data as 'instruct.json' files, and generates and saves code graphs.\r\n\nThe `read_file` function accepts a Path object which is passed as an argument and returns a dictionary containing the contents of a JSON or YAML file. It handles two different cases where the provided path has a `.json` extension or a `.yaml` extension. If it's a `.json` file, it uses `json.load()` to load the data from the file into a dictionary; if it's a `.yaml` file, it loads using the SafeLoader for handling references.\r\n\nThe `write_file` function writes the contents of the dictionary provided in either JSON or YAML format based on the extension of the file path. It uses `json.dump()` to write data as JSON and `yaml.SafeDumper.ignore_aliases = lambda *args: True; yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)` to write the dictionary as YAML.\r\n\nThe `convert_json_to_html` function converts JSON files within a given directory to HTML format by reading each file in the directory and converting it into an HTML table with preserved spacing and tabs for the 'input' field. It then saves each converted file with a `.html` extension.\r\n\nThe `combine_json_files` function takes a string `directory`, loops through all json files within that directory, merges them using the `read_file` function to remove duplicates based on `instruction` and `output`. It then saves the combined data as 'instruct.json' in the specified directory.\r\n\nThe `create_code_graph` function takes a dictionary of file details, a base name for the output files, and an output directory as arguments. It generates code graphs based on the provided file details using networkx library and saves them as PNG images in the specified output directory. \r\n\nThe `save_python_data` function also accepts three additional arguments: `file_details`, which contains details of a Python file, a base name for the saved files, and an output directory.\r\n"
    },
    {
        "instruction": "Inputs to `write_file` in Python file: `py2dataset.save_output.py`?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "Inputs to `write_file`:data, file_path\n\nPurpose and Significance:\nThe purpose of `write_file()` is to save a dictionary to either JSON or YAML format based on the extension of the provided file path. The function takes two arguments - `data`, which is the dictionary that needs to be saved and `file_path` which is the path to the output file. \r\n\nThe significance of 'Inputs to `write_file`': {'data', 'file_path'} is that `data` contains the data in dictionary format, which will be saved as a JSON or YAML file based on the extension of the provided `file_path`. The function uses `json.dump()` for saving the data in JSON format and `yaml.dump()` to save data as a YAML file. Both `json` and `yaml` library methods require us to specify a parameter that should not be missing (`data`), otherwise we will get an error. \r\n\nThe function also handles exceptions if there is any issue while writing the file, so it logs the error message instead of raising an exception.\r\n\n### Explanation: \r\n\n```python\r\ndef write_file(data: Dict, file_path: Path) -> None:\r\n    \"\"\"\r\n    Writes a dictionary to a JSON or YAML file.\r\n    Args:\r\n        data (Dict): The data to write to the file.\r\n        file_path (Path): The path to the file.\r\n    Returns:\r\n        None\r\n    \"\"\"\r\n    file_type = file_path.suffix[1:]  # get the extension of the file, i.e., json or yaml\r\n    with file_path.open(\"w\") as f:\r\n        if file_type == \"json\":\r\n            json.dump(data, f, indent=4)  # save data in JSON format\r\n        elif file_type == \"yaml\":\r\n            yaml.SafeDumper.ignore_aliases = lambda *args: True  # ignore aliases while saving YAML files\r\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)  # save data in YAML format\r\n```\r\n\n### Purpose of the 'write_file` function \r\n1. Detecting if a given path represents an absolute file (and reject otherwise), ensuring `save` operations always include files\r\n2. Determining the appropriate serialization method based on the extension of the provided file path, and using it to write data to disk.\r\n3. Handling exceptions that may occur during writing of the file, logging them instead of raising an exception so as not to break the program flow.\r\n\n### Reasoning: \r\n- We need to check if a given `file_path` represents a valid file or directory, because otherwise we may run into issues trying to save data. It is always safer to make sure that `save` operations are only performed on files and not directories.\r\n- The function uses the `suffix` attribute of the `Path` object to determine if it's a JSON or YAML file, then uses the appropriate serialization method (`json.dump()` for JSON and `yaml.dump()` for YAML) to write data to disk.\r\n- It also handles exceptions that may occur during writing of the file by logging them instead of raising an exception, so as not to break the program flow. "
    },
    {
        "instruction": "Inputs to `convert_json_to_html` in Python file: `py2dataset.save_output.py`?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')",
        "output": "Inputs to `convert_json_to_html`:directory\n\nPurpose and Significance:\nThe `convert_json_to_html` function takes a directory as an argument, reads all JSON files in that directory, converts them into HTML format, and saves each converted file with a .html extension. The function preserves spaces and tabs within the 'input' field by replacing them with appropriate HTML tags such as `&nbsp;`, which helps maintain the original formatting of the input data when displayed on a web page.\r\n\nThe `combine_json_files` function takes a directory as an argument, merges all JSON files in that directory, removes duplicates based on 'instruction' and 'output', saves the merged dataset to 'instruct.json', converts the combined JSON file into HTML format, and returns the 'instruct_list' datasets.\r\n\nThe `create_code_graph` function generates a graph using the nodes and edges in a given file, creates a PNG image of it using `matplotlib`, and saves it to the specified output directory.\r\n\nThe `save_python_data` function takes details of a Python file, a base name, and an output directory as arguments. It saves the details of the Python file as a YAML file, saves the instruction data as JSON files, generates code graphs using `create_code_graph`, and saves them in the specified output directory.\r\n\nAll these functions are used by the main py2dataset script to generate HTML reports, save the instructions as JSON files, and create code graphs. "
    },
    {
        "instruction": "Inputs to `preserve_spacing` in Python file: `py2dataset.save_output.py`?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "Inputs to `preserve_spacing`:text, tab_width\n\nPurpose and Significance:\nIn `utils.py`, `write_file(data, file_path)` accepts two parameters `data` which is a dictionary to be saved as JSON or YAML format and `file_path` which is the path to the file where it will be saved. It then uses `json.dump()` function to save the data in JSON format if the extension of `file_path` is '.json' or `yaml.dump()` function to save data in YAML format if the extension of `file_path` is '.yaml'. \n\nIn `utils.py`, `read_file(file_path)` accepts a file path as an argument and reads its contents based on its extension (either '.json' or '.yaml') using `json.load()` function for JSON files and `yaml.load()` function for YAML files. It returns the contents of the file as a dictionary.\r\n\nIn `utils.py`, `convert_json_to_html(directory)` converts all `.json` files within a given directory to an HTML format using a simple loop that reads each JSON file, creates an HTML table with headers and rows, and saves it in a new file with the same name but '.html' extension.\r\n\nIn `utils.py`, `combine_json_files(directory)` is used to merge all JSON files within a given directory into one `instruct.json` file by iterating through each `.json` file and removing duplicates based on 'input' and 'output' fields, and saves it as a dictionary. \r\n\nIn `utils.py`, `create_code_graph(file_details, base_name, output_subdir)` is used to create code graphs using the NetworkX library in Python. It takes three parameters: `file_details` which contains information about the file, `base_name` which is a string representing the name of the graph and `output_subdir` which is the directory where the graph will be saved.\r\n\nIn `utils.py`, `save_python_data(file_details, instruct_list, base_name, relative_path, output_dir)` saves Python file details as a YAML file, the instruction data as a JSON file, and code graphs. It takes four parameters: `file_details` which contains information about the file, `instruct_list` which is a list of dictionaries containing instructions, `base_name` which is a string representing the name of the graph, `relative_path` which is the relative path to the Python file and `output_dir` which is the directory where the output files will be saved. It creates an output subdirectory for each Python file and saves all required files in it. "
    },
    {
        "instruction": "Inputs to `convert_json_to_markdown` in Python file: `py2dataset.save_output.py`?",
        "input": "def convert_json_to_markdown(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to Markdown format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def escape_markdown(text: str) -> str:\n        \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n        markdown_special_chars = '\\\\`*_{}[]()#+-.!'\n        for char in markdown_special_chars:\n            text = text.replace(char, f'\\\\{char}')\n        return text\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        markdown_content = '# Data Report\\n\\n'\n        headers = dataset[0].keys()\n        markdown_content += '| ' + ' | '.join(headers) + ' |\\n'\n        markdown_content += '| ' + ' | '.join(['---'] * len(headers)) + ' |\\n'\n        for entry in dataset:\n            row = '| ' + ' | '.join((escape_markdown(str(entry[key])) for key in headers)) + ' |\\n'\n            markdown_content += row\n        markdown_file_path = json_file.with_suffix('.md')\n        try:\n            with open(markdown_file_path, 'w', encoding='utf-8') as file:\n                file.write(markdown_content)\n        except Exception as e:\n            logging.error(f'Failed to save Markdown file {markdown_file_path}: {e}')",
        "output": "Inputs to `convert_json_to_markdown`:directory\n\nPurpose and Significance:\nThe `convert_json_to_markdown` function is used to convert JSON files within a given directory to Markdown format. It reads each JSON file in the specified directory, creates a table with headers and rows based on the keys of the first entry in the dataset, and writes it as an HTML file with the same name but different extension. The `directory` parameter specifies the path to the directory where the JSON files are located.\r\n\nThe `convert_json_to_html` function is similar to `convert_json_to_markdown`, but instead of creating a Markdown table, it creates an HTML table with preserved spacing and tabs for the 'input' field. It also accepts a directory path as an argument and saves each converted file with a .html extension.\r\n\nThe `combine_json_files` function is used to combine all JSON files in the output directory into 'instruct.json', remove duplicates, and write it back to the same directory. The `directory` parameter specifies the path to the directory where the output files are located. It also converts the combined JSON file to HTML format.\r\n\nThe `create_code_graph` function is used to generate code graphs based on the provided file details, saves them as PNG images in a specified output directory using NetworkX library. The `base_name` parameter specifies the name of the output files and `output_subdir` parameter specifies the path to save the generated graph image.\r\n\nThe `save_python_data` function is used to save the details of a Python file as a YAML file, write the instruction data as JSON files, and generate code graphs. The `file_details` parameter contains the extracted information from the Python file, `instruct_list` parameter contains the instruction data extracted from the Python file, `base_name` specifies the base name of the output files, `relative_path` parameter is a path to the directory where the input file is located and `output_dir` parameter specifies the path to save the generated graph image. "
    },
    {
        "instruction": "Inputs to `escape_markdown` in Python file: `py2dataset.save_output.py`?",
        "input": "def escape_markdown(text: str) -> str:\n    \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n    markdown_special_chars = '\\\\`*_{}[]()#+-.!'\n    for char in markdown_special_chars:\n        text = text.replace(char, f'\\\\{char}')\n    return text",
        "output": "Inputs to `escape_markdown`:text\n\nPurpose and Significance:\nThe `escape_markdown` function is used in the `convert_json_to_markdown` function to escape Markdown special characters in the provided text. It takes a string as input and returns a modified version of that string with all Markdown special characters escaped using backslashes. This ensures that the text will be properly displayed when converted to Markdown format.\r\n\nThe `read_file` function is used to read JSON or YAML files and return their contents as dictionaries. It takes a file path as an argument, opens the file, reads its content, and returns it as a dictionary. \r\n\nThe `write_file` function is used to write data to a JSON or YAML file. It takes a dictionary and a file path as arguments, opens the file in write mode, and writes the dictionary to it using the appropriate format (JSON or YAML).\r\n\nThe `convert_json_to_html` function is used to convert JSON files within a given directory to HTML format. It iterates over all JSON files in the specified directory, reads their contents as dictionaries, and then generates an HTML table with each row representing one entry from the dictionary. The function uses `preserve_spacing` to preserve spacing and tabs in the 'input' field.\r\n\nThe `combine_json_files` function is used to merge all JSON files in a directory into a single file called 'instruct.json', remove duplicates, and convert the merged data to HTML format. It first creates an empty list called `instruct_data`, then iterates over each JSON file in the specified directory. For each file, it reads its contents as a dictionary using `read_file` and appends any entries that are not already in `instruct_data`. After all files have been processed, it removes duplicates from `instruct_data` based on the 'instruction' and 'output' keys and writes the result to 'instruct.json'. The function also creates a 'training.json' file with the same content as 'instruct.json', but filtered to only include entries where the instruction starts with \"1) Describe the Purpose\".\r\n\nThe `create_code_graph` function is used to generate code graphs based on the provided file details. It takes a dictionary of file details, a base name for the output files, and an output directory as arguments. The function creates a directed graph using NetworkX, adds nodes from the 'nodes' list in the file details, and then iterates over each edge in the 'edges' list to add edges between the nodes with their corresponding source and target attributes. It then draws the graph using `nx.draw` and saves it as a PNG image in the specified output directory.\r\n\nThe `save_python_data` function is used to save Python file details, instruction data, and generate code graphs. It takes a dictionary of file details, a list of instruction data, a base name for the output files, and an output directory as arguments. The function creates a subdirectory within the specified output directory based on the relative path to the Python file and writes both the `file_details` dictionary (as YAML) and `instruct_list` as JSON using the `write_file` function.\r\nIt also generates code graphs using the `create_code_graph` function and saves them as PNG images with the base name provided. "
    },
    {
        "instruction": "Inputs to `combine_json_files` in Python file: `py2dataset.save_output.py`?",
        "input": "def combine_json_files(directory: str, html: bool=False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            try:\n                json_file_data = read_file(json_file)\n                combined_data.extend(json_file_data)\n                combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n                instruct_data = combined_data.copy()\n                purpose_data = [item for item in combined_data if item['instruction'].startswith('1) Describe the Purpose')]\n                code_output = []\n                for item in purpose_data:\n                    instruction = 'Define a Python code file that is described as follows:\\n' + item['output']\n                    code_output.append({'instruction': instruction, 'output': item['input']})\n                write_file(code_output, Path(directory) / 'training.json')\n            except Exception as e:\n                logging.info(f'Error processing: {json_file}: {e}')\n        write_file(combined_data, file_path)\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "Inputs to `combine_json_files`:directory, html\n\nPurpose and Significance:\nThe function `read_file()` accepts a file path as an argument and returns its contents as a dictionary. The file type (YAML or JSON) is automatically identified by its suffix [req01-a].\r\n\n`write_file()` is similar but can be used to save either a JSON or YAML formatted data into the provided file path [req02-b]. \r\n\nThe `convert_json_to_html()` function takes a directory as input, finds all the JSON files in it and converts them to HTML format while preserving spacing and tabs for the 'input' field. It then saves each converted file with a .html extension [req03-a]. \r\n\nThe `combine_json_files()` function accepts a directory path as an argument, merges all JSON files in it, removes duplicates from the combined data, writes them to 'instruct.json' and returns the 'instruct_list' datasets [req04-b-c]. \r\n\nThe `create_code_graph()` function generates code graphs based on the provided file details, saves them as PNG images in a specified output directory [req05-a] and d. The `save_python_data()` function accepts details of a Python file, a base name, and an output directory, and does all three steps mentioned in the requirements [req06]. "
    },
    {
        "instruction": "Inputs to `remove_duplicate_dataset_entries` in Python file: `py2dataset.save_output.py`?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "Inputs to `remove_duplicate_dataset_entries`:dataset, key1, key2\n\nPurpose and Significance:\nThe `remove_duplicate_dataset_entries` function is a helper function that removes duplicate entries from the provided dataset based on the provided keys. It takes in three arguments - `dataset`, which is a list of dictionaries, `key1`, and `key2`. The function iterates through each item in the dataset, checks if the combination of values for `key1` and `key2` has already been seen before, and adds it to a set called `seen` if not. If an entry is already in `seen`, it will be skipped over. Finally, the function returns a new list with only unique entries based on the provided keys.\r\n\nThe `convert_json_to_html` function converts JSON files within a given directory to HTML format by reading each file, extracting its contents as a dictionary, and writing an HTML table representation of it back out to a file with the same name but `.html` extension. The `preserve_spacing` helper function is used to preserve spaces and tabs in the `'input'` field.\r\n\nThe `combine_json_files` function merges all JSON files in the provided directory into one 'instruct.json' file, removes duplicates based on the `instruction` and `output` keys, and saves it back out. It also creates a `training.json` file containing only the code output for each dataset with an instruction starting with \"1) Describe a Python code file that is described as follows:\". The function then returns a dictionary containing the 'instruct_list' datasets.\r\n\nThe `create_code_graph` function generates and saves code graphs based on the provided file details. It takes in three arguments - `file_details`, which contains information about the Python file, such as its AST, `base_name`, which is used to name the output files, and `output_subdir`, which is where the PNG images will be saved.\r\n\nThe `save_python_data` function takes in details about a Python file (file details, `instruct_list`, base name, and an output directory), saves them as YAML and JSON files, generates code graphs using `create_code_graph`, and saves those graphs to the specified output directory. "
    },
    {
        "instruction": "Inputs to `create_code_graph` in Python file: `py2dataset.save_output.py`?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)\n    plt.close()",
        "output": "Inputs to `create_code_graph`:file_details, base_name, output_subdir\n\nPurpose and Significance:\nThe function `read_file(file_path: Path)` accepts a file path as an argument and reads and returns its contents in dictionary format depending on the file type (JSON or YAML). The `write_file(data: Dict, file_path: Path)` function writes a dictionary to a JSON or YAML file. The `convert_json_to_html(directory: str)` function converts all JSON files within a given directory to HTML format while preserving spacing and tabs for the 'input' field in each dataset. The `combine_json_files(directory: str, html: bool = False)` function merges all JSON files in the specified directory, removes duplicates from the combined data, and saves it as an 'instruct.json' file. It also converts the merged JSON files to HTML format if `html` is set to True. The `create_code_graph(file_details: Dict, base_name: str, output_subdir: Path)` function generates code graphs based on the provided file details and saves them as PNG images in the specified output directory. Finally, the `save_python_data(file_details: dict, instruct_list: list, base_name: str, relative_path: Path, output_dir: str)` function saves the details of a Python file as a YAML file, saves the instruction data as JSON files, and generates code graphs. "
    },
    {
        "instruction": "Inputs to `save_python_data` in Python file: `py2dataset.save_output.py`?",
        "input": "def save_python_data(file_details: dict, instruct_list: list, base_name: str, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f'Error creating graph for {base_name}: {e}')",
        "output": "Inputs to `save_python_data`:file_details, instruct_list, base_name, relative_path, output_dir\n\nPurpose and Significance:\nThe purpose of the `read_file` function is to read a JSON or YAML file and return its contents as a dictionary. The `write_file` function is used to write a dictionary to a JSON or YAML file. The `convert_json_to_html` function converts JSON files within a given directory to HTML format, preserving spacing and tabs for the 'input' field. The `combine_json_files` function merges all JSON files in the specified directory, removes duplicates from the combined JSON files, and writes them to an output file named 'instruct.json'. It also converts the merged data to HTML format using the `convert_json_to_html` function. The `create_code_graph` function generates code graphs based on the provided file details and saves them as PNG images in a specified directory. Finally, the `save_python_data` function saves the details of a Python file as a YAML file, saves the instruction data as JSON files, and generates code graphs using the `create_code_graph` function. "
    },
    {
        "instruction": "Docstring of `read_file` in Python file: `py2dataset.save_output.py`?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}",
        "output": "Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary."
    },
    {
        "instruction": "Docstring of `write_file` in Python file: `py2dataset.save_output.py`?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None"
    },
    {
        "instruction": "Docstring of `convert_json_to_html` in Python file: `py2dataset.save_output.py`?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')",
        "output": "Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None"
    },
    {
        "instruction": "Docstring of `preserve_spacing` in Python file: `py2dataset.save_output.py`?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "Preserve spaces and tabs in the provided text."
    },
    {
        "instruction": "Docstring of `convert_json_to_markdown` in Python file: `py2dataset.save_output.py`?",
        "input": "def convert_json_to_markdown(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to Markdown format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def escape_markdown(text: str) -> str:\n        \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n        markdown_special_chars = '\\\\`*_{}[]()#+-.!'\n        for char in markdown_special_chars:\n            text = text.replace(char, f'\\\\{char}')\n        return text\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        markdown_content = '# Data Report\\n\\n'\n        headers = dataset[0].keys()\n        markdown_content += '| ' + ' | '.join(headers) + ' |\\n'\n        markdown_content += '| ' + ' | '.join(['---'] * len(headers)) + ' |\\n'\n        for entry in dataset:\n            row = '| ' + ' | '.join((escape_markdown(str(entry[key])) for key in headers)) + ' |\\n'\n            markdown_content += row\n        markdown_file_path = json_file.with_suffix('.md')\n        try:\n            with open(markdown_file_path, 'w', encoding='utf-8') as file:\n                file.write(markdown_content)\n        except Exception as e:\n            logging.error(f'Failed to save Markdown file {markdown_file_path}: {e}')",
        "output": "Convert JSON files within a given directory to Markdown format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None"
    },
    {
        "instruction": "Docstring of `escape_markdown` in Python file: `py2dataset.save_output.py`?",
        "input": "def escape_markdown(text: str) -> str:\n    \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n    markdown_special_chars = '\\\\`*_{}[]()#+-.!'\n    for char in markdown_special_chars:\n        text = text.replace(char, f'\\\\{char}')\n    return text",
        "output": "Escape Markdown special characters in the provided text."
    },
    {
        "instruction": "Docstring of `combine_json_files` in Python file: `py2dataset.save_output.py`?",
        "input": "def combine_json_files(directory: str, html: bool=False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            try:\n                json_file_data = read_file(json_file)\n                combined_data.extend(json_file_data)\n                combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n                instruct_data = combined_data.copy()\n                purpose_data = [item for item in combined_data if item['instruction'].startswith('1) Describe the Purpose')]\n                code_output = []\n                for item in purpose_data:\n                    instruction = 'Define a Python code file that is described as follows:\\n' + item['output']\n                    code_output.append({'instruction': instruction, 'output': item['input']})\n                write_file(code_output, Path(directory) / 'training.json')\n            except Exception as e:\n                logging.info(f'Error processing: {json_file}: {e}')\n        write_file(combined_data, file_path)\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets."
    },
    {
        "instruction": "Docstring of `remove_duplicate_dataset_entries` in Python file: `py2dataset.save_output.py`?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries."
    },
    {
        "instruction": "Docstring of `create_code_graph` in Python file: `py2dataset.save_output.py`?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)\n    plt.close()",
        "output": "Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None"
    },
    {
        "instruction": "Docstring of `save_python_data` in Python file: `py2dataset.save_output.py`?",
        "input": "def save_python_data(file_details: dict, instruct_list: list, base_name: str, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f'Error creating graph for {base_name}: {e}')",
        "output": "Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None"
    },
    {
        "instruction": "Calls made in `read_file` in Python file: `py2dataset.save_output.py`?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}",
        "output": "Calls made in `read_file`:file_path.open, json.load, yaml.load\n\nPurpose and Significance:\nThe `read_file` function takes a file path as an argument and returns its contents as a dictionary based on the file type (JSON or YAML). It uses the `json` and `yaml` libraries to load the data from the file. \n\nThe `write_file` function takes a dictionary and a file path as arguments and writes it in JSON or YAML format depending on the extension of the file. \n\nThe `convert_json_to_html` function converts all JSON files within a given directory to HTML format, preserving spacing and tabs for the 'input' field. It saves each converted file with a .html extension. \n\nThe `combine_json_files` function merges all JSON files in the specified directory, removes duplicates from the combined JSON files, and writes them to an 'instruct.json' file. It also converts the merged data to HTML format using the `convert_json_to_html` function. \n\nThe `create_code_graph` function generates code graphs based on the provided file details using the NetworkX library. It saves the graph as a PNG image in the specified output directory. \n\nThe `save_python_data` function saves the details of the Python file as a YAML file and also writes the instruction data to 'instruct.json' files. It generates and saves code graphs using the `create_code_graph` function. "
    },
    {
        "instruction": "Calls made in `write_file` in Python file: `py2dataset.save_output.py`?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "Calls made in `write_file`:file_path.open, json.dump, yaml.dump\n\nPurpose and Significance:\nThe `read_file` function is a utility function that reads JSON or YAML files and returns their contents as dictionaries. The `write_file` function is a utility function that writes dictionaries to JSON or YAML files. The `convert_json_to_html` function converts JSON files within a given directory into HTML format with the original formatting preserved (using tabs). It creates HTML table markup based on the dictionary structure, but the spaces are converted to `&nbsp;`, as appropriate for web browsers, and newline characters are replaced with `<br/>`. The `combine_json_files` function reads all JSON files in a given directory and merges them into one dataset while removing duplicate entries. It then saves this combined dataset to a file called `instruct.json`. The `create_code_graph` function generates code graphs based on the provided file details, and the `save_python_data` function saves Python file details as YAML files, instruction data as JSON files, and generates and saves code graphs.\r\n\nThe `file_path.open`, `json.dump`, and `yaml.dump` functions are called within these utility functions to read/write JSON or YAML files, respectively. "
    },
    {
        "instruction": "Calls made in `convert_json_to_html` in Python file: `py2dataset.save_output.py`?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')",
        "output": "Calls made in `convert_json_to_html`:text.replace(, &nbsp;').replace, text.replace, Path(directory).rglob, Path, read_file, len, dataset[0].keys, escape, str, preserve_spacing, value.replace, row_parts.append, html_rows.append, .join, json_file.with_suffix, open, file.write, logging.info\n\nPurpose and Significance:\nThe `read_file` function reads a JSON or YAML file and returns its contents as a dictionary.\r\n\nThe `write_file` function writes a dictionary to a JSON or YAML file.\r\n\nThe `convert_json_to_html` function converts the JSON files within a given directory into an HTML table format with preserved spaces and tabs in the 'input' field of the data. The resulting files will have `.html` extension.\r\n\nThe `combine_json_files` function combines all JSON files in the output directory, removes duplicates from the combined JSON files, and writes them to a new file called `instruct.json`. It also creates a `training.json` file containing only the Python code for the 'Purpose' section of each dataset.\r\n\nThe `create_code_graph` function generates code graphs based on the provided file details using the NetworkX library and saves them as PNG images in the specified output directory.\r\n\nThe `save_python_data` function saves the details of a Python file as a YAML file, saves the instruction data as JSON files, and generates and saves code graphs. "
    },
    {
        "instruction": "Calls made in `preserve_spacing` in Python file: `py2dataset.save_output.py`?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "Calls made in `preserve_spacing`:text.replace(, &nbsp;').replace, text.replace\n\nPurpose and Significance:\nThe `preserve_spacing` function is used to preserve spaces and tabs in the provided text while converting JSON files to HTML format. It replaces all spaces with `&nbsp;` characters and tabs with a certain number of `&nbsp;` characters equal to the specified tab width (default 4). This is done so that the outputted HTML file maintains the spacing and indentation of the input data when displayed in a web browser. The `escape_markdown` function is used to escape Markdown special characters in the provided text while converting JSON files to Markdown format. It replaces all special characters with a backslash before them so that they are not interpreted as Markdown formatting characters.\r\n\nThe `read_file` function reads a file path and returns its contents as a dictionary, either in JSON or YAML format depending on the file extension. The `write_file` function takes a dictionary and a file path as arguments and writes it to a file in the same format. It can handle both JSON and YAML files.\r\n\nThe `convert_json_to_html` function is used to convert all JSON files within a given directory to HTML format, preserving spacing and tabs for the 'input' field. The `combine_json_files` function takes a directory path as an argument and does three main tasks: (a) combines all JSON files in that directory, removes duplicate entries from each file using a key-value combination of 'instruction' and 'output', (b) saves the combined data to 'instruct.json', and (c) converts the merged JSON files to HTML format.\r\n\nThe `create_code_graph` function generates code graphs based on the provided file details, which includes nodes and edges extracted from a Python file using AST parsing. It then saves the graph as a PNG image in the specified output directory.\r\n\nThe `save_python_data` function takes details of a Python file, a base name, and an output directory as arguments. It saves the details of the Python file as a YAML file (in the specified output directory), saves the instruction data as JSON files (in the same directory), and generates code graphs for each input file using `create_code_graph`. "
    },
    {
        "instruction": "Calls made in `convert_json_to_markdown` in Python file: `py2dataset.save_output.py`?",
        "input": "def convert_json_to_markdown(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to Markdown format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def escape_markdown(text: str) -> str:\n        \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n        markdown_special_chars = '\\\\`*_{}[]()#+-.!'\n        for char in markdown_special_chars:\n            text = text.replace(char, f'\\\\{char}')\n        return text\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        markdown_content = '# Data Report\\n\\n'\n        headers = dataset[0].keys()\n        markdown_content += '| ' + ' | '.join(headers) + ' |\\n'\n        markdown_content += '| ' + ' | '.join(['---'] * len(headers)) + ' |\\n'\n        for entry in dataset:\n            row = '| ' + ' | '.join((escape_markdown(str(entry[key])) for key in headers)) + ' |\\n'\n            markdown_content += row\n        markdown_file_path = json_file.with_suffix('.md')\n        try:\n            with open(markdown_file_path, 'w', encoding='utf-8') as file:\n                file.write(markdown_content)\n        except Exception as e:\n            logging.error(f'Failed to save Markdown file {markdown_file_path}: {e}')",
        "output": "Calls made in `convert_json_to_markdown`:text.replace, Path(directory).rglob, Path, read_file, dataset[0].keys, | '.join, len, escape_markdown, str, json_file.with_suffix, open, file.write, logging.error\n\nPurpose and Significance:\nThe `convert_json_to_html` function is used to convert JSON files within a given directory to HTML format by preserving spacing and tabs for the 'input' field in the dictionary data. It does this by reading each JSON file using the `read_file` function, iterating through the dataset, and writing the contents of each entry as an HTML table row with appropriate formatting. The resulting HTML files are saved to a new file with a `.html` extension.\r\n\nThe `combine_json_files` function is used to combine all JSON files in the output directory into 'instruct.json' and remove duplicates based on the provided keys. It does this by iterating through each JSON file using `Path(directory).rglob(\"*.json\")`, reading the data using `read_file`, removing duplicates, and then writing the combined dataset to a new file with the same name.\r\n\nThe `create_code_graph` function is used to generate code graphs based on the provided file details. It takes in a dictionary of file details, a base name for the output files, and an output directory as arguments. It creates a directed graph using NetworkX, adds nodes from the 'nodes' list in the file details, and then iterates through the edges in the 'edges' list to add edges between nodes. The resulting graphs are saved as PNG images with the specified base name and subdirectory.\r\n\nThe `save_python_data` function is used to save the provided file details as a YAML file and the instruction data as JSON files, as well as generating code graphs. It takes in the file details, a base name for the output files, and an output directory as arguments. It creates a subdirectory using `Path(output_dir) / relative_path.parent` to save all of these files within that directory, and then uses `write_file` to write each dictionary to its respective file type.\r\n\nThese objects are called within the main function to execute their specified functions. The objects include: \r\n\n1. Calls made in `convert_json_to_markdown`:\r\n- text.replace\r\n- Path(directory).rglob\r\n- read_file\r\n- dataset[0].keys\r\n- | '.join\r\n- len\r\n- escape_markdown\r\n- str\r\n- json_file.with_suffix\r\n- open\r\n- file.write\r\n- logging.error\r\n2. Calls made in `combine_json_files`:\r\n- read_file\r\n- Path(directory).rglob(\"*.{file_name}\")\r\n- remove_duplicate_dataset_entries\r\n- write_file\r\n3. Calls made in `create_code_graph`:\r\n- nx.DiGraph()\r\n- nx.spring_layout\r\n- nx.draw\r\n- nx.draw_networkx_edge_labels\r\n- plt.figure\r\n- plt.savefig\r\n- plt.close\r\n4. Calls made in `save_python_data`:\r\n- Path(output_dir) / relative_path.parent\r\n- write_file "
    },
    {
        "instruction": "Calls made in `escape_markdown` in Python file: `py2dataset.save_output.py`?",
        "input": "def escape_markdown(text: str) -> str:\n    \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n    markdown_special_chars = '\\\\`*_{}[]()#+-.!'\n    for char in markdown_special_chars:\n        text = text.replace(char, f'\\\\{char}')\n    return text",
        "output": "Calls made in `escape_markdown`:text.replace\n\nPurpose and Significance:\nThe function `preserve_spacing()` in `convert_json_to_html` uses a hack to preserve the original spacing of each entry in a JSON file and escape all HTML characters for security purposes.\r\n\n`escape_markdown` function in `convert_json_to_markdown` is used to escape Markdown special characters in the provided text. This ensures that the output will be valid Markdown syntax.\r\n\nThe `remove_duplicate_dataset_entries()` function removes duplicate entries from a dataset based on two keys: 'instruction' and 'output'. It takes a list of dictionaries as input, where each dictionary has these two keys. The function returns a new list with only unique entries.\r\n\n`combine_json_files()` reads all JSON files in the specified directory, removes duplicates from them based on the 'instruction' and 'output' keys, and saves the combined data to an output file named 'instruct.json'. It also creates a training dataset by extracting the input code from purpose statements.\r\n\n`create_code_graph()` generates a directed graph using NetworkX library and saves it as a PNG image in the provided path, while `save_python_data()` creates and writes Python-specific file information in both JSON format for `details.json`, which is also generated, and YAML format for instructions under an output directory with subdirectories.\r\n\nOverall, these functions are used to read, write, and manipulate files in various formats, such as JSON, YAML, HTML, and Markdown. "
    },
    {
        "instruction": "Calls made in `combine_json_files` in Python file: `py2dataset.save_output.py`?",
        "input": "def combine_json_files(directory: str, html: bool=False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            try:\n                json_file_data = read_file(json_file)\n                combined_data.extend(json_file_data)\n                combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n                instruct_data = combined_data.copy()\n                purpose_data = [item for item in combined_data if item['instruction'].startswith('1) Describe the Purpose')]\n                code_output = []\n                for item in purpose_data:\n                    instruction = 'Define a Python code file that is described as follows:\\n' + item['output']\n                    code_output.append({'instruction': instruction, 'output': item['input']})\n                write_file(code_output, Path(directory) / 'training.json')\n            except Exception as e:\n                logging.info(f'Error processing: {json_file}: {e}')\n        write_file(combined_data, file_path)\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "Calls made in `combine_json_files`:logging.info, set, seen.add, result.append, Path, Path(directory).rglob, read_file, combined_data.extend, remove_duplicate_dataset_entries, combined_data.copy, item['instruction'].startswith, code_output.append, write_file, convert_json_to_html\n\nPurpose and Significance:\nThe `read_file` function is used to read the contents of a JSON or YAML file as a dictionary based on its input file path and format type.\r\n\nThe `write_file` function writes data as JSON or YAML file depending upon the format provided by the file name suffix, accepts two arguments: data and file path.\r\n\nThe `convert_json_to_html` function converts all JSON files within a given directory to HTML format while preserving spaces and tabs for the 'input' field in each dataset. It creates an HTML table from the contents of the JSON file.\r\n\nThe `combine_json_files` function is used to merge all JSON files located within the input directory and removes duplicate entries from it. This merged data is saved into a single `instruct.json` file which is then returned as the dataset for the instruct_list. The `html` parameter determines whether the output should be converted to HTML format or not.\r\n\nThe `create_code_graph` function is used to create a code graph of nodes and edges within the given directory path. This will use Networkx package and create directed graphs by generating Graph objects that contain node names as Python files, edge labels as inputs and outputs, and their connections.\r\n\nThe `save_python_data` function saves the details of the Python file as a YAML file, saves the instruction data as JSON files, generates code graphs, and saves them in the specified output directory. "
    },
    {
        "instruction": "Calls made in `remove_duplicate_dataset_entries` in Python file: `py2dataset.save_output.py`?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "Calls made in `remove_duplicate_dataset_entries`:set, seen.add, result.append\n\nPurpose and Significance:\nThe `remove_duplicate_dataset_entries` function is called inside the `combine_json_files` function to remove duplicates from a dataset based on the keys provided in the function's arguments (`key1` and `key2`). It uses a set to keep track of unique entries, which ensures that only one entry with the same values for both keys will be kept. The function iterates over each item in the input dataset, checks if the tuple `(item[key1], item[key2])` is not already in `seen`, and adds it to a new list called `result` if it's unique. It returns this list without duplicates.\r\n\nThe 'Calls made in `remove_duplicate_dataset_entries`' explanation provided above does not accurately reflect the function's implementation, as it only mentions the `set`, `add`, and `append` methods of Python sets and lists respectively. The actual calls made are `seen.add((item[key1], item[key2])` (adding a tuple to the set) and `result.append(item)` (appending an item to the list). "
    },
    {
        "instruction": "Calls made in `create_code_graph` in Python file: `py2dataset.save_output.py`?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)\n    plt.close()",
        "output": "Calls made in `create_code_graph`:nx.DiGraph, G.add_nodes_from, G.add_edge, edge.items, plt.figure, nx.spring_layout, nx.draw, G.edges, label.append, , .join, \\\\n'.join, nx.draw_networkx_edge_labels, plt.savefig, plt.close\n\nPurpose and Significance:\nThe purpose of `read_file` is to read the contents of a JSON or YAML file as a dictionary.\r\n\nThe significance of `read_file` function is that it accepts a file path and reads the contents of the given JSON or YAML file and returns it as a dictionary. \r\n\nThe purpose of `write_file` is to write a dictionary to a JSON or YAML file.\r\n\nThe significance of `write_file` function is that it takes in a dictionary and a file path, opens the file with the given suffix (either .json or .yaml) and writes the contents of the dictionary to the file. \r\n\nThe purpose of `convert_json_to_html` is to convert JSON files within a given directory to HTML format.\r\n\nThe significance of `convert_json_to_html` function is that it reads all the JSON files in the specified directory, converts them into an HTML table format and saves each converted file with a .html extension. It also preserves spaces and tabs for the 'input' field in the provided JSON files.\r\n\nThe purpose of `convert_json_to_markdown` is to convert JSON files within a given directory to Markdown format.\r\n\nThe significance of `convert_json_to_markdown` function is that it reads all the JSON files in the specified directory, converts them into an HTML table format and saves each converted file with a .md extension. It also escapes special characters for Markdown formatting.\r\n\nThe purpose of `combine_json_files` is to combine all JSON files in the output directory into 'instruct.json' and remove duplicates from the combined JSON files.\r\n\nThe significance of `combine_json_files` function is that it takes a directory where the JSON files are located, combines them together to form a single file named \"instruct.json\", removes any duplicate entries based on the \"instruction\" and \"output\" keys in each dictionary entry, and returns a dictionary containing the 'instruct_list' datasets.\r\n\nThe purpose of `create_code_graph` is to generate code graphs based on the provided file details.\r\n\nThe significance of `create_code_graph` function is that it takes in details of a Python file, a base name, and an output directory as arguments. It generates code graphs using NetworkX library and saves them as PNG images in the specified output directory.\r\n\nThe purpose of `save_python_data` is to save the details of a Python file as a YAML file, save the instruction data as JSON files, and generate code graphs.\r\n\nThe significance of `save_python_data` function is that it takes in details of a Python file, a base name, and an output directory as arguments. It saves the file's information (using yaml), combines and removes any duplicate entries in \"instruct.json\" files, generates code graphs using NetworkX library, and saves them as PNG images in the specified output directory. "
    },
    {
        "instruction": "Calls made in `save_python_data` in Python file: `py2dataset.save_output.py`?",
        "input": "def save_python_data(file_details: dict, instruct_list: list, base_name: str, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f'Error creating graph for {base_name}: {e}')",
        "output": "Calls made in `save_python_data`:Path, output_subdir.mkdir, zip, write_file, create_code_graph, logging.info\n\nPurpose and Significance:\nThe `read_file` function is used to read a JSON or YAML file and return its contents as a dictionary. It takes in a `file_path` argument which is the path to the file that needs to be read and returns the content of the file as a dictionary.\r\n\nThe `write_file` function is used to write a dictionary to a JSON or YAML file. It takes in two arguments - `data`, which is the dictionary to be written, and `file_path` which is the path where it will be saved as either JSON or YAML file depending on the file type specified in the input path.\r\n\nThe `convert_json_to_html` function reads JSON files within a given directory and saves each converted file with a `.html` extension while preserving the spacing and tabs for the 'input' field. It loops through all the JSON files, reads them as dictionaries using the `read_file` function, then creates an HTML table structure for each dictionary.\r\n\nThe `combine_json_files` function accepts a directory path as input and merges all JSON files in that directory. It also removes duplicates from the combined JSON files by checking if there are any duplicate entries based on two keys - 'instruction' and 'output'. Finally, it saves the merged data to 'instruct.json' file and converts them into HTML format using `convert_json_to_html` function.\r\n\nThe `create_code_graph` function is used to generate code graphs based on the provided file details. It takes in three arguments - `file_details`, which contains information about the Python file, `base_name` which is a string that will be used as the base name for saving the graph, and `output_subdir` which is the directory where the graph will be saved.\r\n\nThe `save_python_data` function is used to save details of a Python file in YAML format and generate instruction data in JSON files. It takes three arguments - `file_details`, which contains information about the Python file, `instruct_list` which is the list of instructions extracted from the Python file, `base_name` which is a string that will be used as the base name for saving the graph and output files, and `output_dir` which is the directory where all the output files will be saved.\r\n\nThe function first creates an output subdirectory using `Path.mkdir()` method and then saves the file details in YAML format using `write_file`. It also generates code graphs using `create_code_graph` function and saves them as PNG images in the specified output directory. "
    },
    {
        "instruction": "Variables defined in `read_file` in Python file: `py2dataset.save_output.py`?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}",
        "output": "Variables defined in `read_file`:file_type\n\nPurpose and Significance:\nThe `read_file` function is a utility function that reads the contents of a JSON or YAML file as a dictionary. It takes a file path as an argument and returns the contents of the file as a dictionary.\r\n\nThe `write_file` function writes data to a specified JSON or YAML file based on its file type in the file extension, either a json file with indent level set at 4 for indented format. In case the given file is not a valid JSON or YAML file, it returns an empty dictionary.\r\n\nThe `convert_json_to_html` function converts all JSON files within a specified directory to HTML format and saves them with a `.html` extension. It preserves spacing and tabs for the 'input' field in each dataset.\r\n\nThe `combine_json_files` function merges all JSON files in a given directory, removes duplicates from the combined data, and writes it to an output file called 'instruct.json'. Additionally, it creates an HTML format version of each input dataset. If specified with `html=True`, it converts all merged datasets into markdown format.\r\n\nThe `create_code_graph` function generates code graphs based on the provided file details and saves them as PNG images in a given output directory. It uses NetworkX library to create directed graphs from the input data, which is then drawn using Matplotlib.\r\n\nThe `save_python_data` function saves the details of a Python file as a YAML file, saves the instruction data as JSON files, and generates code graphs based on the provided file details. "
    },
    {
        "instruction": "Variables defined in `write_file` in Python file: `py2dataset.save_output.py`?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "Variables defined in `write_file`:file_type\n\nPurpose and Significance:\nThe `write_file` function is a utility function that takes a dictionary and a file path as arguments, reads the contents of a JSON or YAML file based on the provided file type (json or yaml), and writes the dictionary to the specified file. "
    },
    {
        "instruction": "Variables defined in `convert_json_to_html` in Python file: `py2dataset.save_output.py`?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')",
        "output": "Variables defined in `convert_json_to_html`:html_rows, html_file_path, dataset, column_count, row_parts, html_content, value, column_width\n\nPurpose and Significance:\nVariables defined in convert_json_to_html:\n\n- **html_rows** (List[str]): a list of rows generated by converting each dictionary in the `dataset` to HTML. \n- **html_file_path** (Path): pathlib object that represents the HTML file that will be written with the converted JSON data.\n- **dataset** (Dict): The contents of the JSON file read using `read_file()` function.\n- **column_count** (int): number of columns in the dataset, used to calculate the width for each column.\n- **row_parts** (List[str]): a list that will be used to store the HTML content for each row.\n- **value** (str): the value of a key in the `entry` dictionary, which is then escaped and preserved with spaces and tabs.\n- **column_width** (float): the width for each column based on the number of columns in the dataset. "
    },
    {
        "instruction": "Variables defined in `convert_json_to_markdown` in Python file: `py2dataset.save_output.py`?",
        "input": "def convert_json_to_markdown(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to Markdown format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def escape_markdown(text: str) -> str:\n        \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n        markdown_special_chars = '\\\\`*_{}[]()#+-.!'\n        for char in markdown_special_chars:\n            text = text.replace(char, f'\\\\{char}')\n        return text\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        markdown_content = '# Data Report\\n\\n'\n        headers = dataset[0].keys()\n        markdown_content += '| ' + ' | '.join(headers) + ' |\\n'\n        markdown_content += '| ' + ' | '.join(['---'] * len(headers)) + ' |\\n'\n        for entry in dataset:\n            row = '| ' + ' | '.join((escape_markdown(str(entry[key])) for key in headers)) + ' |\\n'\n            markdown_content += row\n        markdown_file_path = json_file.with_suffix('.md')\n        try:\n            with open(markdown_file_path, 'w', encoding='utf-8') as file:\n                file.write(markdown_content)\n        except Exception as e:\n            logging.error(f'Failed to save Markdown file {markdown_file_path}: {e}')",
        "output": "Variables defined in `convert_json_to_markdown`:headers, text, markdown_file_path, row, dataset, markdown_content, markdown_special_chars\n\nPurpose and Significance:\nThe function `convert_json_to_markdown` takes a directory as an argument and converts all JSON files in that directory to Markdown format. It first defines the headers of the table by reading the keys of the dictionary for each file and adds them as a row of headers. Then it loops through the list of dictionaries for each entry and writes their key-value pairs into rows of the table. The function then opens a new markdown file with the same name as the JSON file, appends all the rows to it, and saves it in the specified directory.\r\n\nVariables defined in `convert_json_to_markdown`:\r\n1. headers: A list of dictionary keys that will be used as headers for the Markdown table.\r\n2. text: The current line of text being processed.\r\n3. markdown_file_path: The path to the Markdown file that is being written to.\r\n4. row: A string representing a single row in the Markdown table.\r\n5. dataset: A list of dictionaries containing the data from each JSON file.\r\n6. markdown_content: A string containing all the rows of the Markdown table.\r\n7. markdown_special_chars: A string containing special characters that need to be escaped in Markdown format. "
    },
    {
        "instruction": "Variables defined in `escape_markdown` in Python file: `py2dataset.save_output.py`?",
        "input": "def escape_markdown(text: str) -> str:\n    \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n    markdown_special_chars = '\\\\`*_{}[]()#+-.!'\n    for char in markdown_special_chars:\n        text = text.replace(char, f'\\\\{char}')\n    return text",
        "output": "Variables defined in `escape_markdown`:markdown_special_chars, text\n\nPurpose and Significance:\nThe `escape_markdown` function is a helper function used by the `convert_json_to_markdown` function to escape special characters in Markdown format. It takes a string as input and returns an escaped version of that string so it can be displayed correctly as markdown.\r\n\nFor example: `str1 = \"*text*\"` would be escaped using `escape_markdown(str1)` to `\"\\*text\\*\"`. "
    },
    {
        "instruction": "Variables defined in `combine_json_files` in Python file: `py2dataset.save_output.py`?",
        "input": "def combine_json_files(directory: str, html: bool=False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            try:\n                json_file_data = read_file(json_file)\n                combined_data.extend(json_file_data)\n                combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n                instruct_data = combined_data.copy()\n                purpose_data = [item for item in combined_data if item['instruction'].startswith('1) Describe the Purpose')]\n                code_output = []\n                for item in purpose_data:\n                    instruction = 'Define a Python code file that is described as follows:\\n' + item['output']\n                    code_output.append({'instruction': instruction, 'output': item['input']})\n                write_file(code_output, Path(directory) / 'training.json')\n            except Exception as e:\n                logging.info(f'Error processing: {json_file}: {e}')\n        write_file(combined_data, file_path)\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "Variables defined in `combine_json_files`:result, purpose_data, file_path, seen, combined_data, instruction, instruct_data, code_output, json_file_data\n\nPurpose and Significance:\nThe `read_file` function is a utility function that reads the contents of a JSON or YAML file as a dictionary based on its given file path. It uses `json.load` to read JSON files and `yaml.load` to read YAML files.\r\n\nThe `write_file` function is a utility function that writes a dictionary to a JSON or YAML file based on the provided file path. It uses `json.dump` to write JSON files and `yaml.SafeDumper` to write YAML files.\r\n\nThe `convert_json_to_html` function converts JSON files within a given directory to HTML format, preserving spacing and tabs for the 'input' field while maintaining code format with a HTML file of same name and same contents but having the extension as .html instead.\r\n\nThe `combine_json_files` function merges all JSON files in the provided directory, removes duplicates from the combined JSON files, writes them to an output file named 'instruct.json', and returns the merged data in a dictionary format. It also creates Markdown tables from these files by converting each dataset into a separate markdown table and saves it with a .md extension.\r\n\nThe `create_code_graph` function generates code graphs based on the provided file details, saves them as PNG images in the specified output directory using NetworkX library. It accepts a dictionary of file details, a base name for the graph image filename, and an output directory as arguments.\r\n\nThe `save_python_data` function saves the details of a Python file as a YAML file, saves the instruction data as JSON files, generates code graphs, and then writes these data to specified directories based on provided information. "
    },
    {
        "instruction": "Variables defined in `remove_duplicate_dataset_entries` in Python file: `py2dataset.save_output.py`?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "Variables defined in `remove_duplicate_dataset_entries`:result, seen\n\nPurpose and Significance:\nIn this code block, two new objects are being created and assigned values to them. \n\n1. `seen`: This is a set object which will store all the unique combinations of `key1` and `key2`. It is used to keep track of the unique entries in the dataset.\n2. `result`: This is an empty list that will be populated with non-duplicate data from the `combined_data`. It is a variable created for storing the combined dataset after removing duplicates.\r\n\nBoth of these variables are local to the `remove_duplicate_dataset_entries` function and are used to remove duplicate entries from the provided dataset based on the given keys. "
    },
    {
        "instruction": "Variables defined in `create_code_graph` in Python file: `py2dataset.save_output.py`?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)\n    plt.close()",
        "output": "Variables defined in `create_code_graph`:edge_labels, pos, graph_type, G, output_file, label\n\nPurpose and Significance:\nThe variables defined in the `create_code_graph` function are as follows:\r\n\n- edge_labels: This is a dictionary that will be used to store the labels for each edge in the directed graph. The keys of this dictionary are tuples containing the source and target nodes, and the values are the edge labels.\r\n- pos: A dictionary generated by Networkx's spring_layout function which specifies the positions of each node on the graph. It is used to specify where to draw each node in the graph.\r\n- graph_type: The type of graph that needs to be created, either 'entire_code_graph', 'function_graph', or 'class_graph'.\r\n- output_file: A Path object representing the file path where the graph will be saved as a PNG image.\r\n- label: This is a list that contains all the labels for each edge in the graph. It is used to display the input and return values of each function or method in the graph.\r\n\nThe `create_code_graph` function takes three arguments - file_details, base_name, and output_subdir. These are explained below:\r\n- **file_details**: A dictionary containing all the details extracted from the Python file. It includes the following keys: 'file_info', 'functions', 'classes', 'imports', 'comments', and 'docstrings'. The 'file_info' key contains information about the file such as its name, path, size, and a list of nodes in the graph.\r\n- **base_name**: A string representing the base name of the output files. This is used to create unique names for each file that will be saved.\r\n- **output_subdir**: A Path object representing the subdirectory where the output files will be saved. The function creates a new directory with this name if it doesn't exist, and saves all the required files in this directory. "
    },
    {
        "instruction": "Variables defined in `save_python_data` in Python file: `py2dataset.save_output.py`?",
        "input": "def save_python_data(file_details: dict, instruct_list: list, base_name: str, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f'Error creating graph for {base_name}: {e}')",
        "output": "Variables defined in `save_python_data`:file_names, contents, output_subdir\n\nPurpose and Significance:\nThe `read_file`, `write_file`, `convert_json_to_html`, `combine_json_files`, and `create_code_graph` functions are utility functions for reading the input and saving the output of the py2dataset script. The `save_python_data` function is responsible for saving Python file details as a YAML file, the instruction data as JSON files, and generating code graphs based on the provided file details. "
    },
    {
        "instruction": "Returns from `read_file` in Python file: `py2dataset.save_output.py`?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}",
        "output": "Returns from `read_file`:{}, json.load(f), yaml.load(f, Loader=yaml.SafeLoader)\n\nPurpose and Significance:\n## Context Explanation:Here, a custom YAML (which follows specific conventions or specific formats for data handling) and JSON (JavaScript Object Notation) files are used to store and retrieve data in a structured way. The `read_file` function is used to read the contents of a file based on its extension i.e., if it's a YAML or JSON file, and returns the content as a dictionary. The `write_file` function is used to write a dictionary to a file in either JSON or YAML format. The `convert_json_to_html` function converts all JSON files within a given directory to HTML format while preserving spacing and tabs for the 'input' field, and the `combine_json_files` function merges all JSON files in the specified directory, removes duplicates from them, and saves them as 'instruct.json'. The `create_code_graph` function is used to create a networkx directed graph by first reading an external YAML or JSON file with required specifications about input variables/arguments for different nodes (functions), output variables of those functions, and their relationships, and then creates a visual representation of the code. Finally, the `save_python_data` function saves details of a Python file as a YAML file, saves instruction data in 'instruct.json' files, and generates and saves code graphs based on the provided file details.\r\n\n### Explanation:\r\n- The `read_file` function accepts a file path as an argument and returns its contents as a dictionary. It reads the content of the file depending upon its extension i.e., if it's a YAML or JSON file, and returns the content as a dictionary.\r\n- The `write_file` function accepts a dictionary and a file path as arguments and writes the dictionary to a file in either JSON or YAML format.\r\n- The `convert_json_to_html` function converts all JSON files within a given directory to HTML format while preserving spacing and tabs for the 'input' field. It reads each JSON file, generates an HTML table from its content, and saves it as an HTML file with `.html` extension.\r\n- The `combine_json_files` function accepts a directory path as an argument and returns a dictionary containing the `instruct_list` datasets after merging all JSON files in that directory, removing duplicates based on the provided keys. It also generates Markdown format of the same data using the `convert_json_to_markdown` function.\r\n- The `create_code_graph` function accepts details of a Python file, a base name, and an output directory as arguments, and generates code graphs based on the provided file details by first reading an external YAML or JSON file with required specifications about input variables/arguments for different nodes (functions), output variables of those functions, and their relationships. Then, it creates a visual representation of the code using networkx library and saves them as PNG images in the specified output directory.\r\n- The `save_python_data` function accepts details of a Python file, a base name, and an output directory as arguments, and saves the details of the Python file as a YAML file. It also saves instruction data in 'instruct.json' files and generates code graphs using the `create_code_graph` function.\r\n\n- The `save_python_data` function is used to save Python file details as a YAML file, the instruction data as a JSON file, and code graphs. It creates an output subdirectory based on the relative path of the input file and saves the files in that directory. "
    },
    {
        "instruction": "Returns from `convert_json_to_html` in Python file: `py2dataset.save_output.py`?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        html_rows = []\n        for entry in dataset:\n            row_parts = ['<tr>']\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                row_parts.append(f'<td>{value}</td>')\n            row_parts.append('</tr>')\n            html_rows.append(''.join(row_parts))\n        html_content += ''.join(html_rows)\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')",
        "output": "Returns from `convert_json_to_html`:text.replace(, &nbsp;').replace('\\\\t, &nbsp;' * tab_width)\n\nPurpose and Significance:\nThe function `convert_json_to_html()` converts JSON files within a given directory to HTML format. It replaces all spaces with `&nbsp;` and tabs with the specified number of `&nbsp;`s for proper spacing in the HTML file. The function takes a directory as an argument, reads each JSON file within that directory, creates an HTML table with headers from the first entry in the dataset, and writes the HTML content to a new file with a `.html` extension.\r\n\nThe `convert_json_to_markdown()` function converts JSON files within a given directory to Markdown format. It escapes special characters in the provided text using backslashes and creates a table with headers from the first entry in the dataset, then writes the Markdown content to a new file with a `.md` extension.\r\n\nThe `combine_json_files()` function combines all JSON files in the output directory into 'instruct.json' and removes duplicates based on the provided keys. It also saves the combined data as an HTML file if specified.\r\n\nThe `create_code_graph()` function generates code graphs based on the provided file details, saves them as PNG images in the specified output directory, and returns a dictionary containing the 'instruct_list' datasets.\r\n\nThe `save_python_data()` function saves the details of a Python file as a YAML file, saves the instruction data as JSON files, generates code graphs, and saves them to the specified output directory. "
    },
    {
        "instruction": "Returns from `preserve_spacing` in Python file: `py2dataset.save_output.py`?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "Returns from `preserve_spacing`:text.replace(, &nbsp;').replace('\\\\t, &nbsp;' * tab_width)\n\nPurpose and Significance:\nThe `preserve_spacing` function is a helper function used by the `convert_json_to_html` function to replace spaces and tabs with non-breaking spaces in HTML format. It takes a string as input and returns the modified string. The modified string can then be saved into an HTML file without the layout of the table being altered when converted from JSON format to HTML.\n'{req04, combine_json_files}': \"This function combines all JSON files in the provided directory, removes duplicates based on the 'instruction' and 'output' keys, and saves the combined data as a new file named 'instruct.json'. It also generates an HTML representation of the dataset using the `convert_json_to_html` function.\"\n'{req05, create_code_graph}': \"This function takes in the extracted details from a Python file, a base name for the output files, and an output directory to save the code graphs. It generates code graphs based on the provided file details using NetworkX library and saves them as PNG images.\"\n'{req06, save_python_data}': \"This function takes in the extracted details from a Python file, the base name for the output files, and an output directory to save the YAML file with the extracted data and the instruction dataset. It also generates code graphs using `create_code_graph` and saves them as PNG images.\" "
    },
    {
        "instruction": "Returns from `convert_json_to_markdown` in Python file: `py2dataset.save_output.py`?",
        "input": "def convert_json_to_markdown(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to Markdown format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def escape_markdown(text: str) -> str:\n        \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n        markdown_special_chars = '\\\\`*_{}[]()#+-.!'\n        for char in markdown_special_chars:\n            text = text.replace(char, f'\\\\{char}')\n        return text\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        markdown_content = '# Data Report\\n\\n'\n        headers = dataset[0].keys()\n        markdown_content += '| ' + ' | '.join(headers) + ' |\\n'\n        markdown_content += '| ' + ' | '.join(['---'] * len(headers)) + ' |\\n'\n        for entry in dataset:\n            row = '| ' + ' | '.join((escape_markdown(str(entry[key])) for key in headers)) + ' |\\n'\n            markdown_content += row\n        markdown_file_path = json_file.with_suffix('.md')\n        try:\n            with open(markdown_file_path, 'w', encoding='utf-8') as file:\n                file.write(markdown_content)\n        except Exception as e:\n            logging.error(f'Failed to save Markdown file {markdown_file_path}: {e}')",
        "output": "Returns from `convert_json_to_markdown`:text\n\nPurpose and Significance:\nThe `convert_json_to_html` function takes a directory as an argument, reads all JSON files in that directory, converts them to HTML format, and saves each converted file with a .html extension. The `preserve_spacing` function is used to preserve spaces and tabs within the 'input' field while preserving spacing. \r\nThe `convert_json_to_markdown` function takes a directory as an argument, reads all JSON files in that directory, converts them to Markdown format, and saves each converted file with a .md extension.\r\nThe `combine_json_files` function accepts a directory path as an argument, merges all JSON files in the directory, removes duplicates from the combined JSON files, writes the merged data to 'instruct.json' files, and returns the 'instruct_list' datasets. \r\nThe `create_code_graph` function takes details of a Python file, a base name, and an output directory as arguments, generates code graphs based on the provided file details, and saves them as PNG images in the specified output directory.\r\nThe `save_python_data` function accepts details of a Python file, a base name, and an output directory as arguments, saves the details of the Python file as a YAML file, saves the instruction data as JSON files, generates code graphs, and saves them to the specified output directory. "
    },
    {
        "instruction": "Returns from `escape_markdown` in Python file: `py2dataset.save_output.py`?",
        "input": "def escape_markdown(text: str) -> str:\n    \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n    markdown_special_chars = '\\\\`*_{}[]()#+-.!'\n    for char in markdown_special_chars:\n        text = text.replace(char, f'\\\\{char}')\n    return text",
        "output": "Returns from `escape_markdown`:text\n\nPurpose and Significance:\nThe `escape_markdown` function is used to escape Markdown special characters in the provided text. It replaces all the special characters with a backslash to prevent them from being interpreted as Markdown syntax and causing formatting issues. "
    },
    {
        "instruction": "Returns from `combine_json_files` in Python file: `py2dataset.save_output.py`?",
        "input": "def combine_json_files(directory: str, html: bool=False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f'Combining JSON files in {directory}')\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            try:\n                json_file_data = read_file(json_file)\n                combined_data.extend(json_file_data)\n                combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n                instruct_data = combined_data.copy()\n                purpose_data = [item for item in combined_data if item['instruction'].startswith('1) Describe the Purpose')]\n                code_output = []\n                for item in purpose_data:\n                    instruction = 'Define a Python code file that is described as follows:\\n' + item['output']\n                    code_output.append({'instruction': instruction, 'output': item['input']})\n                write_file(code_output, Path(directory) / 'training.json')\n            except Exception as e:\n                logging.info(f'Error processing: {json_file}: {e}')\n        write_file(combined_data, file_path)\n    if html:\n        logging.info('Converting JSON files to HTML')\n        convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "Returns from `combine_json_files`:{'instruct_list': instruct_data}, result\n\nPurpose and Significance:\nThe function `combine_json_files` returns a dictionary containing the `instruct_list` dataset. The `instruct_list` is a list of dictionaries which stores instructions, output values, and code outputs in each dataset found in JSON files in a directory passed as an argument.\r\n\nIt removes any duplicates based on two keys: `instruction` and `output`. It then saves the combined data to a file named `instruct.json` in the same directory.\r\n\nThe function also converts all JSON files within the given directory to HTML format using the `convert_json_to_html` function, which preserves spacing and tabs for the 'input' field. "
    },
    {
        "instruction": "Returns from `remove_duplicate_dataset_entries` in Python file: `py2dataset.save_output.py`?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "Returns from `remove_duplicate_dataset_entries`:result\n\nPurpose and Significance:\nThe `remove_duplicate_dataset_entries` function is a helper function that removes duplicate entries from the provided dataset based on the keys `key1` and `key2`. It returns a new list of dictionaries without any duplicates. The `{'Returns from `remove_duplicate_dataset_entries`: 'result'}` object refers to the return value of this function, which is a list of dictionaries that does not contain any duplicate entries based on the provided keys.\r\n\nThe `convert_json_to_html` function converts JSON files within a given directory to HTML format. It reads each JSON file in the specified directory and creates an HTML table with the same data as the original JSON file, preserving spacing and tabs for the 'input' field. The resulting HTML file is saved with a `.html` extension.\r\n\nThe `combine_json_files` function accepts a directory path as input and merges all JSON files in that directory into one file named 'instruct.json'. It also removes any duplicates from the combined dataset based on the keys 'instruction' and 'output', and saves the resulting dataset to the same file.\r\n\nThe `create_code_graph` function generates code graphs based on the provided file details, saves them as PNG images in a specified output directory, and returns None. The graph type is hardcoded to be `'entire_code_graph'` for now.\r\n\nThe `save_python_data` function accepts details of a Python file, a base name, and an output directory as arguments. It saves the details of the Python file as a YAML file with the same name as the provided base name, saves the instruction data as JSON files named 'instruct.json', generates code graphs using `create_code_graph`, and saves them in the specified output directory.\r\n\nThese objects are used within these functions to help accomplish their respective tasks: "
    },
    {
        "instruction": "1) Describe the Purpose and Processing summary of Python file: `py2dataset.save_output.py`; 2) Summarize the Significance of applicable Function, Class, and Method; 3) Explain what each Input, Output, and Variable does in the code.",
        "input": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = (\n            100 / column_count\n        )  # Calculate the width for each column based on the number of columns\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []  # List to store each row's HTML content\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))  # Join all parts of the row and append to the list\n\n        # After the loop, join all rows\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef convert_json_to_markdown(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to Markdown format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def escape_markdown(text: str) -> str:\n        \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n        markdown_special_chars = \"\\\\`*_{}[]()#+-.!\"\n        for char in markdown_special_chars:\n            text = text.replace(char, f\"\\\\{char}\")\n        return text\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        markdown_content = \"# Data Report\\n\\n\"\n        # Create Markdown table headers\n        headers = dataset[0].keys()\n        markdown_content += \"| \" + \" | \".join(headers) + \" |\\n\"\n        markdown_content += \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n\n        # Create Markdown table rows\n        for entry in dataset:\n            row = \"| \" + \" | \".join(escape_markdown(str(entry[key])) for key in headers) + \" |\\n\"\n            markdown_content += row\n\n        markdown_file_path = json_file.with_suffix(\".md\")\n        try:\n            with open(markdown_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(markdown_content)\n        except Exception as e:\n            logging.error(f\"Failed to save Markdown file {markdown_file_path}: {e}\")\n            \n\ndef combine_json_files(directory: str, html: bool = False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    def remove_duplicate_dataset_entries(\n        dataset: List[Dict], key1: str, key2: str\n    ) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n\n    instruct_data = []\n    for file_name in [\"instruct.json\"]:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f\"*.{file_name}\"):\n            try:\n                json_file_data = read_file(json_file)\n                combined_data.extend(json_file_data)\n                combined_data = remove_duplicate_dataset_entries(\n                    combined_data, \"instruction\", \"output\"\n                )\n                instruct_data = combined_data.copy()\n                purpose_data = [\n                    item\n                    for item in combined_data\n                    if item[\"instruction\"].startswith(\"1) Describe the Purpose\")\n                ]\n\n                code_output = []\n                for item in purpose_data:\n                    instruction = (\n                        \"Define a Python code file that is described as follows:\\n\"\n                        + item[\"output\"]\n                    )\n                    code_output.append(\n                        {\"instruction\": instruction, \"output\": item[\"input\"]}\n                    )\n                write_file(code_output, Path(directory) / \"training.json\")\n            except Exception as e:\n                logging.info(f\"Error processing: {json_file}: {e}\")\n\n        write_file(combined_data, file_path)\n\n    # Save html / markdown file for each json file in the output directory\n    # convert_json_to_markdown(directory)\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n    \n    return {\"instruct_list\": instruct_data}\n    \n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = \"entire_code_graph\"\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n\n    # Create graphs, add nodes, and add edges\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n    # Draw graphs\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)  # Save the figure\n    plt.close()  # Close the figure\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, base_name: str, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # want output subdir to include the absolute path to the output dir + the directories between the output dir and the file\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n\n    # write instrunct.json files\n    file_names = [f\"{base_name}.instruct.json\", f\"{base_name}.details.yaml\"]\n    contents = [instruct_list, file_details]\n\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f\"Error creating graph for {base_name}: {e}\")\n\n```",
        "output": "The purpose of the provided `save_output` python file is to handle data from extracted code elements from different modules to perform some processing actions as outlined below.\n- Processes data and stores them in different formats such as JSON, YAML, HTML, and Markdown files. \n- Handles the conversion of JSON files into HTML format. \n- Combines all JSON files in a given directory and removes duplicates from it. \n- Generates code graphs based on provided file details. \n- Saves Python file details as YAML files. \n- Saves instruction data as JSON files and generates code graphs.\n\nIn terms of Significance:\n- The `read_file` function is responsible for reading a file with a specific extension (JSON or YAML) and returning its contents as a dictionary. It takes in the path to the file as an argument and returns the content of that file as a dictionary. \n- The `write_file` function is responsible for writing data into a JSON or YAML file. It takes in two arguments, the data to be written and the path to the file. \n- The `convert_json_to_html` function converts JSON files within a given directory to HTML format. It accepts a directory as an argument and saves each converted file with a .html extension while preserving spacing and tabs for the 'input' field. \n- The `combine_json_files` function merges all JSON files in the directory, removes duplicates from the combined JSON files, writes them to 'instruct.json', and converts the merged JSON files to HTML format. It returns the 'instruct_list' datasets. \n- The `create_code_graph` function generates code graphs based on the provided file details. It accepts details of a Python file, a base name, and an output directory as arguments. \n- The `save_python_data` function saves the details of a Python file as a YAML file and saves instruction data as JSON files. It also generates and saves code graphs. \n\nIn terms of Inputs:\n- The input for the `read_file` function is a Path object to identify which file should be read based on its extension. It will read in JSON or YAML file and return its contents as a dictionary. \n- The input for the `write_file` function is two arguments, data to be written and the path to the file where it will be saved. It also includes an optional parameter `indent=4` for indentation level in case of writing JSON files. \n- The `convert_json_to_html` function takes a directory as input to identify which files should be converted into HTML format. \n- The `combine_json_files` function takes the directory path where all the JSON files are located, it will read in each file and merge them while removing duplicates from the combined dataset based on the instruction and output key/values pairs.\n- The input for `create_code_graph` includes the file details along with base name, an output directory path where to save the graph image. \n- The `save_python_data` function takes in a dictionary of Python file details, a list of instructions, and the relative path to the Python file and output directory for saving the YAML file, instruction data as JSON files, and code graphs.\n\nIn terms of Outputs:\n- The output of the `read_file` function is a dictionary containing the contents of the file based on its extension (JSON or YAML). \n- The output of the `write_file` function is None since it only writes data to a file and does not return anything. \n- The output of the `convert_json_to_html` function will create HTML files from JSON files with preserved spacing and tabs for the 'input' field. \n- The output of the `combine_json_files` function is a dictionary containing the 'instruct_list' datasets after merging all JSON files in the directory and removing duplicates based on instruction and output keys/values.\n- The `create_code_graph` function saves graphs as PNG images to an output directory using the base name specified, along with some other file details such as nodes, edges, etc. \n- The output of the `save_python_data` function is None since it only writes data and does not return anything. It also generates code graphs based on provided file details. \n{\n    \"Code Elements\": {\n        \"Dependencies\": \"logging, yaml, networkx, json, html, pathlib, matplotlib.pyplot, typing\",\n        \"Functions defined\": \"read_file, write_file, convert_json_to_html, preserve_spacing, convert_json_to_markdown, escape_markdown, combine_json_files, remove_duplicate_dataset_entries, create_code_graph, save_python_data\",\n        \"`combine_json_files` Calls\": \"logging.info, set, seen.add, result.append, Path, Path(directory).rglob, read_file, combined_data.extend, remove_duplicate_dataset_entries, combined_data.copy, item['instruction'].startswith, code_output.append, write_file, convert_json_to_html\",\n        \"`combine_json_files` Inputs\": \"directory, html\",\n        \"`combine_json_files` Returns\": \"{'instruct_list': instruct_data}, result\",\n        \"`combine_json_files` Variables\": \"result, purpose_data, file_path, seen, combined_data, instruction, instruct_data, code_output, json_file_data\",\n        \"`convert_json_to_html` Calls\": \"text.replace(, &nbsp;').replace, text.replace, Path(directory).rglob, Path, read_file, len, dataset[0].keys, escape, str, preserve_spacing, value.replace, row_parts.append, html_rows.append, .join, json_file.with_suffix, open, file.write, logging.info\",\n        \"`convert_json_to_html` Inputs\": \"directory\",\n        \"`convert_json_to_html` Returns\": \"text.replace(, &nbsp;').replace('\\\\\\\\t, &nbsp;' * tab_width)\",\n        \"`convert_json_to_html` Variables\": \"html_rows, html_file_path, dataset, column_count, row_parts, html_content, value, column_width\",\n        \"`convert_json_to_markdown` Calls\": \"text.replace, Path(directory).rglob, Path, read_file, dataset[0].keys, | '.join, len, escape_markdown, str, json_file.with_suffix, open, file.write, logging.error\",\n        \"`convert_json_to_markdown` Inputs\": \"directory\",\n        \"`convert_json_to_markdown` Returns\": \"text\",\n        \"`convert_json_to_markdown` Variables\": \"headers, text, markdown_file_path, row, dataset, markdown_content, markdown_special_chars\",\n        \"`create_code_graph` Calls\": \"nx.DiGraph, G.add_nodes_from, G.add_edge, edge.items, plt.figure, nx.spring_layout, nx.draw, G.edges, label.append, , .join, \\\\\\\\n'.join, nx.draw_networkx_edge_labels, plt.savefig, plt.close\",\n        \"`create_code_graph` Inputs\": \"file_details, base_name, output_subdir\",\n        \"`create_code_graph` Variables\": \"edge_labels, pos, graph_type, G, output_file, label\",\n        \"`escape_markdown` Calls\": \"text.replace\",\n        \"`escape_markdown` Inputs\": \"text\",\n        \"`escape_markdown` Returns\": \"text\",\n        \"`escape_markdown` Variables\": \"markdown_special_chars, text\",\n        \"`preserve_spacing` Calls\": \"text.replace(, &nbsp;').replace, text.replace\",\n        \"`preserve_spacing` Inputs\": \"text, tab_width\",\n        \"`preserve_spacing` Returns\": \"text.replace(, &nbsp;').replace('\\\\\\\\t, &nbsp;' * tab_width)\",\n        \"`read_file` Calls\": \"file_path.open, json.load, yaml.load\",\n        \"`read_file` Inputs\": \"file_path\",\n        \"`read_file` Returns\": \"{}, json.load(f), yaml.load(f, Loader=yaml.SafeLoader)\",\n        \"`read_file` Variables\": \"file_type\",\n        \"`remove_duplicate_dataset_entries` Calls\": \"set, seen.add, result.append\",\n        \"`remove_duplicate_dataset_entries` Inputs\": \"dataset, key1, key2\",\n        \"`remove_duplicate_dataset_entries` Returns\": \"result\",\n        \"`remove_duplicate_dataset_entries` Variables\": \"result, seen\",\n        \"`save_python_data` Calls\": \"Path, output_subdir.mkdir, zip, write_file, create_code_graph, logging.info\",\n        \"`save_python_data` Inputs\": \"file_details, instruct_list, base_name, relative_path, output_dir\",\n        \"`save_python_data` Variables\": \"file_names, contents, output_subdir\",\n        \"`write_file` Calls\": \"file_path.open, json.dump, yaml.dump\",\n        \"`write_file` Inputs\": \"data, file_path\",\n        \"`write_file` Variables\": \"file_type\"\n    }\n}"
    }
]