[
    {
        "instruction": "Dependencies of Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count  # Calculate the width for each column based on the number of columns\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        for entry in dataset:\n            html_content += \"<tr>\"\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f\"<td>{value}</td>\"\n            html_content += \"</tr>\"\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')\n\n\ndef combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and \n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            # gen training datasets that contains purpose and graph data formatted for each item:\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                instruction = 'Define a Python code file that is described as follows:\\n'+ item['output']\n                code_output.append({'instruction': instruction, 'output': item['input']})\n            for item in graph_data:\n                instruction = 'Define the call code graph for this Python file:\\n' + item['input']\n                graph_output.append({'instruction': instruction, 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n\n        write_file(combined_data, file_path)\n\n    # Save html file for each json file in the output directory\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n\n    # Create graphs, add nodes, and add edges\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = edge['source'], edge['target']\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    # Draw graphs\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file) # Save the figure\n    plt.close()  # Close the figure\n\n\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join(part for part in relative_path.parts)\n\n    # write instrunct.json files\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f'Error creating graph for {base_name}: {e}')\n",
        "output": "networkx, matplotlib.pyplot, typing, json, html, pathlib, logging, yaml"
    },
    {
        "instruction": "Call code graph of Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count  # Calculate the width for each column based on the number of columns\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        for entry in dataset:\n            html_content += \"<tr>\"\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f\"<td>{value}</td>\"\n            html_content += \"</tr>\"\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')\n\n\ndef combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and \n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            # gen training datasets that contains purpose and graph data formatted for each item:\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                instruction = 'Define a Python code file that is described as follows:\\n'+ item['output']\n                code_output.append({'instruction': instruction, 'output': item['input']})\n            for item in graph_data:\n                instruction = 'Define the call code graph for this Python file:\\n' + item['input']\n                graph_output.append({'instruction': instruction, 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n\n        write_file(combined_data, file_path)\n\n    # Save html file for each json file in the output directory\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n\n    # Create graphs, add nodes, and add edges\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = edge['source'], edge['target']\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    # Draw graphs\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file) # Save the figure\n    plt.close()  # Close the figure\n\n\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join(part for part in relative_path.parts)\n\n    # write instrunct.json files\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f'Error creating graph for {base_name}: {e}')\n",
        "output": "{'nodes': ['read_file', 'write_file', 'convert_json_to_html', 'preserve_spacing', 'combine_json_files', 'remove_duplicate_dataset_entries', 'create_code_graph', 'save_python_data', 'file_path.open', 'json.load', 'yaml.load', 'json.dump', 'yaml.dump', \"text.replace(' ', '&nbsp;').replace\", 'text.replace', 'Path(directory).rglob', 'Path', 'len', 'dataset[0].keys', 'escape', 'str', 'value.replace', 'json_file.with_suffix', 'open', 'file.write', 'logging.info', 'set', 'seen.add', 'result.append', 'combined_data.extend', 'combined_data.copy', \"item['instruction'].startswith\", 'code_output.append', 'graph_output.append', 'nx.DiGraph', 'G.add_nodes_from', 'G.add_edge', 'edge.items', 'plt.figure', 'nx.spring_layout', 'nx.draw', 'G.edges', 'label.append', \"', '.join\", \"'\\\\n'.join\", 'nx.draw_networkx_edge_labels', 'plt.savefig', 'plt.close', 'output_subdir.mkdir', \"'.'.join\", 'zip'], 'edges': [{'source': 'read_file', 'target': 'file_path.open', 'target_inputs': []}, {'source': 'read_file', 'target': 'json.load', 'target_inputs': ['f']}, {'source': 'read_file', 'target': 'yaml.load', 'target_inputs': ['f']}, {'source': 'write_file', 'target': 'file_path.open', 'target_inputs': [\"'w'\"]}, {'source': 'write_file', 'target': 'json.dump', 'target_inputs': ['data', 'f']}, {'source': 'write_file', 'target': 'yaml.dump', 'target_inputs': ['data', 'f']}, {'source': 'convert_json_to_html', 'target': \"text.replace(' ', '&nbsp;').replace\", 'target_inputs': [\"'\\\\t'\", \"'&nbsp;' * tab_width\"]}, {'source': 'convert_json_to_html', 'target': 'text.replace', 'target_inputs': [\"' '\", \"'&nbsp;'\"]}, {'source': 'convert_json_to_html', 'target': 'Path(directory).rglob', 'target_inputs': [\"'*.json'\"]}, {'source': 'convert_json_to_html', 'target': 'Path', 'target_inputs': ['directory']}, {'source': 'convert_json_to_html', 'target': 'read_file', 'target_inputs': ['json_file'], 'target_returns': ['yaml.load(f, Loader=yaml.SafeLoader)', 'json.load(f)', '{}']}, {'source': 'convert_json_to_html', 'target': 'len', 'target_inputs': ['dataset[0].keys()']}, {'source': 'convert_json_to_html', 'target': 'dataset[0].keys', 'target_inputs': []}, {'source': 'convert_json_to_html', 'target': 'escape', 'target_inputs': ['str(entry[key])']}, {'source': 'convert_json_to_html', 'target': 'str', 'target_inputs': ['entry[key]']}, {'source': 'convert_json_to_html', 'target': 'preserve_spacing', 'target_inputs': ['value'], 'target_returns': [\"text.replace(' ', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)\"]}, {'source': 'convert_json_to_html', 'target': 'value.replace', 'target_inputs': [\"'\\\\n'\", \"'<br/>'\"]}, {'source': 'convert_json_to_html', 'target': 'json_file.with_suffix', 'target_inputs': [\"'.html'\"]}, {'source': 'convert_json_to_html', 'target': 'open', 'target_inputs': ['html_file_path', \"'w'\"]}, {'source': 'convert_json_to_html', 'target': 'file.write', 'target_inputs': ['html_content']}, {'source': 'convert_json_to_html', 'target': 'logging.info', 'target_inputs': [\"f'Failed saving: {html_file_path}'\"]}, {'source': 'preserve_spacing', 'target': \"text.replace(' ', '&nbsp;').replace\", 'target_inputs': [\"'\\\\t'\", \"'&nbsp;' * tab_width\"]}, {'source': 'preserve_spacing', 'target': 'text.replace', 'target_inputs': [\"' '\", \"'&nbsp;'\"]}, {'source': 'combine_json_files', 'target': 'set', 'target_inputs': []}, {'source': 'combine_json_files', 'target': 'seen.add', 'target_inputs': ['(item[key1], item[key2])']}, {'source': 'combine_json_files', 'target': 'result.append', 'target_inputs': ['item']}, {'source': 'combine_json_files', 'target': 'Path', 'target_inputs': ['directory']}, {'source': 'combine_json_files', 'target': 'Path(directory).rglob', 'target_inputs': [\"f'*.{file_name}'\"]}, {'source': 'combine_json_files', 'target': 'read_file', 'target_inputs': ['json_file'], 'target_returns': ['yaml.load(f, Loader=yaml.SafeLoader)', 'json.load(f)', '{}']}, {'source': 'combine_json_files', 'target': 'combined_data.extend', 'target_inputs': ['json_file_data']}, {'source': 'combine_json_files', 'target': 'remove_duplicate_dataset_entries', 'target_inputs': ['combined_data', \"'instruction'\", \"'output'\"], 'target_returns': ['result']}, {'source': 'combine_json_files', 'target': 'combined_data.copy', 'target_inputs': []}, {'source': 'combine_json_files', 'target': \"item['instruction'].startswith\", 'target_inputs': [\"'Call code graph'\"]}, {'source': 'combine_json_files', 'target': 'code_output.append', 'target_inputs': [\"{'instruction': instruction, 'output': item['input']}\"]}, {'source': 'combine_json_files', 'target': 'graph_output.append', 'target_inputs': [\"{'instruction': instruction, 'output': item['output']}\"]}, {'source': 'combine_json_files', 'target': 'write_file', 'target_inputs': ['combined_data', 'file_path'], 'target_returns': []}, {'source': 'combine_json_files', 'target': 'convert_json_to_html', 'target_inputs': ['directory'], 'target_returns': [\"text.replace(' ', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)\"]}, {'source': 'remove_duplicate_dataset_entries', 'target': 'set', 'target_inputs': []}, {'source': 'remove_duplicate_dataset_entries', 'target': 'seen.add', 'target_inputs': ['(item[key1], item[key2])']}, {'source': 'remove_duplicate_dataset_entries', 'target': 'result.append', 'target_inputs': ['item']}, {'source': 'create_code_graph', 'target': 'nx.DiGraph', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'G.add_nodes_from', 'target_inputs': [\"file_details['file_info'][graph_type]['nodes']\"]}, {'source': 'create_code_graph', 'target': 'G.add_edge', 'target_inputs': ['source', 'target']}, {'source': 'create_code_graph', 'target': 'edge.items', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'plt.figure', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'nx.spring_layout', 'target_inputs': ['G']}, {'source': 'create_code_graph', 'target': 'nx.draw', 'target_inputs': ['G', 'pos']}, {'source': 'create_code_graph', 'target': 'G.edges', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'label.append', 'target_inputs': ['f\"\\\\nReturns: {\\', \\'.join(edge[2][\\'target_returns\\'])}\"']}, {'source': 'create_code_graph', 'target': \"', '.join\", 'target_inputs': [\"edge[2]['target_returns']\"]}, {'source': 'create_code_graph', 'target': \"'\\\\n'.join\", 'target_inputs': ['label']}, {'source': 'create_code_graph', 'target': 'nx.draw_networkx_edge_labels', 'target_inputs': ['G', 'pos']}, {'source': 'create_code_graph', 'target': 'plt.savefig', 'target_inputs': ['output_file']}, {'source': 'create_code_graph', 'target': 'plt.close', 'target_inputs': []}, {'source': 'save_python_data', 'target': 'Path', 'target_inputs': ['output_dir']}, {'source': 'save_python_data', 'target': 'output_subdir.mkdir', 'target_inputs': []}, {'source': 'save_python_data', 'target': \"'.'.join\", 'target_inputs': ['(part for part in relative_path.parts)']}, {'source': 'save_python_data', 'target': 'zip', 'target_inputs': ['file_names', 'contents']}, {'source': 'save_python_data', 'target': 'write_file', 'target_inputs': ['content', 'output_subdir / file_name'], 'target_returns': []}, {'source': 'save_python_data', 'target': 'create_code_graph', 'target_inputs': ['file_details', 'base_name', 'output_subdir'], 'target_returns': []}, {'source': 'save_python_data', 'target': 'logging.info', 'target_inputs': [\"f'Error creating graph for {base_name}: {e}'\"]}]}"
    },
    {
        "instruction": "Functions defined in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count  # Calculate the width for each column based on the number of columns\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        for entry in dataset:\n            html_content += \"<tr>\"\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f\"<td>{value}</td>\"\n            html_content += \"</tr>\"\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')\n\n\ndef combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and \n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            # gen training datasets that contains purpose and graph data formatted for each item:\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                instruction = 'Define a Python code file that is described as follows:\\n'+ item['output']\n                code_output.append({'instruction': instruction, 'output': item['input']})\n            for item in graph_data:\n                instruction = 'Define the call code graph for this Python file:\\n' + item['input']\n                graph_output.append({'instruction': instruction, 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n\n        write_file(combined_data, file_path)\n\n    # Save html file for each json file in the output directory\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n\n    # Create graphs, add nodes, and add edges\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = edge['source'], edge['target']\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    # Draw graphs\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file) # Save the figure\n    plt.close()  # Close the figure\n\n\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join(part for part in relative_path.parts)\n\n    # write instrunct.json files\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f'Error creating graph for {base_name}: {e}')\n",
        "output": "create_code_graph, save_python_data, read_file, preserve_spacing, convert_json_to_html, write_file, remove_duplicate_dataset_entries, combine_json_files"
    },
    {
        "instruction": "Inputs to function: `read_file` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}",
        "output": "file_path"
    },
    {
        "instruction": "Inputs to function: `write_file` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "file_path, data"
    },
    {
        "instruction": "Inputs to function: `convert_json_to_html` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')",
        "output": "directory"
    },
    {
        "instruction": "Inputs to function: `preserve_spacing` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "tab_width, text"
    },
    {
        "instruction": "Inputs to function: `combine_json_files` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and \n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                instruction = 'Define a Python code file that is described as follows:\\n' + item['output']\n                code_output.append({'instruction': instruction, 'output': item['input']})\n            for item in graph_data:\n                instruction = 'Define the call code graph for this Python file:\\n' + item['input']\n                graph_output.append({'instruction': instruction, 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n        write_file(combined_data, file_path)\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "directory"
    },
    {
        "instruction": "Inputs to function: `remove_duplicate_dataset_entries` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "key2, key1, dataset"
    },
    {
        "instruction": "Inputs to function: `create_code_graph` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)\n    plt.close()",
        "output": "file_details, output_subdir, base_name"
    },
    {
        "instruction": "Inputs to function: `save_python_data` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join((part for part in relative_path.parts))\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f'Error creating graph for {base_name}: {e}')",
        "output": "file_details, output_dir, relative_path, instruct_list"
    },
    {
        "instruction": "Docstring of function: `read_file` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}",
        "output": "Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary."
    },
    {
        "instruction": "Docstring of function: `write_file` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None"
    },
    {
        "instruction": "Docstring of function: `convert_json_to_html` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')",
        "output": "Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None"
    },
    {
        "instruction": "Docstring of function: `preserve_spacing` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "Preserve spaces and tabs in the provided text."
    },
    {
        "instruction": "Docstring of function: `combine_json_files` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and \n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                instruction = 'Define a Python code file that is described as follows:\\n' + item['output']\n                code_output.append({'instruction': instruction, 'output': item['input']})\n            for item in graph_data:\n                instruction = 'Define the call code graph for this Python file:\\n' + item['input']\n                graph_output.append({'instruction': instruction, 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n        write_file(combined_data, file_path)\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "Combine all JSON files in the output directory into 'instruct.json', and \n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets."
    },
    {
        "instruction": "Docstring of function: `remove_duplicate_dataset_entries` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries."
    },
    {
        "instruction": "Docstring of function: `create_code_graph` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)\n    plt.close()",
        "output": "Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None"
    },
    {
        "instruction": "Docstring of function: `save_python_data` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join((part for part in relative_path.parts))\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f'Error creating graph for {base_name}: {e}')",
        "output": "Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None"
    },
    {
        "instruction": "Calls made in function: `read_file` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}",
        "output": "file_path.open, json.load, yaml.load"
    },
    {
        "instruction": "Calls made in function: `write_file` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "file_path.open, json.dump, yaml.dump"
    },
    {
        "instruction": "Calls made in function: `convert_json_to_html` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')",
        "output": "text.replace , nbsp.replace, Path, str, value.replace, json_file.with_suffix, read_file, Pathdirectory.rglob, escape, preserve_spacing, logging.info, open, file.write, len, text.replace, dataset0.keys"
    },
    {
        "instruction": "Calls made in function: `preserve_spacing` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "text.replace , nbsp.replace, text.replace"
    },
    {
        "instruction": "Calls made in function: `combine_json_files` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and \n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                instruction = 'Define a Python code file that is described as follows:\\n' + item['output']\n                code_output.append({'instruction': instruction, 'output': item['input']})\n            for item in graph_data:\n                instruction = 'Define the call code graph for this Python file:\\n' + item['input']\n                graph_output.append({'instruction': instruction, 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n        write_file(combined_data, file_path)\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "iteminstruction.startswith, set, combined_data.copy, Path, result.append, seen.add, read_file, Pathdirectory.rglob, code_output.append, convert_json_to_html, remove_duplicate_dataset_entries, combined_data.extend, write_file, graph_output.append"
    },
    {
        "instruction": "Calls made in function: `remove_duplicate_dataset_entries` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "set, seen.add, result.append"
    },
    {
        "instruction": "Calls made in function: `create_code_graph` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)\n    plt.close()",
        "output": ", label.append, .join, nx.draw, G.add_edge, plt.savefig, nx.DiGraph, plt.close, n.join, G.add_nodes_from, G.edges, edge.items, plt.figure, nx.spring_layout, nx.draw_networkx_edge_labels"
    },
    {
        "instruction": "Calls made in function: `save_python_data` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join((part for part in relative_path.parts))\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f'Error creating graph for {base_name}: {e}')",
        "output": "..join, create_code_graph, Path, output_subdir.mkdir, logging.info, write_file, zip"
    },
    {
        "instruction": "Variables defined in function: `read_file` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}",
        "output": "file_type"
    },
    {
        "instruction": "Variables defined in function: `write_file` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "file_type"
    },
    {
        "instruction": "Variables defined in function: `convert_json_to_html` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')",
        "output": "html_content, column_count, column_width, value, dataset, html_file_path"
    },
    {
        "instruction": "Variables defined in function: `combine_json_files` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and \n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                instruction = 'Define a Python code file that is described as follows:\\n' + item['output']\n                code_output.append({'instruction': instruction, 'output': item['input']})\n            for item in graph_data:\n                instruction = 'Define the call code graph for this Python file:\\n' + item['input']\n                graph_output.append({'instruction': instruction, 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n        write_file(combined_data, file_path)\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "instruction, code_graph_output, result, instruct_data, graph_data, graph_output, purpose_data, json_file_data, file_path, seen, combined_data, code_output"
    },
    {
        "instruction": "Variables defined in function: `remove_duplicate_dataset_entries` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "seen, result"
    },
    {
        "instruction": "Variables defined in function: `create_code_graph` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)\n    plt.close()",
        "output": "G, graph_type, pos, edge_labels, label, output_file"
    },
    {
        "instruction": "Variables defined in function: `save_python_data` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join((part for part in relative_path.parts))\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f'Error creating graph for {base_name}: {e}')",
        "output": "output_subdir, file_names, contents, base_name"
    },
    {
        "instruction": "Returned items from function: `read_file` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}",
        "output": ", json.loadf, Loaderyaml.SafeLoader, yaml.loadf"
    },
    {
        "instruction": "Returned items from function: `convert_json_to_html` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')",
        "output": "text.replace , nbsp  tab_width, nbsp.replacet"
    },
    {
        "instruction": "Returned items from function: `preserve_spacing` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "text.replace , nbsp  tab_width, nbsp.replacet"
    },
    {
        "instruction": "Returned items from function: `combine_json_files` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and \n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                instruction = 'Define a Python code file that is described as follows:\\n' + item['output']\n                code_output.append({'instruction': instruction, 'output': item['input']})\n            for item in graph_data:\n                instruction = 'Define the call code graph for this Python file:\\n' + item['input']\n                graph_output.append({'instruction': instruction, 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n        write_file(combined_data, file_path)\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "instruct_list: instruct_data, result"
    },
    {
        "instruction": "Returned items from function: `remove_duplicate_dataset_entries` in Python file: `py2dataset.save_py2dataset_output.py`?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "result"
    },
    {
        "instruction": "1) DESCRIBE the purpose and processing summary of Python file: `py2dataset.save_py2dataset_output.py`; 2) PROVIDE an itemized and detailed description of each applicable function, class, and method; 3) EXPLAIN what each input, output, and variable does in the code.",
        "input": "\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        if file_type == 'yaml':\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count  # Calculate the width for each column based on the number of columns\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        for entry in dataset:\n            html_content += \"<tr>\"\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f\"<td>{value}</td>\"\n            html_content += \"</tr>\"\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f'Failed saving: {html_file_path}')\n\n\ndef combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and \n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            # gen training datasets that contains purpose and graph data formatted for each item:\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                instruction = 'Define a Python code file that is described as follows:\\n'+ item['output']\n                code_output.append({'instruction': instruction, 'output': item['input']})\n            for item in graph_data:\n                instruction = 'Define the call code graph for this Python file:\\n' + item['input']\n                graph_output.append({'instruction': instruction, 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n\n        write_file(combined_data, file_path)\n\n    # Save html file for each json file in the output directory\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n\n    # Create graphs, add nodes, and add edges\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = edge['source'], edge['target']\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    # Draw graphs\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file) # Save the figure\n    plt.close()  # Close the figure\n\n\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join(part for part in relative_path.parts)\n\n    # write instrunct.json files\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f'Error creating graph for {base_name}: {e}')\n",
        "output": "This Python file provides utilities for reading files (JSON or YAML), converting JSON files to HTML format, merging JSON files into a single JSON file, creating code graphs based on provided file details, and saving Python data as YAML files. \n\nFunctions defined in `py2dataset.save_py2dataset_output.py`:\n1. `read_file` - reads the contents of a JSON or YAML file as a dictionary.\n   Input: file path (str)\n   Output: dictionary containing the contents of the file\n   Variables: file type, f (file object), json/yaml loader\n2. `write_file` - writes a dictionary to a JSON or YAML file. \n   Inputs: data (dict), file path (Path)\n   Output: None\n   Variables: file type, f (file object), json/yaml dumper\n3. `convert_json_to_html` - convert JSON files within a given directory to HTML format, save each converted file with a .html extension while preserving input's spacing and tabs for the 'input' field. \n   Input: directory (str)\n   Output: None\n   Variables: html_content (HTML formatted content), Path(), html_file_path (file path of the output HTML file)\n4. `combine_json_files` - merge all JSON files in a given directory, remove duplicates from the combined JSON files, and write them to 'instruct.json' while generating HTML versions for each input dataset.\n   Inputs: directory (str)\n   Outputs: `{'instruct_list': List[Dict]}` containing the merged datasets without duplicates\n   Variables: instruct_data (List[Dict]), seen (set), result (List[Dict]), json_file, combined_data (List[Dict])\n5. `create_code_graph` - generate code graphs based on provided file details and save them as PNG images in the specified output directory. \n   Inputs: file_details (dict), base_name (str), output_subdir (Path)\n   Output: None\n   Variables: G (Graph object), pos (node positions), edge_labels (edge labels for graph), output_file (file path of the PNG image)\n6. `save_python_data` - save details of a Python file as YAML, generate and save instruction data as JSON files, and create code graphs. \n   Inputs: file_details (dict), instruct_list (List[Dict]), relative_path (Path), output_dir (str)\n   Output: None\n   Variables: output_subdir (Path object), base_name (str), file_names, contents, graph_type, pos, edge_labels, label, output_file\n\nFunctions in `py2dataset.save_py2dataset_output.py` perform the following tasks:\n1. Read a JSON or YAML file and return its contents as a dictionary using `read_file`. The purpose of this function is to simplify file read operations for all files. This ensures consistency when handling different formats for reading data from files.\n2. Write a dictionary to a JSON or YAML file using `write_file` to ensure consistency in writing the output data to disk. \n3. Convert JSON files within a given directory to HTML format using `convert_json_to_html`. The purpose of this function is to simplify file conversion operations and maintain consistency in handling different formats for output files. \n4. Combine all JSON files in an output directory into 'instruct.json' and remove duplicates from the combined data using `combine_json_files`. This function also generates HTML versions of each input dataset, which is useful for displaying the data visually.\n5. Generate code graphs based on provided file details using `create_code_graph` and save them as PNG images in a specified output directory using `save_python_data`. The purpose of this function is to visualize the relationships between different functions in a Python script, which can help with understanding the flow of execution. \n6. Save Python file details as a YAML file and the instruction data as JSON files using `save_python_data`, which allows for easy modification and updating of the input data. This function also ensures consistency in handling different formats for output files."
    }
]