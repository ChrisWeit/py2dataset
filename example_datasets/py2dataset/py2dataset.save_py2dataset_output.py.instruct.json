[
    {
        "instruction": "Dependencies of Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List, Union\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        html_file = json_file.with_suffix('.html')\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count  # Calculate the width for each column based on the number of columns\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        for entry in dataset:\n            html_content += \"<tr>\"\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f\"<td>{value}</td>\"\n            html_content += \"</tr>\"\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix('.html')\n        try:   \n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))\n\n\ndef combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n   \n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            # gen training datasets that contains purpose and graph data formatted as follow for each item in the dataset:\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                code_output.append({'instruction': 'Define a Python code file that is described as follows:\\n'+ item['output'], 'output': item['input']})\n            for item in graph_data:\n                graph_output.append({'instruction': 'Define the call code graph for this Python file:\\n' + item['input'], 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n\n        write_file(combined_data, file_path)\n\n    # Save html file for each json file in the output directory\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n\n    # Create graphs, add nodes, and add edges\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = edge['source'], edge['target']\n        if source in G.nodes and target in G.nodes:\n           G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    # Draw graphs\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file) # Save the figure\n    plt.close()  # Close the figure\n\n\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join(part for part in relative_path.parts)\n\n    # write instrunct.json files\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except:\n        logging.info(f'Error creating graph for {base_name}')\n",
        "output": "os, typing, html, yaml, matplotlib.pyplot, json, logging, sys, pathlib, re, networkx"
    },
    {
        "instruction": "Call code graph of Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List, Union\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        html_file = json_file.with_suffix('.html')\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count  # Calculate the width for each column based on the number of columns\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        for entry in dataset:\n            html_content += \"<tr>\"\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f\"<td>{value}</td>\"\n            html_content += \"</tr>\"\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix('.html')\n        try:   \n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))\n\n\ndef combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n   \n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            # gen training datasets that contains purpose and graph data formatted as follow for each item in the dataset:\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                code_output.append({'instruction': 'Define a Python code file that is described as follows:\\n'+ item['output'], 'output': item['input']})\n            for item in graph_data:\n                graph_output.append({'instruction': 'Define the call code graph for this Python file:\\n' + item['input'], 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n\n        write_file(combined_data, file_path)\n\n    # Save html file for each json file in the output directory\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n\n    # Create graphs, add nodes, and add edges\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = edge['source'], edge['target']\n        if source in G.nodes and target in G.nodes:\n           G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    # Draw graphs\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file) # Save the figure\n    plt.close()  # Close the figure\n\n\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join(part for part in relative_path.parts)\n\n    # write instrunct.json files\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except:\n        logging.info(f'Error creating graph for {base_name}')\n",
        "output": "{'nodes': ['read_file', 'write_file', 'convert_json_to_html', 'preserve_spacing', 'combine_json_files', 'remove_duplicate_dataset_entries', 'create_code_graph', 'save_python_data', 'file_path.open', 'json.load', 'yaml.load', 'json.dump', 'yaml.dump', \"text.replace(' ', '&nbsp;').replace\", 'text.replace', 'Path(directory).rglob', 'Path', 'json_file.with_suffix', 'len', 'dataset[0].keys', 'escape', 'str', 'value.replace', 'open', 'file.write', 'logging.save', 'logging.info', 'set', 'seen.add', 'result.append', 'combined_data.extend', 'combined_data.copy', \"item['instruction'].startswith\", 'code_output.append', 'graph_output.append', 'nx.DiGraph', 'G.add_nodes_from', 'G.add_edge', 'edge.items', 'plt.figure', 'nx.spring_layout', 'nx.draw', 'G.edges', 'label.append', \"', '.join\", \"'\\\\n'.join\", 'nx.draw_networkx_edge_labels', 'plt.savefig', 'plt.close', 'output_subdir.mkdir', \"'.'.join\", 'zip'], 'edges': [{'source': 'read_file', 'target': 'file_path.open', 'target_inputs': []}, {'source': 'read_file', 'target': 'json.load', 'target_inputs': ['f']}, {'source': 'read_file', 'target': 'yaml.load', 'target_inputs': ['f']}, {'source': 'write_file', 'target': 'file_path.open', 'target_inputs': [\"'w'\"]}, {'source': 'write_file', 'target': 'json.dump', 'target_inputs': ['data', 'f']}, {'source': 'write_file', 'target': 'yaml.dump', 'target_inputs': ['data', 'f']}, {'source': 'convert_json_to_html', 'target': \"text.replace(' ', '&nbsp;').replace\", 'target_inputs': [\"'\\\\t'\", \"'&nbsp;' * tab_width\"]}, {'source': 'convert_json_to_html', 'target': 'text.replace', 'target_inputs': [\"' '\", \"'&nbsp;'\"]}, {'source': 'convert_json_to_html', 'target': 'Path(directory).rglob', 'target_inputs': [\"'*.json'\"]}, {'source': 'convert_json_to_html', 'target': 'Path', 'target_inputs': ['directory']}, {'source': 'convert_json_to_html', 'target': 'read_file', 'target_inputs': ['json_file'], 'target_returns': ['yaml.load(f)', 'json.load(f)']}, {'source': 'convert_json_to_html', 'target': 'json_file.with_suffix', 'target_inputs': [\"'.html'\"]}, {'source': 'convert_json_to_html', 'target': 'len', 'target_inputs': ['dataset[0].keys()']}, {'source': 'convert_json_to_html', 'target': 'dataset[0].keys', 'target_inputs': []}, {'source': 'convert_json_to_html', 'target': 'escape', 'target_inputs': ['str(entry[key])']}, {'source': 'convert_json_to_html', 'target': 'str', 'target_inputs': ['entry[key]']}, {'source': 'convert_json_to_html', 'target': 'preserve_spacing', 'target_inputs': ['value'], 'target_returns': [\"text.replace(' ', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)\"]}, {'source': 'convert_json_to_html', 'target': 'value.replace', 'target_inputs': [\"'\\\\n'\", \"'<br/>'\"]}, {'source': 'convert_json_to_html', 'target': 'open', 'target_inputs': ['html_file_path', \"'w'\"]}, {'source': 'convert_json_to_html', 'target': 'file.write', 'target_inputs': ['html_content']}, {'source': 'convert_json_to_html', 'target': 'logging.save', 'target_inputs': [\"logging.info(f'Failed saving: {html_file_path}')\"]}, {'source': 'convert_json_to_html', 'target': 'logging.info', 'target_inputs': [\"f'Failed saving: {html_file_path}'\"]}, {'source': 'preserve_spacing', 'target': \"text.replace(' ', '&nbsp;').replace\", 'target_inputs': [\"'\\\\t'\", \"'&nbsp;' * tab_width\"]}, {'source': 'preserve_spacing', 'target': 'text.replace', 'target_inputs': [\"' '\", \"'&nbsp;'\"]}, {'source': 'combine_json_files', 'target': 'set', 'target_inputs': []}, {'source': 'combine_json_files', 'target': 'seen.add', 'target_inputs': ['(item[key1], item[key2])']}, {'source': 'combine_json_files', 'target': 'result.append', 'target_inputs': ['item']}, {'source': 'combine_json_files', 'target': 'Path', 'target_inputs': ['directory']}, {'source': 'combine_json_files', 'target': 'Path(directory).rglob', 'target_inputs': [\"f'*.{file_name}'\"]}, {'source': 'combine_json_files', 'target': 'read_file', 'target_inputs': ['json_file'], 'target_returns': ['yaml.load(f)', 'json.load(f)']}, {'source': 'combine_json_files', 'target': 'combined_data.extend', 'target_inputs': ['json_file_data']}, {'source': 'combine_json_files', 'target': 'remove_duplicate_dataset_entries', 'target_inputs': ['combined_data', \"'instruction'\", \"'output'\"], 'target_returns': ['result']}, {'source': 'combine_json_files', 'target': 'combined_data.copy', 'target_inputs': []}, {'source': 'combine_json_files', 'target': \"item['instruction'].startswith\", 'target_inputs': [\"'Call code graph'\"]}, {'source': 'combine_json_files', 'target': 'code_output.append', 'target_inputs': [\"{'instruction': 'Define a Python code file that is described as follows:\\\\n' + item['output'], 'output': item['input']}\"]}, {'source': 'combine_json_files', 'target': 'graph_output.append', 'target_inputs': [\"{'instruction': 'Define the call code graph for this Python file:\\\\n' + item['input'], 'output': item['output']}\"]}, {'source': 'combine_json_files', 'target': 'write_file', 'target_inputs': ['combined_data', 'file_path'], 'target_returns': []}, {'source': 'combine_json_files', 'target': 'convert_json_to_html', 'target_inputs': ['directory'], 'target_returns': [\"text.replace(' ', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)\"]}, {'source': 'remove_duplicate_dataset_entries', 'target': 'set', 'target_inputs': []}, {'source': 'remove_duplicate_dataset_entries', 'target': 'seen.add', 'target_inputs': ['(item[key1], item[key2])']}, {'source': 'remove_duplicate_dataset_entries', 'target': 'result.append', 'target_inputs': ['item']}, {'source': 'create_code_graph', 'target': 'nx.DiGraph', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'G.add_nodes_from', 'target_inputs': [\"file_details['file_info'][graph_type]['nodes']\"]}, {'source': 'create_code_graph', 'target': 'G.add_edge', 'target_inputs': ['source', 'target']}, {'source': 'create_code_graph', 'target': 'edge.items', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'plt.figure', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'nx.spring_layout', 'target_inputs': ['G']}, {'source': 'create_code_graph', 'target': 'nx.draw', 'target_inputs': ['G', 'pos']}, {'source': 'create_code_graph', 'target': 'G.edges', 'target_inputs': []}, {'source': 'create_code_graph', 'target': 'label.append', 'target_inputs': ['f\"\\\\nReturns: {\\', \\'.join(edge[2][\\'target_returns\\'])}\"']}, {'source': 'create_code_graph', 'target': \"', '.join\", 'target_inputs': [\"edge[2]['target_returns']\"]}, {'source': 'create_code_graph', 'target': \"'\\\\n'.join\", 'target_inputs': ['label']}, {'source': 'create_code_graph', 'target': 'nx.draw_networkx_edge_labels', 'target_inputs': ['G', 'pos']}, {'source': 'create_code_graph', 'target': 'plt.savefig', 'target_inputs': ['output_file']}, {'source': 'create_code_graph', 'target': 'plt.close', 'target_inputs': []}, {'source': 'save_python_data', 'target': 'Path', 'target_inputs': ['output_dir']}, {'source': 'save_python_data', 'target': 'output_subdir.mkdir', 'target_inputs': []}, {'source': 'save_python_data', 'target': \"'.'.join\", 'target_inputs': ['(part for part in relative_path.parts)']}, {'source': 'save_python_data', 'target': 'zip', 'target_inputs': ['file_names', 'contents']}, {'source': 'save_python_data', 'target': 'write_file', 'target_inputs': ['content', 'output_subdir / file_name'], 'target_returns': []}, {'source': 'save_python_data', 'target': 'create_code_graph', 'target_inputs': ['file_details', 'base_name', 'output_subdir'], 'target_returns': []}, {'source': 'save_python_data', 'target': 'logging.info', 'target_inputs': [\"f'Error creating graph for {base_name}'\"]}]}"
    },
    {
        "instruction": "Functions defined in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List, Union\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        html_file = json_file.with_suffix('.html')\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count  # Calculate the width for each column based on the number of columns\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        for entry in dataset:\n            html_content += \"<tr>\"\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f\"<td>{value}</td>\"\n            html_content += \"</tr>\"\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix('.html')\n        try:   \n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))\n\n\ndef combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n   \n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            # gen training datasets that contains purpose and graph data formatted as follow for each item in the dataset:\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                code_output.append({'instruction': 'Define a Python code file that is described as follows:\\n'+ item['output'], 'output': item['input']})\n            for item in graph_data:\n                graph_output.append({'instruction': 'Define the call code graph for this Python file:\\n' + item['input'], 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n\n        write_file(combined_data, file_path)\n\n    # Save html file for each json file in the output directory\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n\n    # Create graphs, add nodes, and add edges\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = edge['source'], edge['target']\n        if source in G.nodes and target in G.nodes:\n           G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    # Draw graphs\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file) # Save the figure\n    plt.close()  # Close the figure\n\n\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join(part for part in relative_path.parts)\n\n    # write instrunct.json files\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except:\n        logging.info(f'Error creating graph for {base_name}')\n",
        "output": "preserve_spacing, convert_json_to_html, combine_json_files, remove_duplicate_dataset_entries, read_file, write_file, save_python_data, create_code_graph"
    },
    {
        "instruction": "Inputs to function: 'read_file' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "file_path"
    },
    {
        "instruction": "Inputs to function: 'write_file' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "file_path, data"
    },
    {
        "instruction": "Inputs to function: 'convert_json_to_html' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_file = json_file.with_suffix('.html')\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))",
        "output": "directory"
    },
    {
        "instruction": "Inputs to function: 'preserve_spacing' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "tab_width, text"
    },
    {
        "instruction": "Inputs to function: 'combine_json_files' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                code_output.append({'instruction': 'Define a Python code file that is described as follows:\\n' + item['output'], 'output': item['input']})\n            for item in graph_data:\n                graph_output.append({'instruction': 'Define the call code graph for this Python file:\\n' + item['input'], 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n        write_file(combined_data, file_path)\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "directory"
    },
    {
        "instruction": "Inputs to function: 'remove_duplicate_dataset_entries' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "key1, key2, dataset"
    },
    {
        "instruction": "Inputs to function: 'create_code_graph' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)\n    plt.close()",
        "output": "file_details, output_subdir, base_name"
    },
    {
        "instruction": "Inputs to function: 'save_python_data' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join((part for part in relative_path.parts))\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except:\n        logging.info(f'Error creating graph for {base_name}')",
        "output": "file_details, instruct_list, relative_path, output_dir"
    },
    {
        "instruction": "Docstring of function: 'read_file' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary."
    },
    {
        "instruction": "Docstring of function: 'write_file' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None"
    },
    {
        "instruction": "Docstring of function: 'convert_json_to_html' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_file = json_file.with_suffix('.html')\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))",
        "output": "Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None"
    },
    {
        "instruction": "Docstring of function: 'preserve_spacing' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "Preserve spaces and tabs in the provided text."
    },
    {
        "instruction": "Docstring of function: 'combine_json_files' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                code_output.append({'instruction': 'Define a Python code file that is described as follows:\\n' + item['output'], 'output': item['input']})\n            for item in graph_data:\n                graph_output.append({'instruction': 'Define the call code graph for this Python file:\\n' + item['input'], 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n        write_file(combined_data, file_path)\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "Combine all JSON files in the output directory into 'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets."
    },
    {
        "instruction": "Docstring of function: 'remove_duplicate_dataset_entries' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries."
    },
    {
        "instruction": "Docstring of function: 'create_code_graph' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)\n    plt.close()",
        "output": "Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None"
    },
    {
        "instruction": "Docstring of function: 'save_python_data' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join((part for part in relative_path.parts))\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except:\n        logging.info(f'Error creating graph for {base_name}')",
        "output": "Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None"
    },
    {
        "instruction": "Calls made in function: 'read_file' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "file_path.open, json.load, yaml.load"
    },
    {
        "instruction": "Calls made in function: 'write_file' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "json.dump, yaml.dump, file_path.open"
    },
    {
        "instruction": "Calls made in function: 'convert_json_to_html' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_file = json_file.with_suffix('.html')\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))",
        "output": "Path, preserve_spacing, Pathdirectory.rglob, open, text.replace , escape, str, text.replace, value.replace, nbsp.replace, json_file.with_suffix, len, file.write, logging.save, logging.info, read_file, dataset0.keys"
    },
    {
        "instruction": "Calls made in function: 'preserve_spacing' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "text.replace, nbsp.replace, text.replace"
    },
    {
        "instruction": "Calls made in function: 'combine_json_files' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                code_output.append({'instruction': 'Define a Python code file that is described as follows:\\n' + item['output'], 'output': item['input']})\n            for item in graph_data:\n                graph_output.append({'instruction': 'Define the call code graph for this Python file:\\n' + item['input'], 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n        write_file(combined_data, file_path)\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "Path, combined_data.copy, Pathdirectory.rglob, seen.add, combined_data.extend, convert_json_to_html, result.append, remove_duplicate_dataset_entries, iteminstruction.startswith, read_file, graph_output.append, write_file, code_output.append, set"
    },
    {
        "instruction": "Calls made in function: 'remove_duplicate_dataset_entries' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "result.append, seen.add, set"
    },
    {
        "instruction": "Calls made in function: 'create_code_graph' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)\n    plt.close()",
        "output": ", edge.items, nx.DiGraph, plt.close, G.add_nodes_from, nx.draw_networkx_edge_labels, .join, plt.figure, plt.savefig, n.join, nx.draw, nx.spring_layout, label.append, G.add_edge, G.edges"
    },
    {
        "instruction": "Calls made in function: 'save_python_data' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join((part for part in relative_path.parts))\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except:\n        logging.info(f'Error creating graph for {base_name}')",
        "output": "Path, output_subdir.mkdir, zip, logging.info, ..join, write_file, create_code_graph"
    },
    {
        "instruction": "Variables defined in function: 'read_file' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "file_type"
    },
    {
        "instruction": "Variables defined in function: 'write_file' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "file_type"
    },
    {
        "instruction": "Variables defined in function: 'convert_json_to_html' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_file = json_file.with_suffix('.html')\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))",
        "output": "value, html_file, dataset, column_count, column_width, html_content, html_file_path"
    },
    {
        "instruction": "Variables defined in function: 'combine_json_files' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                code_output.append({'instruction': 'Define a Python code file that is described as follows:\\n' + item['output'], 'output': item['input']})\n            for item in graph_data:\n                graph_output.append({'instruction': 'Define the call code graph for this Python file:\\n' + item['input'], 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n        write_file(combined_data, file_path)\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "result, seen, combined_data, code_output, instruct_data, file_path, graph_output, graph_data, json_file_data, purpose_data, code_graph_output"
    },
    {
        "instruction": "Variables defined in function: 'remove_duplicate_dataset_entries' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "result, seen"
    },
    {
        "instruction": "Variables defined in function: 'create_code_graph' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = (edge['source'], edge['target'])\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)\n    plt.close()",
        "output": "graph_type, pos, label, G, edge_labels, output_file"
    },
    {
        "instruction": "Variables defined in function: 'save_python_data' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join((part for part in relative_path.parts))\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except:\n        logging.info(f'Error creating graph for {base_name}')",
        "output": "output_subdir, base_name, contents, file_names"
    },
    {
        "instruction": "Returned items from function: 'read_file' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "yaml.loadf, json.loadf"
    },
    {
        "instruction": "Returned items from function: 'convert_json_to_html' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_file = json_file.with_suffix('.html')\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\\n                th, td {\\n                    border: 1px solid black;\\n                    padding: 8px;\\n                    text-align: left;\\n                    white-space: pre-line;\\n                    vertical-align: top;\\n                    word-wrap: break-word;\\n                }\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))",
        "output": "nbsp.replacet, nbsp  tab_width, text.replace"
    },
    {
        "instruction": "Returned items from function: 'preserve_spacing' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "nbsp.replacet, nbsp  tab_width, text.replace"
    },
    {
        "instruction": "Returned items from function: 'combine_json_files' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n\n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                code_output.append({'instruction': 'Define a Python code file that is described as follows:\\n' + item['output'], 'output': item['input']})\n            for item in graph_data:\n                graph_output.append({'instruction': 'Define the call code graph for this Python file:\\n' + item['input'], 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n        write_file(combined_data, file_path)\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}",
        "output": "result, instruct_list: instruct_data"
    },
    {
        "instruction": "Returned items from function: 'remove_duplicate_dataset_entries' in Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n    \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n    seen = set()\n    result = []\n    for item in dataset:\n        if (item[key1], item[key2]) not in seen:\n            seen.add((item[key1], item[key2]))\n            result.append(item)\n    return result",
        "output": "result"
    },
    {
        "instruction": "1) DESCRIBE the purpose and processing summary of Python file: 'py2dataset.save_py2dataset_output.py'; 2) PROVIDE an itemized and detailed description of each applicable function, class, and method; 3) EXPLAIN what each input, output, and variable does in the code.",
        "input": "\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List, Union\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None    \n    \"\"\"\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        html_file = json_file.with_suffix('.html')\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = 100 / column_count  # Calculate the width for each column based on the number of columns\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        for entry in dataset:\n            html_content += \"<tr>\"\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f\"<td>{value}</td>\"\n            html_content += \"</tr>\"\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix('.html')\n        try:   \n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))\n\n\ndef combine_json_files(directory: str) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n   \n    def remove_duplicate_dataset_entries(dataset: List[Dict], key1: str, key2: str) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n\n    instruct_data = []\n    for file_name in ['instruct.json']:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f'*.{file_name}'):\n            json_file_data = read_file(json_file)\n            combined_data.extend(json_file_data)\n            combined_data = remove_duplicate_dataset_entries(combined_data, 'instruction', 'output')\n            instruct_data = combined_data.copy()\n            # gen training datasets that contains purpose and graph data formatted as follow for each item in the dataset:\n            purpose_data = [item for item in combined_data if item['instruction'].startswith('1) DESCRIBE the purpose')]\n            graph_data = [item for item in combined_data if item['instruction'].startswith('Call code graph')]\n            code_output = []\n            graph_output = []\n            for item in purpose_data:\n                code_output.append({'instruction': 'Define a Python code file that is described as follows:\\n'+ item['output'], 'output': item['input']})\n            for item in graph_data:\n                graph_output.append({'instruction': 'Define the call code graph for this Python file:\\n' + item['input'], 'output': item['output']})\n            code_graph_output = code_output + graph_output\n            write_file(code_graph_output, Path(directory) / 'training.json')\n\n        write_file(combined_data, file_path)\n\n    # Save html file for each json file in the output directory\n    convert_json_to_html(directory)\n    return {'instruct_list': instruct_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = 'entire_code_graph'\n    output_file = output_subdir / f'{base_name}.{graph_type}.png'\n\n    # Create graphs, add nodes, and add edges\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details['file_info'][graph_type]['nodes'])\n    for edge in file_details['file_info'][graph_type]['edges']:\n        source, target = edge['source'], edge['target']\n        if source in G.nodes and target in G.nodes:\n           G.add_edge(source, target, **{k: v for k, v in edge.items() if k in ['target_inputs', 'target_returns']})\n    # Draw graphs\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if 'target_returns' in edge[2] and edge[2]['target_returns']:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file) # Save the figure\n    plt.close()  # Close the figure\n\n\ndef save_python_data(file_details: dict, instruct_list: list, relative_path: Path, output_dir: str) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    output_subdir = Path(output_dir) / relative_path.parts[0]\n    output_subdir.mkdir(parents=True, exist_ok=True)\n    base_name = '.'.join(part for part in relative_path.parts)\n\n    # write instrunct.json files\n    file_names = [f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n    contents = [instruct_list, file_details]\n\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except:\n        logging.info(f'Error creating graph for {base_name}')\n",
        "output": "The purpose of 'py2dataset.save_py2dataset_output.py' is to generate the following tasks: read files from a given directory, convert JSON files within the directory into HTML format, merge all JSON files in the directory, remove duplicates from the combined JSON files, and save them as 'instruct.json', create code graphs based on provided file details, and save Python data as YAML and instruction data as JSON files.\r\n\nThe `read_file` function takes a path to a JSON or YAML file as input, reads its contents into a dictionary, and returns it. It is used in the `convert_json_to_html`, `combine_json_files`, and `save_python_data` functions.\r\n\nThe `write_file` function writes the data in the format of the input path extension to either JSON or YAML format at a specified file path. It is used in the `convert_json_to_html`, `combine_json_files`, and `save_python_data` functions.\r\n\nThe `convert_json_to_html` function converts all JSON files within a given directory to HTML format, preserving spacing and tabs for the 'input' field. It is called by the `combine_json_files` function.\r\n\nThe `combine_json_files` function merges all JSON files in a given directory, removes duplicates based on specified keys (in this case, the instructions and output values), writes them to a single file called 'instruct.json', and converts the merged data into HTML format. It returns the combined dataset.\r\n\nThe `create_code_graph` function generates code graphs based on provided file details and saves them as PNG images in a specified directory.\r\n\nThe `save_python_data` function saves Python file details as YAML, instruction data as JSON files, and creates code graphs. It is called by the main script to save all necessary output files for each input Python file."
    }
]