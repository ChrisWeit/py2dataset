[
    {
        "instruction": "What are the dependencies of the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "\"\"\"\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'qa.json' and 'instruct.json' files.\n        e. Generate and save 'qa_purpose.json', 'qa_instruct.json', and 'qa_cleaned_instruct.json' files.\n        f. Convert the merged JSON files to HTML format if the html flag is set to True.\n        g. Return the 'qa_list' and 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n\"\"\"\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List, Union\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        html_file = json_file.with_suffix('.html')\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse;width: 100%;}\n                th, td {border: 1px solid black; padding: 8px; text-align: left; white-space: pre-line;}\n                td:nth-child(2) { width: 50%; overflow-wrap: anywhere;}\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        for key in dataset[0].keys():\n            html_content += f\"<th>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        for entry in dataset:\n            html_content += \"<tr>\"\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                if key == \"input\":  \n                    value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f\"<td>{value}</td>\"\n            html_content += \"</tr>\"\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix('.html')\n        try:   \n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))\n\n\ndef combine_json_files(directory: str, html: bool) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'qa.json':\n            qa_data = combined_data.copy()\n        if file == 'instruct.json':\n            instruct_data = combined_data.copy()\n        \n        # Create purpose-specific file\n        purpose_data = []\n        for item in combined_data:\n            if item[keys[file_names.index(file)]].startswith('Purpose of'):\n                purpose_data.append(item)\n        if purpose_data:\n            purpose_file_path = file_path.with_name(file_path.stem + '_purpose.json')\n            write_file(purpose_data, purpose_file_path)\n\n        # Reset combined_data for the next iteration\n        combined_data = []\n\n    # remove duplicate \"input\" info in the instruct.json and instruct_purpose.json information \n    # to make a cleaned_instruct.json cleaned_instruct_purpose_json\n    file_names = ['instruct.json', 'instruct_purpose.json']\n    for file in file_names:\n        seen_inputs = set()\n        file_path = Path(directory) / file\n        if not file_path.exists():\n            continue\n        dataset = read_file(file_path)\n        if not dataset:\n            continue\n        for item in dataset:\n            if item['input'] in seen_inputs:\n                item['input'] = ''\n            else:\n                seen_inputs.add(item['input'])\n        cleaned_instruct_file_path = Path(directory) / ('cleaned_'+ file) \n        write_file(dataset, cleaned_instruct_file_path)\n\n    # save html file for each json file in the output directory\n    if html:\n        convert_json_to_html(directory)\n\n    return {'qa_list': qa_data, 'instruct_list': instruct_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        # Create graphs\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'  \n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        # Add edges\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_inputs' in edge:\n                    edge_data['target_inputs'] = edge['target_inputs']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n            \n        # Save code graph as png\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()  # Close the figure",
        "output": "networkx, logging, sys, os, json, re, typing, yaml, html, matplotlib.pyplot, pathlib"
    },
    {
        "instruction": "What are the structural relationships between the functions and classes defined in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "\"\"\"\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'qa.json' and 'instruct.json' files.\n        e. Generate and save 'qa_purpose.json', 'qa_instruct.json', and 'qa_cleaned_instruct.json' files.\n        f. Convert the merged JSON files to HTML format if the html flag is set to True.\n        g. Return the 'qa_list' and 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n\"\"\"\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List, Union\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        html_file = json_file.with_suffix('.html')\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse;width: 100%;}\n                th, td {border: 1px solid black; padding: 8px; text-align: left; white-space: pre-line;}\n                td:nth-child(2) { width: 50%; overflow-wrap: anywhere;}\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        for key in dataset[0].keys():\n            html_content += f\"<th>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        for entry in dataset:\n            html_content += \"<tr>\"\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                if key == \"input\":  \n                    value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f\"<td>{value}</td>\"\n            html_content += \"</tr>\"\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix('.html')\n        try:   \n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))\n\n\ndef combine_json_files(directory: str, html: bool) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'qa.json':\n            qa_data = combined_data.copy()\n        if file == 'instruct.json':\n            instruct_data = combined_data.copy()\n        \n        # Create purpose-specific file\n        purpose_data = []\n        for item in combined_data:\n            if item[keys[file_names.index(file)]].startswith('Purpose of'):\n                purpose_data.append(item)\n        if purpose_data:\n            purpose_file_path = file_path.with_name(file_path.stem + '_purpose.json')\n            write_file(purpose_data, purpose_file_path)\n\n        # Reset combined_data for the next iteration\n        combined_data = []\n\n    # remove duplicate \"input\" info in the instruct.json and instruct_purpose.json information \n    # to make a cleaned_instruct.json cleaned_instruct_purpose_json\n    file_names = ['instruct.json', 'instruct_purpose.json']\n    for file in file_names:\n        seen_inputs = set()\n        file_path = Path(directory) / file\n        if not file_path.exists():\n            continue\n        dataset = read_file(file_path)\n        if not dataset:\n            continue\n        for item in dataset:\n            if item['input'] in seen_inputs:\n                item['input'] = ''\n            else:\n                seen_inputs.add(item['input'])\n        cleaned_instruct_file_path = Path(directory) / ('cleaned_'+ file) \n        write_file(dataset, cleaned_instruct_file_path)\n\n    # save html file for each json file in the output directory\n    if html:\n        convert_json_to_html(directory)\n\n    return {'qa_list': qa_data, 'instruct_list': instruct_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        # Create graphs\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'  \n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        # Add edges\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_inputs' in edge:\n                    edge_data['target_inputs'] = edge['target_inputs']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n            \n        # Save code graph as png\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()  # Close the figure",
        "output": "{'nodes': ['read_file', 'write_file', 'convert_json_to_html', 'preserve_spacing', 'combine_json_files', 'create_code_graph'], 'edges': [{'source': 'convert_json_to_html', 'target': 'preserve_spacing', 'target_inputs': ['text', 'tab_width'], 'target_returns': [\"text.replace(' ', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)\"]}, {'source': 'convert_json_to_html', 'target': 'read_file', 'target_inputs': ['file_path'], 'target_returns': ['yaml.load(f)', 'json.load(f)']}, {'source': 'combine_json_files', 'target': 'write_file', 'target_inputs': ['data', 'file_path'], 'target_returns': []}, {'source': 'combine_json_files', 'target': 'read_file', 'target_inputs': ['file_path'], 'target_returns': ['yaml.load(f)', 'json.load(f)']}, {'source': 'combine_json_files', 'target': 'convert_json_to_html', 'target_inputs': ['directory'], 'target_returns': [\"text.replace(' ', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)\"]}]}"
    },
    {
        "instruction": "What are the structural relationships between the functions and classes defined and used in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "\"\"\"\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'qa.json' and 'instruct.json' files.\n        e. Generate and save 'qa_purpose.json', 'qa_instruct.json', and 'qa_cleaned_instruct.json' files.\n        f. Convert the merged JSON files to HTML format if the html flag is set to True.\n        g. Return the 'qa_list' and 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n\"\"\"\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List, Union\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        html_file = json_file.with_suffix('.html')\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse;width: 100%;}\n                th, td {border: 1px solid black; padding: 8px; text-align: left; white-space: pre-line;}\n                td:nth-child(2) { width: 50%; overflow-wrap: anywhere;}\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        for key in dataset[0].keys():\n            html_content += f\"<th>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        for entry in dataset:\n            html_content += \"<tr>\"\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                if key == \"input\":  \n                    value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f\"<td>{value}</td>\"\n            html_content += \"</tr>\"\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix('.html')\n        try:   \n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))\n\n\ndef combine_json_files(directory: str, html: bool) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'qa.json':\n            qa_data = combined_data.copy()\n        if file == 'instruct.json':\n            instruct_data = combined_data.copy()\n        \n        # Create purpose-specific file\n        purpose_data = []\n        for item in combined_data:\n            if item[keys[file_names.index(file)]].startswith('Purpose of'):\n                purpose_data.append(item)\n        if purpose_data:\n            purpose_file_path = file_path.with_name(file_path.stem + '_purpose.json')\n            write_file(purpose_data, purpose_file_path)\n\n        # Reset combined_data for the next iteration\n        combined_data = []\n\n    # remove duplicate \"input\" info in the instruct.json and instruct_purpose.json information \n    # to make a cleaned_instruct.json cleaned_instruct_purpose_json\n    file_names = ['instruct.json', 'instruct_purpose.json']\n    for file in file_names:\n        seen_inputs = set()\n        file_path = Path(directory) / file\n        if not file_path.exists():\n            continue\n        dataset = read_file(file_path)\n        if not dataset:\n            continue\n        for item in dataset:\n            if item['input'] in seen_inputs:\n                item['input'] = ''\n            else:\n                seen_inputs.add(item['input'])\n        cleaned_instruct_file_path = Path(directory) / ('cleaned_'+ file) \n        write_file(dataset, cleaned_instruct_file_path)\n\n    # save html file for each json file in the output directory\n    if html:\n        convert_json_to_html(directory)\n\n    return {'qa_list': qa_data, 'instruct_list': instruct_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        # Create graphs\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'  \n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        # Add edges\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_inputs' in edge:\n                    edge_data['target_inputs'] = edge['target_inputs']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n            \n        # Save code graph as png\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()  # Close the figure",
        "output": "{'nodes': ['read_file', 'write_file', 'convert_json_to_html', 'preserve_spacing', 'combine_json_files', 'create_code_graph', 'file_path.open', 'json.load', 'yaml.load', 'json.dump', 'yaml.dump', 'Path', 'logging.info', \"text.replace(' ', '&nbsp;').replace\", 'file.write', 'str', 'escape', 'logging.save', 'dataset[0].keys', 'json_file.with_suffix', 'Path(directory).rglob', 'text.replace', 'open', 'value.replace', 'item[keys[file_names.index(file)]].startswith', 'file_path.with_name', 'set', 'purpose_data.append', 'list', 'file_names.index', 'seen_inputs.add', 'combined_data.extend', '{i[keys[file_names.index(file)]]: i for i in combined_data}.values', 'file_path.exists', 'combined_data.copy', 'nx.DiGraph', 'G.add_node', 'plt.figure', 'nx.draw_networkx_edge_labels', 'label.append', 'plt.savefig', 'G.edges', \"'\\\\n'.join\", \"', '.join\", 'nx.draw', 'G.add_edge', 'nx.spring_layout', 'plt.close'], 'edges': [{'source': 'read_file', 'target': 'file_path.open'}, {'source': 'read_file', 'target': 'json.load'}, {'source': 'read_file', 'target': 'yaml.load'}, {'source': 'write_file', 'target': 'file_path.open'}, {'source': 'write_file', 'target': 'json.dump'}, {'source': 'write_file', 'target': 'yaml.dump'}, {'source': 'convert_json_to_html', 'target': 'Path'}, {'source': 'convert_json_to_html', 'target': 'logging.info'}, {'source': 'convert_json_to_html', 'target': \"text.replace(' ', '&nbsp;').replace\"}, {'source': 'convert_json_to_html', 'target': 'preserve_spacing', 'target_inputs': ['text', 'tab_width'], 'target_returns': [\"text.replace(' ', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)\"]}, {'source': 'convert_json_to_html', 'target': 'file.write'}, {'source': 'convert_json_to_html', 'target': 'str'}, {'source': 'convert_json_to_html', 'target': 'escape'}, {'source': 'convert_json_to_html', 'target': 'logging.save'}, {'source': 'convert_json_to_html', 'target': 'dataset[0].keys'}, {'source': 'convert_json_to_html', 'target': 'json_file.with_suffix'}, {'source': 'convert_json_to_html', 'target': 'read_file', 'target_inputs': ['file_path'], 'target_returns': ['yaml.load(f)', 'json.load(f)']}, {'source': 'convert_json_to_html', 'target': 'Path(directory).rglob'}, {'source': 'convert_json_to_html', 'target': 'text.replace'}, {'source': 'convert_json_to_html', 'target': 'open'}, {'source': 'convert_json_to_html', 'target': 'value.replace'}, {'source': 'preserve_spacing', 'target': \"text.replace(' ', '&nbsp;').replace\"}, {'source': 'preserve_spacing', 'target': 'text.replace'}, {'source': 'combine_json_files', 'target': 'Path'}, {'source': 'combine_json_files', 'target': 'item[keys[file_names.index(file)]].startswith'}, {'source': 'combine_json_files', 'target': 'write_file', 'target_inputs': ['data', 'file_path'], 'target_returns': []}, {'source': 'combine_json_files', 'target': 'file_path.with_name'}, {'source': 'combine_json_files', 'target': 'set'}, {'source': 'combine_json_files', 'target': 'purpose_data.append'}, {'source': 'combine_json_files', 'target': 'list'}, {'source': 'combine_json_files', 'target': 'file_names.index'}, {'source': 'combine_json_files', 'target': 'seen_inputs.add'}, {'source': 'combine_json_files', 'target': 'combined_data.extend'}, {'source': 'combine_json_files', 'target': '{i[keys[file_names.index(file)]]: i for i in combined_data}.values'}, {'source': 'combine_json_files', 'target': 'read_file', 'target_inputs': ['file_path'], 'target_returns': ['yaml.load(f)', 'json.load(f)']}, {'source': 'combine_json_files', 'target': 'file_path.exists'}, {'source': 'combine_json_files', 'target': 'Path(directory).rglob'}, {'source': 'combine_json_files', 'target': 'convert_json_to_html', 'target_inputs': ['directory'], 'target_returns': [\"text.replace(' ', '&nbsp;').replace('\\\\t', '&nbsp;' * tab_width)\"]}, {'source': 'combine_json_files', 'target': 'combined_data.copy'}, {'source': 'create_code_graph', 'target': 'nx.DiGraph'}, {'source': 'create_code_graph', 'target': 'G.add_node'}, {'source': 'create_code_graph', 'target': 'plt.figure'}, {'source': 'create_code_graph', 'target': 'nx.draw_networkx_edge_labels'}, {'source': 'create_code_graph', 'target': 'label.append'}, {'source': 'create_code_graph', 'target': 'plt.savefig'}, {'source': 'create_code_graph', 'target': 'G.edges'}, {'source': 'create_code_graph', 'target': \"'\\\\n'.join\"}, {'source': 'create_code_graph', 'target': \"', '.join\"}, {'source': 'create_code_graph', 'target': 'nx.draw'}, {'source': 'create_code_graph', 'target': 'G.add_edge'}, {'source': 'create_code_graph', 'target': 'nx.spring_layout'}, {'source': 'create_code_graph', 'target': 'plt.close'}]}"
    },
    {
        "instruction": "What functions are defined in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "\"\"\"\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'qa.json' and 'instruct.json' files.\n        e. Generate and save 'qa_purpose.json', 'qa_instruct.json', and 'qa_cleaned_instruct.json' files.\n        f. Convert the merged JSON files to HTML format if the html flag is set to True.\n        g. Return the 'qa_list' and 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n\"\"\"\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List, Union\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        html_file = json_file.with_suffix('.html')\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse;width: 100%;}\n                th, td {border: 1px solid black; padding: 8px; text-align: left; white-space: pre-line;}\n                td:nth-child(2) { width: 50%; overflow-wrap: anywhere;}\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        for key in dataset[0].keys():\n            html_content += f\"<th>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        for entry in dataset:\n            html_content += \"<tr>\"\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                if key == \"input\":  \n                    value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f\"<td>{value}</td>\"\n            html_content += \"</tr>\"\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix('.html')\n        try:   \n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))\n\n\ndef combine_json_files(directory: str, html: bool) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'qa.json':\n            qa_data = combined_data.copy()\n        if file == 'instruct.json':\n            instruct_data = combined_data.copy()\n        \n        # Create purpose-specific file\n        purpose_data = []\n        for item in combined_data:\n            if item[keys[file_names.index(file)]].startswith('Purpose of'):\n                purpose_data.append(item)\n        if purpose_data:\n            purpose_file_path = file_path.with_name(file_path.stem + '_purpose.json')\n            write_file(purpose_data, purpose_file_path)\n\n        # Reset combined_data for the next iteration\n        combined_data = []\n\n    # remove duplicate \"input\" info in the instruct.json and instruct_purpose.json information \n    # to make a cleaned_instruct.json cleaned_instruct_purpose_json\n    file_names = ['instruct.json', 'instruct_purpose.json']\n    for file in file_names:\n        seen_inputs = set()\n        file_path = Path(directory) / file\n        if not file_path.exists():\n            continue\n        dataset = read_file(file_path)\n        if not dataset:\n            continue\n        for item in dataset:\n            if item['input'] in seen_inputs:\n                item['input'] = ''\n            else:\n                seen_inputs.add(item['input'])\n        cleaned_instruct_file_path = Path(directory) / ('cleaned_'+ file) \n        write_file(dataset, cleaned_instruct_file_path)\n\n    # save html file for each json file in the output directory\n    if html:\n        convert_json_to_html(directory)\n\n    return {'qa_list': qa_data, 'instruct_list': instruct_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        # Create graphs\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'  \n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        # Add edges\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_inputs' in edge:\n                    edge_data['target_inputs'] = edge['target_inputs']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n            \n        # Save code graph as png\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()  # Close the figure",
        "output": "preserve_spacing, write_file, create_code_graph, read_file, convert_json_to_html, combine_json_files"
    },
    {
        "instruction": "What is the control flow of the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "\"\"\"\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'qa.json' and 'instruct.json' files.\n        e. Generate and save 'qa_purpose.json', 'qa_instruct.json', and 'qa_cleaned_instruct.json' files.\n        f. Convert the merged JSON files to HTML format if the html flag is set to True.\n        g. Return the 'qa_list' and 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n\"\"\"\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List, Union\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        html_file = json_file.with_suffix('.html')\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse;width: 100%;}\n                th, td {border: 1px solid black; padding: 8px; text-align: left; white-space: pre-line;}\n                td:nth-child(2) { width: 50%; overflow-wrap: anywhere;}\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        for key in dataset[0].keys():\n            html_content += f\"<th>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        for entry in dataset:\n            html_content += \"<tr>\"\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                if key == \"input\":  \n                    value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f\"<td>{value}</td>\"\n            html_content += \"</tr>\"\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix('.html')\n        try:   \n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))\n\n\ndef combine_json_files(directory: str, html: bool) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'qa.json':\n            qa_data = combined_data.copy()\n        if file == 'instruct.json':\n            instruct_data = combined_data.copy()\n        \n        # Create purpose-specific file\n        purpose_data = []\n        for item in combined_data:\n            if item[keys[file_names.index(file)]].startswith('Purpose of'):\n                purpose_data.append(item)\n        if purpose_data:\n            purpose_file_path = file_path.with_name(file_path.stem + '_purpose.json')\n            write_file(purpose_data, purpose_file_path)\n\n        # Reset combined_data for the next iteration\n        combined_data = []\n\n    # remove duplicate \"input\" info in the instruct.json and instruct_purpose.json information \n    # to make a cleaned_instruct.json cleaned_instruct_purpose_json\n    file_names = ['instruct.json', 'instruct_purpose.json']\n    for file in file_names:\n        seen_inputs = set()\n        file_path = Path(directory) / file\n        if not file_path.exists():\n            continue\n        dataset = read_file(file_path)\n        if not dataset:\n            continue\n        for item in dataset:\n            if item['input'] in seen_inputs:\n                item['input'] = ''\n            else:\n                seen_inputs.add(item['input'])\n        cleaned_instruct_file_path = Path(directory) / ('cleaned_'+ file) \n        write_file(dataset, cleaned_instruct_file_path)\n\n    # save html file for each json file in the output directory\n    if html:\n        convert_json_to_html(directory)\n\n    return {'qa_list': qa_data, 'instruct_list': instruct_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        # Create graphs\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'  \n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        # Add edges\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_inputs' in edge:\n                    edge_data['target_inputs'] = edge['target_inputs']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n            \n        # Save code graph as png\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()  # Close the figure",
        "output": "module -> def read_file -> with -> if -> if -> def write_file -> with -> if -> if -> def convert_json_to_html -> def preserve_spacing -> for -> if -> for -> for -> for -> if -> try -> with -> except -> def combine_json_files -> for -> if -> for -> if -> if -> for -> if -> if -> for -> if -> if -> for -> if -> if -> def create_code_graph -> for -> for -> for -> if -> if -> if -> for -> if -> if"
    },
    {
        "instruction": "What are the inputs to the function: 'read_file' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "file_path"
    },
    {
        "instruction": "What are the inputs to the function: 'write_file' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "data, file_path"
    },
    {
        "instruction": "What are the inputs to the function: 'convert_json_to_html' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def convert_json_to_html(directory: str) -> None:\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_file = json_file.with_suffix('.html')\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse;width: 100%;}\\n                th, td {border: 1px solid black; padding: 8px; text-align: left; white-space: pre-line;}\\n                td:nth-child(2) { width: 50%; overflow-wrap: anywhere;}\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        for key in dataset[0].keys():\n            html_content += f'<th>{key}</th>'\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                if key == 'input':\n                    value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))",
        "output": "directory"
    },
    {
        "instruction": "What are the inputs to the function: 'preserve_spacing' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "text, tab_width"
    },
    {
        "instruction": "What are the inputs to the function: 'combine_json_files' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def combine_json_files(directory: str, html: bool) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'qa.json':\n            qa_data = combined_data.copy()\n        if file == 'instruct.json':\n            instruct_data = combined_data.copy()\n        purpose_data = []\n        for item in combined_data:\n            if item[keys[file_names.index(file)]].startswith('Purpose of'):\n                purpose_data.append(item)\n        if purpose_data:\n            purpose_file_path = file_path.with_name(file_path.stem + '_purpose.json')\n            write_file(purpose_data, purpose_file_path)\n        combined_data = []\n    file_names = ['instruct.json', 'instruct_purpose.json']\n    for file in file_names:\n        seen_inputs = set()\n        file_path = Path(directory) / file\n        if not file_path.exists():\n            continue\n        dataset = read_file(file_path)\n        if not dataset:\n            continue\n        for item in dataset:\n            if item['input'] in seen_inputs:\n                item['input'] = ''\n            else:\n                seen_inputs.add(item['input'])\n        cleaned_instruct_file_path = Path(directory) / ('cleaned_' + file)\n        write_file(dataset, cleaned_instruct_file_path)\n    if html:\n        convert_json_to_html(directory)\n    return {'qa_list': qa_data, 'instruct_list': instruct_data}",
        "output": "directory, html"
    },
    {
        "instruction": "What are the inputs to the function: 'create_code_graph' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'\n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_inputs' in edge:\n                    edge_data['target_inputs'] = edge['target_inputs']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()",
        "output": "output_subdir, base_name, file_details"
    },
    {
        "instruction": "What is the docstring of the function: 'read_file' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "Reads a JSON or YAML file and returns its contents as a dictionary. Args: file_path Path: The path to the file. Returns: The contents of the file as a dictionary."
    },
    {
        "instruction": "What is the docstring of the function: 'write_file' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "Writes a dictionary to a JSON or YAML file. Args: data Dict: The data to write to the file. file_path Path: The path to the file."
    },
    {
        "instruction": "What is the docstring of the function: 'preserve_spacing' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "Preserve spaces and tabs in the provided text."
    },
    {
        "instruction": "What is the docstring of the function: 'combine_json_files' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def combine_json_files(directory: str, html: bool) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'qa.json':\n            qa_data = combined_data.copy()\n        if file == 'instruct.json':\n            instruct_data = combined_data.copy()\n        purpose_data = []\n        for item in combined_data:\n            if item[keys[file_names.index(file)]].startswith('Purpose of'):\n                purpose_data.append(item)\n        if purpose_data:\n            purpose_file_path = file_path.with_name(file_path.stem + '_purpose.json')\n            write_file(purpose_data, purpose_file_path)\n        combined_data = []\n    file_names = ['instruct.json', 'instruct_purpose.json']\n    for file in file_names:\n        seen_inputs = set()\n        file_path = Path(directory) / file\n        if not file_path.exists():\n            continue\n        dataset = read_file(file_path)\n        if not dataset:\n            continue\n        for item in dataset:\n            if item['input'] in seen_inputs:\n                item['input'] = ''\n            else:\n                seen_inputs.add(item['input'])\n        cleaned_instruct_file_path = Path(directory) / ('cleaned_' + file)\n        write_file(dataset, cleaned_instruct_file_path)\n    if html:\n        convert_json_to_html(directory)\n    return {'qa_list': qa_data, 'instruct_list': instruct_data}",
        "output": "Combine all JSON files in the output directory into qa.json and instruct.json, and then remove duplicates. Args: directory str: The directory where the output files are located."
    },
    {
        "instruction": "What is the docstring of the function: 'create_code_graph' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'\n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_inputs' in edge:\n                    edge_data['target_inputs'] = edge['target_inputs']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()",
        "output": "Generate graphs from the file_details and save them as PNG images. Args: file_details dict: The details extracted from the Python file. base_name str: The base name of the output files. output_subdir Path: The subdirectory where the output files will be saved."
    },
    {
        "instruction": "What calls are made in the function: 'read_file' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "file_path.open, json.load, yaml.load"
    },
    {
        "instruction": "What calls are made in the function: 'write_file' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "file_path.open, json.dump, yaml.dump"
    },
    {
        "instruction": "What calls are made in the function: 'convert_json_to_html' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def convert_json_to_html(directory: str) -> None:\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_file = json_file.with_suffix('.html')\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse;width: 100%;}\\n                th, td {border: 1px solid black; padding: 8px; text-align: left; white-space: pre-line;}\\n                td:nth-child(2) { width: 50%; overflow-wrap: anywhere;}\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        for key in dataset[0].keys():\n            html_content += f'<th>{key}</th>'\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                if key == 'input':\n                    value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))",
        "output": "Path, logging.info, preserve_spacing, file.write, str, logging.save, dataset0.keys, text.replace, text.replace , Pathdirectory.rglob, json_file.with_suffix, read_file, nbsp.replace, escape, open, value.replace"
    },
    {
        "instruction": "What calls are made in the function: 'preserve_spacing' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "text.replace , nbsp.replace, text.replace"
    },
    {
        "instruction": "What calls are made in the function: 'combine_json_files' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def combine_json_files(directory: str, html: bool) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'qa.json':\n            qa_data = combined_data.copy()\n        if file == 'instruct.json':\n            instruct_data = combined_data.copy()\n        purpose_data = []\n        for item in combined_data:\n            if item[keys[file_names.index(file)]].startswith('Purpose of'):\n                purpose_data.append(item)\n        if purpose_data:\n            purpose_file_path = file_path.with_name(file_path.stem + '_purpose.json')\n            write_file(purpose_data, purpose_file_path)\n        combined_data = []\n    file_names = ['instruct.json', 'instruct_purpose.json']\n    for file in file_names:\n        seen_inputs = set()\n        file_path = Path(directory) / file\n        if not file_path.exists():\n            continue\n        dataset = read_file(file_path)\n        if not dataset:\n            continue\n        for item in dataset:\n            if item['input'] in seen_inputs:\n                item['input'] = ''\n            else:\n                seen_inputs.add(item['input'])\n        cleaned_instruct_file_path = Path(directory) / ('cleaned_' + file)\n        write_file(dataset, cleaned_instruct_file_path)\n    if html:\n        convert_json_to_html(directory)\n    return {'qa_list': qa_data, 'instruct_list': instruct_data}",
        "output": "Path, itemkeysfile_names.indexfile.startswith, write_file, file_path.with_name, set, combined_data.copy, purpose_data.append, Pathdirectory.rglob, file_names.index, seen_inputs.add, combined_data.extend, list, file_path.exists, convert_json_to_html, ikeysfile_names.indexfile: i for i in combined_data.values, read_file"
    },
    {
        "instruction": "What calls are made in the function: 'create_code_graph' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'\n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_inputs' in edge:\n                    edge_data['target_inputs'] = edge['target_inputs']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()",
        "output": "nx.DiGraph, G.add_node, , plt.figure, nx.draw_networkx_edge_labels, n.join, label.append, nx.draw, plt.savefig, .join, G.edges, G.add_edge, nx.spring_layout, plt.close"
    },
    {
        "instruction": "What variables are defined in the function: 'read_file' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "file_type"
    },
    {
        "instruction": "What variables are defined in the function: 'write_file' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "file_type"
    },
    {
        "instruction": "What variables are defined in the function: 'convert_json_to_html' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def convert_json_to_html(directory: str) -> None:\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_file = json_file.with_suffix('.html')\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse;width: 100%;}\\n                th, td {border: 1px solid black; padding: 8px; text-align: left; white-space: pre-line;}\\n                td:nth-child(2) { width: 50%; overflow-wrap: anywhere;}\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        for key in dataset[0].keys():\n            html_content += f'<th>{key}</th>'\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                if key == 'input':\n                    value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))",
        "output": "dataset, html_content, html_file_path, html_file, value"
    },
    {
        "instruction": "What variables are defined in the function: 'combine_json_files' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def combine_json_files(directory: str, html: bool) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'qa.json':\n            qa_data = combined_data.copy()\n        if file == 'instruct.json':\n            instruct_data = combined_data.copy()\n        purpose_data = []\n        for item in combined_data:\n            if item[keys[file_names.index(file)]].startswith('Purpose of'):\n                purpose_data.append(item)\n        if purpose_data:\n            purpose_file_path = file_path.with_name(file_path.stem + '_purpose.json')\n            write_file(purpose_data, purpose_file_path)\n        combined_data = []\n    file_names = ['instruct.json', 'instruct_purpose.json']\n    for file in file_names:\n        seen_inputs = set()\n        file_path = Path(directory) / file\n        if not file_path.exists():\n            continue\n        dataset = read_file(file_path)\n        if not dataset:\n            continue\n        for item in dataset:\n            if item['input'] in seen_inputs:\n                item['input'] = ''\n            else:\n                seen_inputs.add(item['input'])\n        cleaned_instruct_file_path = Path(directory) / ('cleaned_' + file)\n        write_file(dataset, cleaned_instruct_file_path)\n    if html:\n        convert_json_to_html(directory)\n    return {'qa_list': qa_data, 'instruct_list': instruct_data}",
        "output": "dataset, instruct_data, qa_data, file_names, file_path, cleaned_instruct_file_path, seen_inputs, purpose_file_path, combined_data, keys, purpose_data"
    },
    {
        "instruction": "What variables are defined in the function: 'create_code_graph' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'\n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_inputs' in edge:\n                    edge_data['target_inputs'] = edge['target_inputs']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()",
        "output": "label, source, G, edge_data, pos, output_file, target, edge_labels"
    },
    {
        "instruction": "What are the returned items from the function: 'read_file' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "json.loadf, yaml.loadf"
    },
    {
        "instruction": "What are the returned items from the function: 'convert_json_to_html' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def convert_json_to_html(directory: str) -> None:\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_file = json_file.with_suffix('.html')\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse;width: 100%;}\\n                th, td {border: 1px solid black; padding: 8px; text-align: left; white-space: pre-line;}\\n                td:nth-child(2) { width: 50%; overflow-wrap: anywhere;}\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        for key in dataset[0].keys():\n            html_content += f'<th>{key}</th>'\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                if key == 'input':\n                    value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))",
        "output": "text.replace , nbsp.replacet, nbsp  tab_width"
    },
    {
        "instruction": "What are the returned items from the function: 'preserve_spacing' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "text.replace , nbsp.replacet, nbsp  tab_width"
    },
    {
        "instruction": "What are the returned items from the function: 'combine_json_files' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def combine_json_files(directory: str, html: bool) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'qa.json':\n            qa_data = combined_data.copy()\n        if file == 'instruct.json':\n            instruct_data = combined_data.copy()\n        purpose_data = []\n        for item in combined_data:\n            if item[keys[file_names.index(file)]].startswith('Purpose of'):\n                purpose_data.append(item)\n        if purpose_data:\n            purpose_file_path = file_path.with_name(file_path.stem + '_purpose.json')\n            write_file(purpose_data, purpose_file_path)\n        combined_data = []\n    file_names = ['instruct.json', 'instruct_purpose.json']\n    for file in file_names:\n        seen_inputs = set()\n        file_path = Path(directory) / file\n        if not file_path.exists():\n            continue\n        dataset = read_file(file_path)\n        if not dataset:\n            continue\n        for item in dataset:\n            if item['input'] in seen_inputs:\n                item['input'] = ''\n            else:\n                seen_inputs.add(item['input'])\n        cleaned_instruct_file_path = Path(directory) / ('cleaned_' + file)\n        write_file(dataset, cleaned_instruct_file_path)\n    if html:\n        convert_json_to_html(directory)\n    return {'qa_list': qa_data, 'instruct_list': instruct_data}",
        "output": "instruct_list: instruct_data, qa_list: qa_data"
    },
    {
        "instruction": "What is the purpose and processing summary of the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "\"\"\"\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'qa.json' and 'instruct.json' files.\n        e. Generate and save 'qa_purpose.json', 'qa_instruct.json', and 'qa_cleaned_instruct.json' files.\n        f. Convert the merged JSON files to HTML format if the html flag is set to True.\n        g. Return the 'qa_list' and 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n\"\"\"\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List, Union\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        html_file = json_file.with_suffix('.html')\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse;width: 100%;}\n                th, td {border: 1px solid black; padding: 8px; text-align: left; white-space: pre-line;}\n                td:nth-child(2) { width: 50%; overflow-wrap: anywhere;}\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        for key in dataset[0].keys():\n            html_content += f\"<th>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        for entry in dataset:\n            html_content += \"<tr>\"\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                if key == \"input\":  \n                    value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f\"<td>{value}</td>\"\n            html_content += \"</tr>\"\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix('.html')\n        try:   \n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))\n\n\ndef combine_json_files(directory: str, html: bool) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'qa.json':\n            qa_data = combined_data.copy()\n        if file == 'instruct.json':\n            instruct_data = combined_data.copy()\n        \n        # Create purpose-specific file\n        purpose_data = []\n        for item in combined_data:\n            if item[keys[file_names.index(file)]].startswith('Purpose of'):\n                purpose_data.append(item)\n        if purpose_data:\n            purpose_file_path = file_path.with_name(file_path.stem + '_purpose.json')\n            write_file(purpose_data, purpose_file_path)\n\n        # Reset combined_data for the next iteration\n        combined_data = []\n\n    # remove duplicate \"input\" info in the instruct.json and instruct_purpose.json information \n    # to make a cleaned_instruct.json cleaned_instruct_purpose_json\n    file_names = ['instruct.json', 'instruct_purpose.json']\n    for file in file_names:\n        seen_inputs = set()\n        file_path = Path(directory) / file\n        if not file_path.exists():\n            continue\n        dataset = read_file(file_path)\n        if not dataset:\n            continue\n        for item in dataset:\n            if item['input'] in seen_inputs:\n                item['input'] = ''\n            else:\n                seen_inputs.add(item['input'])\n        cleaned_instruct_file_path = Path(directory) / ('cleaned_'+ file) \n        write_file(dataset, cleaned_instruct_file_path)\n\n    # save html file for each json file in the output directory\n    if html:\n        convert_json_to_html(directory)\n\n    return {'qa_list': qa_data, 'instruct_list': instruct_data}\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        # Create graphs\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'  \n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        # Add edges\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_inputs' in edge:\n                    edge_data['target_inputs'] = edge['target_inputs']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n            \n        # Save code graph as png\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()  # Close the figure",
        "output": "The Python code provided, `py2dataset.save_py2dataset_output.py`, serves as a back-end process for creating dataset files in JSON format to train various machine learning models such as question answering systems or language translation systems. Here's what the purpose and processing summary of this file would look like:\n\n**Purpose:** This Python script is designed to convert large python scripts into a dataset that can be used by AI/ML algorithms, specifically for training purposes. The main purpose is to allow models to read, analyze, manipulate and understand complex code in an objective way. \n\n**Processing Summary:** This file contains two functions (`read_file` and `write_file`) which are responsible for reading a python script into memory (through the use of `open()`), as well as writing that data back out to disk as JSON or YAML format files respectively. The script then enters an 'if' condition, checking if it's dealing with either JSON or YAML file formats and creates appropriate dictionaries based on those file types.\n\nThe next group contains three more functions (`convert_json_to_html`, `combine_json_files` and `create_code_graph`). The first function is a helper function that converts all the json files in a given directory into an HTML format, while the second one combines multiple JSON files together, removes duplicates from those combined files, generates new datasets based on these operations, creates purpose-specific dataset files, cleans up 'input' information and finally, uses `create_code_graph` to generate network graphs of code.\n\nThe graph generated by `create_code_graph`, if present in the file, is a representation of various internal and entire code structures as an image that can be used for visualizing these complex relationships within the python script."
    },
    {
        "instruction": "What is the purpose and processing summary of the function: 'read_file' defined in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "The purpose of the function `read_file(file_path: Path) -> Dict` is to read a JSON or YAML file and return its contents as a dictionary based on the provided file path parameter. This processing summary includes several key points:\n\n1. The function is defined in 'py2dataset.save_py2dataset_output.py', which suggests that this code was part of an effort by AI language models to train themselves about Python programming, including functions like this one.\n\n2. The function takes a Path object as input and returns a Dict. This indicates that the function will generally perform some operation involving paths and dictionaries.\n\n3. Inside the function's docstring, there is a brief explanation of what the function does (\"Reads a JSON or YAML file and returns its contents as a dictionary.\"), which can give insight into how the function might be used in practice. The parameters and return types are also included to provide clarity about what type of inputs it expects and what type of output it generates, respectively.\n\n4. The 'with' keyword is used within the function body for file handling. This suggests that the function may have some operations involving opening a file, reading from it, or closing it.\n\n5. If the file_path's suffix (i.e., extension) is '.json', then the function assumes a JSON file and tries to load its contents into a Python dictionary using `json.load()`. Similarly, if the file_path's suffix is '.yaml', then the function assumes a YAML file and tries to load its contents into a Python dictionary using `yaml.load()`.\n\nAll these factors together suggest that 'read_file' might be used in various parts of Python code for reading JSON or YAML files, either as dictionaries for further processing or converting their content into another data structure (like pandas DataFrames)."
    },
    {
        "instruction": "What is the purpose and processing summary of the function: 'write_file' defined in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "The 'write_file' function in `py2dataset/save_py2dataset_output.py` serves as a crucial part of our data preprocessing script. It is designed to write dictionaries (data) into JSON or YAML files based on the file extension provided. The main purpose behind writing these outputs is that it allows us to easily store and retrieve the processed data for later use in other parts of the project.\n\nHere's a summary of how this function works:\n\n- It takes two parameters, 'data' (a dictionary) and 'file_path' (the path where you want your file to be saved).\n- The function then determines what kind of extension is provided by the 'file_path'. If it's JSON ('json'), it will dump this data into a json formatted string. Similarly, if YAML ('yaml') is provided, it will convert these dictionary data into a yaml formatted string and write to that file.\n- The function uses a context manager `with file_path.open('w') as f:` which opens the specified 'file_path' in write mode and assigns the resultant file object to variable 'f'. This ensures any changes made to the dictionary data will be written back to this file once we complete our script.\n- In case of YAML, there is a line `yaml.SafeDumper.ignore_aliases = lambda *args: True`. This piece of code ignores aliases when dumping objects with YAML, as it could lead to issues in certain situations where the same object is referenced multiple times within an object graph. We set this function to ensure safe YAML file creation and processing.\n- The 'data' parameter is a dictionary that contains all the necessary data related to each language we are preprocessing from GitHub for our dataset. \n- This function will be used in various other parts of our `py2dataset/save_py2dataset_output.py` script, where it's important to store and retrieve preprocessed output."
    },
    {
        "instruction": "What is the purpose and processing summary of the function: 'convert_json_to_html' defined in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def convert_json_to_html(directory: str) -> None:\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_file = json_file.with_suffix('.html')\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse;width: 100%;}\\n                th, td {border: 1px solid black; padding: 8px; text-align: left; white-space: pre-line;}\\n                td:nth-child(2) { width: 50%; overflow-wrap: anywhere;}\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        for key in dataset[0].keys():\n            html_content += f'<th>{key}</th>'\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                if key == 'input':\n                    value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))",
        "output": "The 'convert_json_to_html' function is a helper method within another script, specifically located at 'py2dataset.save_py2dataset_output.py', which serves to convert JSON files into HTML for easy visualization purposes. \n\nThe purpose of this function is twofold. Firstly, it creates an HTML table structure that displays the content from each individual JSON file in a structured manner, and secondly, it preserves spaces and tabs within text fields, as these can often be crucial when dealing with raw data or code output.\n\nThe processing summary of this function largely aligns with its purpose - creating an HTML table structure for the provided JSON files while preserving spacing and tabs. The function iterates over each file in a specified directory (the 'directory' parameter), reads each file's content, creates a corresponding HTML table based on that data, and writes out the resulting HTML to a new file within the same directory. \n\nThe main logic of the function involves reading JSON files from a given directory using Python's built-in Path library, creating an HTML structure for those files' contents, preserving any whitespace characters (such as spaces or tabs), writing each resultant HTML page to a new file in the specified directory, and then moving on to the next file.\n\nThe function is designed to handle JSON data where some fields contain multi-line strings of code/text. It uses Python's built-in 're' module for replacing newline characters with HTML breaks (`<br/>`), as well as the `escape()` method from the 'cgi' library for escaping any unsafe characters in those text fields.\n\nIn summary, this function is used to transform data into a format that can be more easily visualized and understood by humans, particularly when it comes to multi-line strings of code or text. It's important to note however, that the actual transformation done by the function may not always accurately represent what was present in the original JSON file due to factors like variable naming conventions, formatting changes between languages/platforms, etc."
    },
    {
        "instruction": "What is the purpose and processing summary of the function: 'preserve_spacing' defined in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def preserve_spacing(text: str, tab_width: int=4) -> str:\n    \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n    return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)",
        "output": "The given python function, `'preserve_spacing(text: str, tab_width: int=4) -> str:`, is a utility function that preserves spaces and tabs within a provided text string. The purpose of this function is to maintain the original spacing when displaying code in HTML or other markup languages where such characters can be difficult to display properly due to their size or potential ambiguity for users viewing the document in raw form.\n\nWhen this function is called, it uses Python's built-in `'replace()'` method to replace each space (' ') character with '&nbsp;', and each tab ('\\t') character is replaced by a string of 'tab_width' number of '&nbsp;' characters. The default value for `tab_width` is set to 4 if not provided when calling the function.\n\nThe processing summary includes: \n1) Replacing all spaces with '&nbsp;'. This is done using Python's built-in `'replace()'` method, which replaces a specified sequence of characters in a string (`text`) with another specified sequence of characters. In this case, we're replacing space characters (' ') with '&nbsp;'\n2) Replacing each tab character ('\\t') with 'tab_width' number of '&nbsp;'. This is done using Python's built-in `'replace()'` method again, but instead of a single replacement string, in this case it's a repetition of the `&nbsp;` sequence."
    },
    {
        "instruction": "What is the purpose and processing summary of the function: 'combine_json_files' defined in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def combine_json_files(directory: str, html: bool) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'qa.json':\n            qa_data = combined_data.copy()\n        if file == 'instruct.json':\n            instruct_data = combined_data.copy()\n        purpose_data = []\n        for item in combined_data:\n            if item[keys[file_names.index(file)]].startswith('Purpose of'):\n                purpose_data.append(item)\n        if purpose_data:\n            purpose_file_path = file_path.with_name(file_path.stem + '_purpose.json')\n            write_file(purpose_data, purpose_file_path)\n        combined_data = []\n    file_names = ['instruct.json', 'instruct_purpose.json']\n    for file in file_names:\n        seen_inputs = set()\n        file_path = Path(directory) / file\n        if not file_path.exists():\n            continue\n        dataset = read_file(file_path)\n        if not dataset:\n            continue\n        for item in dataset:\n            if item['input'] in seen_inputs:\n                item['input'] = ''\n            else:\n                seen_inputs.add(item['input'])\n        cleaned_instruct_file_path = Path(directory) / ('cleaned_' + file)\n        write_file(dataset, cleaned_instruct_file_path)\n    if html:\n        convert_json_to_html(directory)\n    return {'qa_list': qa_data, 'instruct_list': instruct_data}",
        "output": "The provided code, specifically `combine_json_files`, is a helper function that is used by another function (`process_data`) to combine multiple JSON files into one final dataset. The main purpose of this function is to take the output from Python's automated testing scripts (i.e., JSON files containing information on tests passed and failed), join them together, remove duplicates, and prepare it for training purposes in a machine learning model.\n\nThe processing summary includes several operations:\n- It reads two separate JSON files (`qa.json` and `instruct.json`) from the specified directory path into Python lists of dictionaries (dictionaries are key-value pairs that contain information about tests).\n- Then, it iterates over all other `.json` files in the directory tree using a recursive glob method. For each file found, the data is extended to the combined list. This means all JSON files from different directories and subdirectories will be added to this one large dataset.\n- After combining all data into one list, duplicates are removed by converting the entire dictionary of lists (or dicts) into a set, which automatically removes duplicates based on its keys. The resulting values in the final list can be thought of as unique instances.\n- Next, if any item in this new dataset starts with 'Purpose of', it is kept separate from the rest of the data and written to another file (`instruct_purpose.json`). This is because the original instructions might have some specific details or requirements that are not useful for training purposes (like unique IDs).\n- Finally, if HTML conversion is enabled, a helper function `convert_json_to_html` is called which converts all JSON files back into their original form but with different file extensions. The data in these new files can then be used to display the same information as before in an interactive web interface or application.\n\nThe purpose of this function and its processing are not immediately apparent from the code, so we'll need more context to fully understand it. For instance, is 'combine_json_files' being called somewhere else in another Python file? If it is, what purpose does that serve? The actual data processed by `combine_json_files` (the combined dataset and potentially other separate datasets) are not immediately clear without additional code snippets or context. Hence, we'll need a more complex example to fully grasp the purpose of 'combine_json_files' with its processing summary."
    },
    {
        "instruction": "What is the purpose and processing summary of the function: 'create_code_graph' defined in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'\n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_inputs' in edge:\n                    edge_data['target_inputs'] = edge['target_inputs']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()",
        "output": "The `create_code_graph` function, which can be found within the provided Python code, serves as a utility for generating visual representations of Python code using networkx and matplotlib libraries. This function is designed to assist with code graph generation operations in the 'py2dataset.save_py2dataset_output' module.\n\nThe purpose of this function is to create visual representations of internal and entire code graphs, which could be used for various purposes, including language model training or understanding processes during AI language modeling. The main processing summary includes:\n\n1) Preparing the necessary data structures from the input `file_details` dictionary. This involves creating a directed graph (using networkx), adding nodes to this graph, and connecting these nodes with edges based on information in 'edges' list of dictionaries that's provided within each value of `'internal_code_graph'` or `'entire_code_graph'` keys in `file_details['file_info']`.\n2) Applying networkx's spring layout algorithm to position the graph. This is done using a call to `nx.spring_layout` function which positions nodes in a circular layout, having good spacing and avoiding overlapping of edges.\n3) Finally, using matplotlib library to create a plot of directed graph with labels for each edge as defined within `'target_inputs'` and `'target_returns'` keys if they exist. The final result is then saved as an image file at the specified output path (`output_subdir / f'{base_name}.{graph_type}.png'`).\n\nThis function does not perform any actual programming operations, but rather prepares visual representations of Python code to aid in understanding and training language models about Python code."
    },
    {
        "instruction": "What is the purpose and usage of each of these variables: 'file_type' defined in the function: 'read_file' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "The context provided does not specify what language model or AI system this code is being used for, which makes it difficult to provide an accurate and comprehensive explanation. However, assuming that we are dealing with a language model trained on Python code, 'file_type', 'read_file()', and the 'py2dataset.save_py2dataset_output.py' file are likely variables related to this.\n\n1. The variable 'file_type' is typically used in programming languages for handling different types of files. In Python, it could be a string representing the extension of the file (e.g., '.json', '.yaml'). This is because each language has its own syntax and rules about dealing with various file formats.\n\n2. The function 'read_file' reads a JSON or YAML file. It's important to note that this does not necessarily have to be Python, it could be any programming language. However, the specifics of how to handle these files may vary depending on the language being used and its built-in functionality for handling various types of files.\n\n3. The 'py2dataset.save_py2dataset_output.py' file is likely a reference to an output file generated by another script in Python. In the context of training a model about Python code, this could refer to a file where an automated program would generate its own dataset and save it somewhere. This data would then be used as input for the language model during training.\n\nSo in terms of purpose and usage:\n- The variable 'file_type' is likely being used as a way to differentiate between JSON files and YAML files, which are two common file formats commonly used with configuration files. These could potentially affect how data is loaded into Python dictionaries or other data structures when reading the file.\n- The function 'read_file()' reads in either a JSON or YAML file, depending on what type of file it's given as input. This can be important for handling different types of information and formats within these files.\n- In the context of training language models about Python code, 'py2dataset.save_py2dataset_output.py' could refer to a script that generated output data from running in the program. The specifics of how this file is used may vary depending on its use case and the purpose for which it was created."
    },
    {
        "instruction": "What is the purpose and usage of each of these variables: 'file_type' defined in the function: 'write_file' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "1) Variable \"file_path\":\n   - Purpose: The variable \"file_path\" is being used as an input to the function 'write_file'. It contains the complete path of a file where data will be written.\n   - Usage: Inside the function, it's expected that user inputs are provided for this parameter. These inputs can be paths to json and yaml files or any other kind of files depending on what format is required by the program.\n\n2) Variable \"data\": \n   - Purpose: The variable 'data' holds all the data which will be written to the file at specified path using 'file_path'.\n   - Usage: This data can be a dictionary, list or any other data structure that needs to be saved in a file. \n\n3) Variable \"file_type\": \n   - Purpose: The variable 'file_type' is used as an identifier for the type of file extension. For example, if '.json' is provided then it's expected json file format. Similarly, if '.yaml' is provided then yaml file format will be assumed.\n   - Usage: Inside the function, we are checking this variable to decide which serialization method (json or yaml) should be used for writing data in a file. The 'if-elif' condition check the value of 'file_type'. If it's '.json', we'll use json library methods; if it's '.yaml', we will use PyYAML module methods."
    },
    {
        "instruction": "What is the purpose and usage of each of these variables: 'dataset, html_content, html_file_path, html_file, value' defined in the function: 'convert_json_to_html' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def convert_json_to_html(directory: str) -> None:\n\n    def preserve_spacing(text: str, tab_width: int=4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(' ', '&nbsp;').replace('\\t', '&nbsp;' * tab_width)\n    for json_file in Path(directory).rglob('*.json'):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n        html_file = json_file.with_suffix('.html')\n        html_content = '\\n        <html>\\n        <head>\\n            <style>\\n                table {border-collapse: collapse;width: 100%;}\\n                th, td {border: 1px solid black; padding: 8px; text-align: left; white-space: pre-line;}\\n                td:nth-child(2) { width: 50%; overflow-wrap: anywhere;}\\n            </style>\\n        </head>\\n        <body>\\n            <table>\\n                <thead>\\n                    <tr>\\n        '\n        for key in dataset[0].keys():\n            html_content += f'<th>{key}</th>'\n        html_content += '\\n                    </tr>\\n                </thead>\\n                <tbody>\\n        '\n        for entry in dataset:\n            html_content += '<tr>'\n            for key in entry:\n                value = escape(str(entry[key]))\n                if key == 'input':\n                    value = preserve_spacing(value)\n                value = value.replace('\\n', '<br/>')\n                html_content += f'<td>{value}</td>'\n            html_content += '</tr>'\n        html_content += '\\n                </tbody>\\n            </table>\\n        </body>\\n        </html>\\n        '\n        html_file_path = json_file.with_suffix('.html')\n        try:\n            with open(html_file_path, 'w', encoding='utf-8') as file:\n                file.write(html_content)\n        except:\n            logging.save(logging.info(f'Failed saving: {html_file_path}'))",
        "output": "The purpose and usage of each variable is as follows:\n- 'dataset': This is a list that contains dictionaries representing data from multiple JSON files. Each dictionary represents an entry or record within those files, with the keys being the column names (or attributes) and the values being the actual data. For example, if we have 5 JSON files each containing different sets of information about books, 'dataset' would be a list of dictionaries where each dictionary is one book's details in its own dictionary.\n- 'html_content': This is an HTML string that will contain all the processed dataset into HTML table format. The structure and layout of this HTML page are controlled by variables like 'key', which represents column names, 'value', which contains actual data for each key, etc. For example, if a book has columns 'Title' and 'Author', then html_content would have '<th>Title</th>' and '<th>Author</th>'.\n- 'html_file_path': This is the full path to where we want to write our HTML file. It's typically derived from 'json_file'. For example, if a JSON file was located at '/home/user/books/book1.json', then html_file_path would be '/home/user/books/book1.html'\n- 'html_file': This is an open file object that points to the HTML file where we will write our processed data. The 'with' keyword ensures this file is automatically closed after writing, which is good practice as it avoids having a million random files lying around taking up space on your disk.\n- 'value': These are individual pieces of data from each row in dataset that are being written to the HTML table. They represent actual values for columns in our book records. For instance, if one entry in dataset is {'Title': 'Harry Potter', 'Author': 'J.K. Rowling'}, then 'value' would be 'Harry Potter' and 'J.K. Rowling'.\n\nIn summary:\n- The variable \"dataset\" is a list of dictionaries that contains all the data from multiple JSON files, represented in HTML format. \n- The variables \"html_content\", \"html_file_path\", and \"value\" are used to create an HTML file for each JSON file, and it includes details about every book record found therein. \n- The variable \"html_file\" is the open file object that points to the new HTML file where data will be written. This ensures proper closing of the file after writing, which is a good practice to prevent leaking resources."
    },
    {
        "instruction": "What is the purpose and usage of each of these variables: 'dataset, instruct_data, qa_data, file_names, file_path, cleaned_instruct_file_path, seen_inputs, purpose_file_path, combined_data, keys, purpose_data' defined in the function: 'combine_json_files' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def combine_json_files(directory: str, html: bool) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'qa.json':\n            qa_data = combined_data.copy()\n        if file == 'instruct.json':\n            instruct_data = combined_data.copy()\n        purpose_data = []\n        for item in combined_data:\n            if item[keys[file_names.index(file)]].startswith('Purpose of'):\n                purpose_data.append(item)\n        if purpose_data:\n            purpose_file_path = file_path.with_name(file_path.stem + '_purpose.json')\n            write_file(purpose_data, purpose_file_path)\n        combined_data = []\n    file_names = ['instruct.json', 'instruct_purpose.json']\n    for file in file_names:\n        seen_inputs = set()\n        file_path = Path(directory) / file\n        if not file_path.exists():\n            continue\n        dataset = read_file(file_path)\n        if not dataset:\n            continue\n        for item in dataset:\n            if item['input'] in seen_inputs:\n                item['input'] = ''\n            else:\n                seen_inputs.add(item['input'])\n        cleaned_instruct_file_path = Path(directory) / ('cleaned_' + file)\n        write_file(dataset, cleaned_instruct_file_path)\n    if html:\n        convert_json_to_html(directory)\n    return {'qa_list': qa_data, 'instruct_list': instruct_data}",
        "output": "The variables provided are used within a method named \"combine_json_files\" to clean and combine multiple JSON files into one final dataset, which is then written back to another json file for further processing. Here's an explanation of what each variable does:\n\n1. 'dataset': This is the list containing all instructions in both 'qa' and 'instruct' formats. It gets populated by iterating over 'instruct_data' (the data from instruct files) with a loop, and then appending these instructions to 'dataset'.\n2. 'instruct_data': These are JSON objects representing instructions. Each instruction object has two keys: \"instruction\" for the actual text of the instruction and \"input\" which is an array containing all inputs that were given during the execution process (this is also captured in instruct files). \n3. 'qa_data': This contains data from qa files, where each object represents a question and answer pair. The keys are 'question' for the actual text of the question and 'answer' which is an array containing all correct answers to that particular question (this is captured in qa files). \n4. 'file_names': These are the file names we'll use when processing, this includes \"qa\" and \"instruct\".\n5. 'file_path': This is a Path object representing the path of each file in the directory. It's used to read data from these files.\n6. 'cleaned_instruct_file_path': This is a Path object representing where we'll write our cleaned instructions, without any duplicates.\n7. 'seen_inputs': A set that keeps track of all inputs seen so far while processing the instruct file. If an input has been added to this set once and then comes again, it will be skipped in the final output (to remove duplicate inputs).\n8. 'purpose_file_path': This is a Path object representing where we'll write our purpose data from both qa and instruct files. It's only applicable when processing qa or instruct files.\n9. 'combined_data': A list that will hold the final dataset, which includes all instructions in \"qa\" format followed by all instructions in \"instruct\" format (this is due to how we've structured our data). \n10. 'keys': An array containing two strings representing keys in qa and instruct JSON objects respectively. We're using these later to access specific values from each object when processing the dataset.\n11. 'purpose_data': This is a list that will hold all purpose data, which includes all \"Purpose of\" instructions found while combining json files. \n\nSo overall, we can see that this function is used for cleaning and combining multiple JSON files into one final dataset, mainly to prepare the dataset for further processing in another script (save_py2dataset_output.py). The purpose of each variable is likely due to different purposes or requirements involved with data handling and manipulation as outlined in the provided code."
    },
    {
        "instruction": "What is the purpose and usage of each of these variables: 'label, source, G, edge_data, pos, output_file, target, edge_labels' defined in the function: 'create_code_graph' in the Python file: 'py2dataset.save_py2dataset_output.py'?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'\n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_inputs' in edge:\n                    edge_data['target_inputs'] = edge['target_inputs']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_inputs' in edge[2] and edge[2]['target_inputs']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()",
        "output": "The purpose and usage of each of these variables can be briefly explained as follows:\n\n1. label (nx.draw_networkx_edge_labels): This is a function from matplotlib which provides the labels for network edges when plotted using nx.draw(). \n\n2. source, target, G (G = nx.DiGraph()): These are used to create directed graphs in NetworkX package where each 'source' and 'target' nodes can be connected with edge_data containing other attributes such as 'target_inputs' and 'target_returns'.\n\n3. edge_data: This is a dictionary which contains information about target_inputs and target_returns (if present) for the given source-target node pair in G graph.\n\n4. pos (nx.spring_layout): This function arranges nodes in a circular layout, with the center of the circle at the point (0, 0). \n\n5. output_file: This is the location where we will store and show network graph plot as PNG file using matplotlib's plt.savefig().\n\n6. target, edge_labels ((target, G.edges(data=True))) for each in nx.draw_networkx_edge_labels(G, pos): These are used to provide labels on the edges of the directed graph. The 'for' loop iterates over all pairs of nodes and their corresponding attributes (if any).\n\nIn general, these variables are being used to create a detailed network plot showing each file's internal code structure as well as entire code structure in a systematic manner for better understanding and analysis purpose. This information can be useful when training models that attempt to predict or analyze such characteristics of Python files, like their complexity, size, etc., thus improving the quality of generated datasets."
    }
]