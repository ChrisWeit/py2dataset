[
    {
        "instruction": "Dependencies in Python file: `py2dataset.py2dataset.py`?",
        "input": "```python\n\"\"\"\nFor each Python file within a given directory, this module is designed to generate, save, \nand return datasets that include responses to questions about the code. \nRequirements:\n[req00] The process_single_python_file function shall:\n    a. Accept parameters for Python file path, start directory, model config \n       pathname, questions dict, use of LLM, and output dir.\n    b. If 'use_llm' is True, use 'get_model' to instantiate LLM model config.\n    c. Use 'get_python_file_details' to retrieve Python file info.\n    d. Use 'get_python_datasets' to acquire instruct.json datasets.\n    e. Use 'save_python_data' to store file details and instruct.json data.\n[req01] The py2dataset function shall:\n    a. Accept parameters for start directory, output dir, questions path, model\n       config path, use of LLM, and quiet mode.\n    b. Adjust logging level based on 'quiet'.\n    c. Use current working dir if no valid start dir is provided.\n    d. Get output dir with 'get_output_dir'.\n    e. Retrieve questions dict with 'get_questions'.\n    f. Search for Python files using 'rglob', excluding those starting with \"_\".\n    g. For each Python file, spawn a child process with 'process_single_python_file'\n       to get file details and instruct.json data, if single_process is False.\n    h. Combine instruct.json files with 'combine_json_files'.\n    i. Return datasets.\n[req02] The main function shall:\n    a. Accept and process command-line args.\n    b. Determine py2dataset parameters based on processed arguments.\n    c. Call py2dataset with derived parameters.\n\"\"\"\nimport sys\nimport argparse\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom multiprocessing import Process\nimport subprocess\nimport os\nimport git\nimport shlex\n\nfrom get_python_file_details import get_python_file_details\nfrom get_python_datasets import get_python_datasets\nfrom get_params import (\n    get_questions,\n    get_model,\n    get_output_dir,\n    get_start_dir,\n)\nfrom save_output import combine_json_files, save_python_data\n\n\ndef process_single_python_file(\n    python_file_path: str,\n    start_dir: str,\n    model_config_path: str,\n    questions_dict: Dict,\n    use_llm: bool,\n    output_dir: str,\n    model_config: Dict = None,\n    single_process_mode: bool = False,\n    detailed_analysis: bool = False,\n) -> None:\n    \"\"\"\n    Processes a single Python file to generate question-answer pairs and instructions.\n\n    Args:\n        python_file_path (str): Path to the Python file.\n        start_dir (str): Starting directory to search for Python files.\n        model_config_path (str): Path to the model configuration file.\n        questions_dict (Dict): Dictionary of questions to answer about the Python file.\n        use_llm (bool): If True, use a Large Language Model for generating JSON answers.\n        output_dir (str): Directory to save the output files.\n        model_config (Dict): Configuration dictionary for the LLM.\n        single_process_mode (bool): Use a single process if True. Defaults to False.\n        detailed_analysis (bool): Perform detailed analysis if True. Defaults to False.\n    \"\"\"\n    logging.info(f\"Processing file: {python_file_path}\")\n\n    # need to define relative path as the having at least one parent directory\n    # e.g relative patch should be equal to <last start_directory directory> / file name without path from python_file_path\n    # Get the last directory in start_dir\n    parent_dir = os.path.dirname(start_dir)\n    relative_path = os.path.relpath(python_file_path, parent_dir)\n    relative_path = Path(relative_path)\n    base_name = \".\".join(relative_path.parts)\n\n    if not single_process_mode and use_llm:\n        model_config = get_model(model_config_path)\n\n    file_details = get_python_file_details(python_file_path)\n    if not file_details:\n        logging.error(f\"Failed to get file details for {python_file_path}\")\n        return\n\n    instruct_data = get_python_datasets(\n        python_file_path,\n        file_details,\n        base_name,\n        questions_dict,\n        model_config,\n        detailed_analysis,\n    )\n\n    if instruct_data:\n        save_python_data(\n            file_details, instruct_data, base_name, relative_path, output_dir\n        )\n    else:\n        logging.error(f\"Failed to get instruct data for {python_file_path}\")\n\n\ndef py2dataset(\n    start_dir: str = \"\",\n    output_dir: str = \"\",\n    questions_pathname: str = \"\",\n    model_config_pathname: str = \"\",\n    use_llm: bool = False,\n    quiet: bool = False,\n    single_process: bool = False,\n    detailed: bool = False,\n    html: bool = False,\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start_dir (str): Starting directory for Python files. Defaults to current directory.\n        output_dir (str): Directory to save the output files.\n        questions_pathname (str): Path and filename of the questions file.\n        model_config_pathname (str): Path and filename of the model configuration file.\n        use_llm (bool): If True, use a Large Language Model for generating answers. Defaults to False.\n        quiet_mode (bool): Reduce logging output if True. Defaults to False.\n        single_process (bool): Use a single process for file processing if use_llm. Defaults to False.\n        detailed (bool): Include detailed analysis if True. Defaults to False.\n        html (bool): Generate HTML outputs if True. Defaults to False.\n    Returns:\n        Dict[str, List[Dict]]: Dictionary of generated datasets.\n    \"\"\"\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n\n    sys.setrecursionlimit(3000)  # Set recursion limit higher for AST parsing\n\n    start_dir = get_start_dir(start_dir)\n    output_dir = get_output_dir(output_dir)\n    questions_dict = get_questions(questions_pathname)\n\n    # Load model configuration if LLM is used and single process mode is enabled\n    model_config = (\n        get_model(model_config_pathname) if use_llm and single_process else None\n    )\n\n    if not use_llm:\n        single_process = True\n\n    # Process each Python file in the directory\n    for python_file_path in Path(start_dir).rglob(\"[!_]*.py\"):\n        # if use_llm is false or single_process is true then process the file within the current process\n\n        if not use_llm and single_process:\n            process_single_python_file(\n                python_file_path,\n                start_dir,\n                model_config_pathname,\n                questions_dict,\n                use_llm,\n                output_dir,\n                model_config,\n                single_process,\n                detailed,\n            )\n        else:\n            # Spawn new process for each file to manage memory and performance\n            proc = Process(\n                target=process_single_python_file,\n                args=(\n                    python_file_path,\n                    start_dir,\n                    model_config_pathname,\n                    questions_dict,\n                    use_llm,\n                    output_dir,\n                    None,\n                    single_process,\n                    detailed,\n                ),\n            )\n            proc.start()\n            proc.join()\n\n    # Combine all the individual datasets into a single dictionary\n    return combine_json_files(output_dir, html)\n\n\ndef clone_github_repo(url: str) -> str:\n    \"\"\"\n    Clone repository or pull the latest changes and return local repository path.\n    Args:\n        url (str): The url of the github repository.\n    Returns:\n        str: The path to the cloned repository.\n    \"\"\"\n    # Check valid Git repository\n    try:\n        command = f\"git ls-remote {shlex.quote(url)}\"\n        subprocess.run(\n            command,\n            shell=True,\n            check=True,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n        )\n    except subprocess.CalledProcessError:\n        print(f\"Invalid or inaccessible repository: {url}\")\n        return \"\"\n\n    # Proceed with cloning or fetching\n    repo_name = url.split(\"/\")[-1]\n    githubrepos_dir = os.path.join(os.getcwd(), \"githubrepos\")\n    os.makedirs(githubrepos_dir, exist_ok=True)\n    path = os.path.join(githubrepos_dir, repo_name)\n    if not os.path.exists(path):\n        git.Repo.clone_from(url, path)\n    else:\n        repo = git.Repo(path)\n        with repo.git.custom_environment(\n            GIT_SSH_COMMAND=\"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no\"\n        ):\n            repo.git.fetch()\n            default_branch = repo.head.reference.tracking_branch().remote_head\n            repo.git.reset(\"--hard\", default_branch)\n    return path\n\n\ndef main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Optional command-line arguments:\n    --start (str, optional): Starting directory for Python files or GitHub repository Python files. Defaults to current working directory.\n    --output_dir (str, optional): Directory to write the output files. Defaults to ./dataset/.\n    --questions_pathname (str, optional): Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\n    --model_config_pathname (str, optional): Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\n    --use_llm (bool, optional): Use llm for generating JSON answers. Defaults to False.\n    --quiet (bool, optional): Limit logging output. Defaults to False.\n    --single_process (bool, optional): If True, only a single process will be used to process Python files. Defaults to False.\n    --detailed (bool, optional): Include detailed analysis if True. Defaults to False.\n    --html (bool, optional): Generate HTML output if True. Defaults to False.\n    --I (str, optional): Interactive mode. Defaults to False.\n    \"\"\"\n\n    class PathArgument(argparse.Action):\n        def __call__(self, parser, namespace, values, option_string=None):\n            cleaned_value = values.strip('\"').strip(\"'\")\n            setattr(namespace, self.dest, cleaned_value)\n\n    def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n        \"\"\"\n        Get a boolean value from user input or retain the current value if the input is invalid.\n        \"\"\"\n        return (\n            True\n            if input_str.lower() in [\"t\", \"true\"]\n            else False\n            if input_str.lower() in [\"f\", \"false\"]\n            else current_value\n        )\n\n    # parse each command line entry\n    parser = argparse.ArgumentParser(\n        description=\"Process Python files to generate datasets.\"\n    )\n    parser.add_argument(\n        \"--start\",\n        default=\".\",\n        action=PathArgument,\n        help=\"Starting directory for Python files. Defaults to current working directory.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        default=\"./dataset/\",\n        action=PathArgument,\n        help=\"Directory to write the output files. Defaults to ./dataset/.\",\n    )\n    parser.add_argument(\n        \"--questions_pathname\",\n        default=\"./py2dataset_questions.json\",\n        action=PathArgument,\n        help=\"Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\",\n    )\n    parser.add_argument(\n        \"--model_config_pathname\",\n        default=\"./py2dataset_model_config.yaml\",\n        action=PathArgument,\n        help=\"Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\",\n    )\n    parser.add_argument(\n        \"--use_llm\",\n        action=\"store_true\",\n        help=\"Use LLM for generating JSON answers. Defaults to False.\",\n    )\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Limit logging output.\")\n    parser.add_argument(\n        \"--single_process\",\n        action=\"store_true\",\n        help=\"Use a single process for processing Python files. Defaults to False.\",\n    )\n    parser.add_argument(\n        \"--detailed\", action=\"store_true\", help=\"Include detailed analysis if True.\"\n    )\n    parser.add_argument(\n        \"--html\", action=\"store_true\", help=\"Generate HTML output if True.\"\n    )\n    parser.add_argument(\n        \"--I\",\n        \"--interactive\",\n        action=\"store_true\",\n        dest=\"interactive\",\n        help=\"Interactive mode.\",\n    )\n    args = parser.parse_args()\n\n    # Interactive mode adjustments\n    if args.interactive:\n        print(\"Input new value or press enter to keep current value.\")\n        pathname_params = [\n            \"start\",\n            \"output_dir\",\n            \"questions_pathname\",\n            \"model_config_pathname\",\n        ]\n        bool_params = [\"use_llm\", \"quiet\", \"single_process\", \"detailed\", \"html\"]\n        for param in pathname_params:\n            setattr(\n                args,\n                param,\n                input(f\"{param} [{getattr(args, param)}]: \") or getattr(args, param),\n            )\n        for param in bool_params:\n            setattr(\n                args,\n                param,\n                get_bool_from_input(\n                    input(f\"{param} [{getattr(args, param)}] (t or f): \"),\n                    getattr(args, param),\n                ),\n            )\n\n    # Validate the start directory\n    if not (os.path.isdir(args.start) or args.start.startswith(\"https://github.com/\")):\n        print(\n            f\"Invalid start directory '{args.start}'. Using current working directory.\"\n        )\n        args.start = os.getcwd()\n\n    # if the start directory is a github repository, clone it and change the start directory to the local repository\n    if args.start.startswith(\"https://github.com/\"):\n        args.start = clone_github_repo(args.start)\n\n    # Call py2dataset with args\n    py2dataset(\n        start_dir=args.start,\n        output_dir=args.output_dir,\n        questions_pathname=args.questions_pathname,\n        model_config_pathname=args.model_config_pathname,\n        use_llm=args.use_llm,\n        quiet=args.quiet,\n        single_process=args.single_process,\n        detailed=args.detailed,\n        html=args.html,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n\n```",
        "output": "Dependencies:pathlib, os, get_params, shlex, save_output, argparse, sys, get_python_file_details, logging, git, typing, subprocess, multiprocessing, get_python_datasets\n\nPurpose and Significance:\nThe code imports the following dependencies to fulfill its purpose of generating question-answer pairs and instructions for a given directory or repository of Python files: \n\n1. pathlib: A built-in library in Python that provides support for handling file and directory paths. It is used to manipulate paths and create directories if they don't exist.\n2. os: The module provides a way of using operating system dependent functionality like working with environment variables, reading the files on a machine and handling operations like deleting, moving, and creating folders or directories.\n3. get_params: This function retrieves required parameters for the py2dataset function such as start directory, output directory, questions path, model config path, use of LLM, and quiet mode from command-line arguments.\n4. shlex: A module used to parse strings containing commands with syntax similar to the shell. It is used to process user input for some interactive mode features in the main function. \n5. argparse: This module is used to create an argument parser that can be used to define and parse command-line arguments passed to a Python script. In this code, it is used to get the values of various parameters such as start directory, output directory, questions path, model config path, use of LLM, quiet mode, single process mode, detailed analysis, and HTML generation from user input.\n6. subprocess: The subprocess module provides the ability to execute programs or call other system functions. In this code, it is used for git command line execution with options.\n7. typing: It is a built-in library in Python that allows us to define custom types and type hints. \n8. save_output: This function saves the file details and instruct.json data into a JSON format.\n9. get_python_file_details: This function retrieves information about the Python files such as number of lines, imports, and functions used in the code.\n10. get_questions: This function retrieves questions from a given JSON file. \n11. get_output_dir: This function gets the output directory based on the input argument or the current working directory. \n12. git: It is used to clone or pull GitHub repositories using Git.\n13. multiprocessing: A module that supports spawning processes to run concurrently and communicate with them. In this code, it is used for parallel processing of Python files. \n14. Process: This class is used to create new process from a function or a callable object. It is used to spawn child processes in the py2dataset function. \n15. combine_json_files: This function combines all the individual datasets into a single dictionary and returns it. \n\nThe main function is responsible for processing command-line arguments, determining py2dataset parameters based on the processed arguments, and calling the py2dataset function with derived parameters. "
    },
    {
        "instruction": "Call code graph in Python file: `py2dataset.py2dataset.py`?",
        "input": "```python\n\"\"\"\nFor each Python file within a given directory, this module is designed to generate, save, \nand return datasets that include responses to questions about the code. \nRequirements:\n[req00] The process_single_python_file function shall:\n    a. Accept parameters for Python file path, start directory, model config \n       pathname, questions dict, use of LLM, and output dir.\n    b. If 'use_llm' is True, use 'get_model' to instantiate LLM model config.\n    c. Use 'get_python_file_details' to retrieve Python file info.\n    d. Use 'get_python_datasets' to acquire instruct.json datasets.\n    e. Use 'save_python_data' to store file details and instruct.json data.\n[req01] The py2dataset function shall:\n    a. Accept parameters for start directory, output dir, questions path, model\n       config path, use of LLM, and quiet mode.\n    b. Adjust logging level based on 'quiet'.\n    c. Use current working dir if no valid start dir is provided.\n    d. Get output dir with 'get_output_dir'.\n    e. Retrieve questions dict with 'get_questions'.\n    f. Search for Python files using 'rglob', excluding those starting with \"_\".\n    g. For each Python file, spawn a child process with 'process_single_python_file'\n       to get file details and instruct.json data, if single_process is False.\n    h. Combine instruct.json files with 'combine_json_files'.\n    i. Return datasets.\n[req02] The main function shall:\n    a. Accept and process command-line args.\n    b. Determine py2dataset parameters based on processed arguments.\n    c. Call py2dataset with derived parameters.\n\"\"\"\nimport sys\nimport argparse\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom multiprocessing import Process\nimport subprocess\nimport os\nimport git\nimport shlex\n\nfrom get_python_file_details import get_python_file_details\nfrom get_python_datasets import get_python_datasets\nfrom get_params import (\n    get_questions,\n    get_model,\n    get_output_dir,\n    get_start_dir,\n)\nfrom save_output import combine_json_files, save_python_data\n\n\ndef process_single_python_file(\n    python_file_path: str,\n    start_dir: str,\n    model_config_path: str,\n    questions_dict: Dict,\n    use_llm: bool,\n    output_dir: str,\n    model_config: Dict = None,\n    single_process_mode: bool = False,\n    detailed_analysis: bool = False,\n) -> None:\n    \"\"\"\n    Processes a single Python file to generate question-answer pairs and instructions.\n\n    Args:\n        python_file_path (str): Path to the Python file.\n        start_dir (str): Starting directory to search for Python files.\n        model_config_path (str): Path to the model configuration file.\n        questions_dict (Dict): Dictionary of questions to answer about the Python file.\n        use_llm (bool): If True, use a Large Language Model for generating JSON answers.\n        output_dir (str): Directory to save the output files.\n        model_config (Dict): Configuration dictionary for the LLM.\n        single_process_mode (bool): Use a single process if True. Defaults to False.\n        detailed_analysis (bool): Perform detailed analysis if True. Defaults to False.\n    \"\"\"\n    logging.info(f\"Processing file: {python_file_path}\")\n\n    # need to define relative path as the having at least one parent directory\n    # e.g relative patch should be equal to <last start_directory directory> / file name without path from python_file_path\n    # Get the last directory in start_dir\n    parent_dir = os.path.dirname(start_dir)\n    relative_path = os.path.relpath(python_file_path, parent_dir)\n    relative_path = Path(relative_path)\n    base_name = \".\".join(relative_path.parts)\n\n    if not single_process_mode and use_llm:\n        model_config = get_model(model_config_path)\n\n    file_details = get_python_file_details(python_file_path)\n    if not file_details:\n        logging.error(f\"Failed to get file details for {python_file_path}\")\n        return\n\n    instruct_data = get_python_datasets(\n        python_file_path,\n        file_details,\n        base_name,\n        questions_dict,\n        model_config,\n        detailed_analysis,\n    )\n\n    if instruct_data:\n        save_python_data(\n            file_details, instruct_data, base_name, relative_path, output_dir\n        )\n    else:\n        logging.error(f\"Failed to get instruct data for {python_file_path}\")\n\n\ndef py2dataset(\n    start_dir: str = \"\",\n    output_dir: str = \"\",\n    questions_pathname: str = \"\",\n    model_config_pathname: str = \"\",\n    use_llm: bool = False,\n    quiet: bool = False,\n    single_process: bool = False,\n    detailed: bool = False,\n    html: bool = False,\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start_dir (str): Starting directory for Python files. Defaults to current directory.\n        output_dir (str): Directory to save the output files.\n        questions_pathname (str): Path and filename of the questions file.\n        model_config_pathname (str): Path and filename of the model configuration file.\n        use_llm (bool): If True, use a Large Language Model for generating answers. Defaults to False.\n        quiet_mode (bool): Reduce logging output if True. Defaults to False.\n        single_process (bool): Use a single process for file processing if use_llm. Defaults to False.\n        detailed (bool): Include detailed analysis if True. Defaults to False.\n        html (bool): Generate HTML outputs if True. Defaults to False.\n    Returns:\n        Dict[str, List[Dict]]: Dictionary of generated datasets.\n    \"\"\"\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n\n    sys.setrecursionlimit(3000)  # Set recursion limit higher for AST parsing\n\n    start_dir = get_start_dir(start_dir)\n    output_dir = get_output_dir(output_dir)\n    questions_dict = get_questions(questions_pathname)\n\n    # Load model configuration if LLM is used and single process mode is enabled\n    model_config = (\n        get_model(model_config_pathname) if use_llm and single_process else None\n    )\n\n    if not use_llm:\n        single_process = True\n\n    # Process each Python file in the directory\n    for python_file_path in Path(start_dir).rglob(\"[!_]*.py\"):\n        # if use_llm is false or single_process is true then process the file within the current process\n\n        if not use_llm and single_process:\n            process_single_python_file(\n                python_file_path,\n                start_dir,\n                model_config_pathname,\n                questions_dict,\n                use_llm,\n                output_dir,\n                model_config,\n                single_process,\n                detailed,\n            )\n        else:\n            # Spawn new process for each file to manage memory and performance\n            proc = Process(\n                target=process_single_python_file,\n                args=(\n                    python_file_path,\n                    start_dir,\n                    model_config_pathname,\n                    questions_dict,\n                    use_llm,\n                    output_dir,\n                    None,\n                    single_process,\n                    detailed,\n                ),\n            )\n            proc.start()\n            proc.join()\n\n    # Combine all the individual datasets into a single dictionary\n    return combine_json_files(output_dir, html)\n\n\ndef clone_github_repo(url: str) -> str:\n    \"\"\"\n    Clone repository or pull the latest changes and return local repository path.\n    Args:\n        url (str): The url of the github repository.\n    Returns:\n        str: The path to the cloned repository.\n    \"\"\"\n    # Check valid Git repository\n    try:\n        command = f\"git ls-remote {shlex.quote(url)}\"\n        subprocess.run(\n            command,\n            shell=True,\n            check=True,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n        )\n    except subprocess.CalledProcessError:\n        print(f\"Invalid or inaccessible repository: {url}\")\n        return \"\"\n\n    # Proceed with cloning or fetching\n    repo_name = url.split(\"/\")[-1]\n    githubrepos_dir = os.path.join(os.getcwd(), \"githubrepos\")\n    os.makedirs(githubrepos_dir, exist_ok=True)\n    path = os.path.join(githubrepos_dir, repo_name)\n    if not os.path.exists(path):\n        git.Repo.clone_from(url, path)\n    else:\n        repo = git.Repo(path)\n        with repo.git.custom_environment(\n            GIT_SSH_COMMAND=\"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no\"\n        ):\n            repo.git.fetch()\n            default_branch = repo.head.reference.tracking_branch().remote_head\n            repo.git.reset(\"--hard\", default_branch)\n    return path\n\n\ndef main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Optional command-line arguments:\n    --start (str, optional): Starting directory for Python files or GitHub repository Python files. Defaults to current working directory.\n    --output_dir (str, optional): Directory to write the output files. Defaults to ./dataset/.\n    --questions_pathname (str, optional): Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\n    --model_config_pathname (str, optional): Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\n    --use_llm (bool, optional): Use llm for generating JSON answers. Defaults to False.\n    --quiet (bool, optional): Limit logging output. Defaults to False.\n    --single_process (bool, optional): If True, only a single process will be used to process Python files. Defaults to False.\n    --detailed (bool, optional): Include detailed analysis if True. Defaults to False.\n    --html (bool, optional): Generate HTML output if True. Defaults to False.\n    --I (str, optional): Interactive mode. Defaults to False.\n    \"\"\"\n\n    class PathArgument(argparse.Action):\n        def __call__(self, parser, namespace, values, option_string=None):\n            cleaned_value = values.strip('\"').strip(\"'\")\n            setattr(namespace, self.dest, cleaned_value)\n\n    def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n        \"\"\"\n        Get a boolean value from user input or retain the current value if the input is invalid.\n        \"\"\"\n        return (\n            True\n            if input_str.lower() in [\"t\", \"true\"]\n            else False\n            if input_str.lower() in [\"f\", \"false\"]\n            else current_value\n        )\n\n    # parse each command line entry\n    parser = argparse.ArgumentParser(\n        description=\"Process Python files to generate datasets.\"\n    )\n    parser.add_argument(\n        \"--start\",\n        default=\".\",\n        action=PathArgument,\n        help=\"Starting directory for Python files. Defaults to current working directory.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        default=\"./dataset/\",\n        action=PathArgument,\n        help=\"Directory to write the output files. Defaults to ./dataset/.\",\n    )\n    parser.add_argument(\n        \"--questions_pathname\",\n        default=\"./py2dataset_questions.json\",\n        action=PathArgument,\n        help=\"Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\",\n    )\n    parser.add_argument(\n        \"--model_config_pathname\",\n        default=\"./py2dataset_model_config.yaml\",\n        action=PathArgument,\n        help=\"Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\",\n    )\n    parser.add_argument(\n        \"--use_llm\",\n        action=\"store_true\",\n        help=\"Use LLM for generating JSON answers. Defaults to False.\",\n    )\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Limit logging output.\")\n    parser.add_argument(\n        \"--single_process\",\n        action=\"store_true\",\n        help=\"Use a single process for processing Python files. Defaults to False.\",\n    )\n    parser.add_argument(\n        \"--detailed\", action=\"store_true\", help=\"Include detailed analysis if True.\"\n    )\n    parser.add_argument(\n        \"--html\", action=\"store_true\", help=\"Generate HTML output if True.\"\n    )\n    parser.add_argument(\n        \"--I\",\n        \"--interactive\",\n        action=\"store_true\",\n        dest=\"interactive\",\n        help=\"Interactive mode.\",\n    )\n    args = parser.parse_args()\n\n    # Interactive mode adjustments\n    if args.interactive:\n        print(\"Input new value or press enter to keep current value.\")\n        pathname_params = [\n            \"start\",\n            \"output_dir\",\n            \"questions_pathname\",\n            \"model_config_pathname\",\n        ]\n        bool_params = [\"use_llm\", \"quiet\", \"single_process\", \"detailed\", \"html\"]\n        for param in pathname_params:\n            setattr(\n                args,\n                param,\n                input(f\"{param} [{getattr(args, param)}]: \") or getattr(args, param),\n            )\n        for param in bool_params:\n            setattr(\n                args,\n                param,\n                get_bool_from_input(\n                    input(f\"{param} [{getattr(args, param)}] (t or f): \"),\n                    getattr(args, param),\n                ),\n            )\n\n    # Validate the start directory\n    if not (os.path.isdir(args.start) or args.start.startswith(\"https://github.com/\")):\n        print(\n            f\"Invalid start directory '{args.start}'. Using current working directory.\"\n        )\n        args.start = os.getcwd()\n\n    # if the start directory is a github repository, clone it and change the start directory to the local repository\n    if args.start.startswith(\"https://github.com/\"):\n        args.start = clone_github_repo(args.start)\n\n    # Call py2dataset with args\n    py2dataset(\n        start_dir=args.start,\n        output_dir=args.output_dir,\n        questions_pathname=args.questions_pathname,\n        model_config_pathname=args.model_config_pathname,\n        use_llm=args.use_llm,\n        quiet=args.quiet,\n        single_process=args.single_process,\n        detailed=args.detailed,\n        html=args.html,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n\n```",
        "output": "{'nodes': ['PathArgument', 'PathArgument.__call__', 'process_single_python_file', 'py2dataset', 'clone_github_repo', 'main', 'get_bool_from_input', 'logging.info', 'os.path.dirname', 'os.path.relpath', 'Path', \"'.'.join\", 'get_model', 'get_python_file_details', 'logging.error', 'get_python_datasets', 'save_python_data', 'logging.getLogger().setLevel', 'logging.getLogger', 'sys.setrecursionlimit', 'get_start_dir', 'get_output_dir', 'get_questions', 'Path(start_dir).rglob', 'Process', 'proc.start', 'proc.join', 'combine_json_files', 'shlex.quote', 'subprocess.run', 'print', 'url.split', 'os.path.join', 'os.getcwd', 'os.makedirs', 'os.path.exists', 'git.Repo.clone_from', 'git.Repo', 'repo.git.custom_environment', 'repo.git.fetch', 'repo.head.reference.tracking_branch', 'repo.git.reset', 'values.strip(\\'\"\\').strip', 'values.strip', 'setattr', 'input_str.lower', 'argparse.ArgumentParser', 'parser.add_argument', 'parser.parse_args', 'input', 'getattr', 'os.path.isdir', 'args.start.startswith'], 'edges': [{'source': 'PathArgument', 'target': 'PathArgument.__call__', 'target_inputs': ['self', 'parser', 'namespace', 'values', 'option_string'], 'target_returns': []}, {'source': 'PathArgument.__call__', 'target': 'values.strip(\\'\"\\').strip', 'target_inputs': ['\"\\'\"']}, {'source': 'PathArgument.__call__', 'target': 'values.strip', 'target_inputs': ['\\'\"\\'']}, {'source': 'PathArgument.__call__', 'target': 'setattr', 'target_inputs': ['namespace', 'self.dest', 'cleaned_value']}, {'source': 'process_single_python_file', 'target': 'logging.info', 'target_inputs': [\"f'Processing file: {python_file_path}'\"]}, {'source': 'process_single_python_file', 'target': 'os.path.dirname', 'target_inputs': ['start_dir']}, {'source': 'process_single_python_file', 'target': 'os.path.relpath', 'target_inputs': ['python_file_path', 'parent_dir']}, {'source': 'process_single_python_file', 'target': 'Path', 'target_inputs': ['relative_path']}, {'source': 'process_single_python_file', 'target': \"'.'.join\", 'target_inputs': ['relative_path.parts']}, {'source': 'process_single_python_file', 'target': 'get_model', 'target_inputs': ['model_config_path']}, {'source': 'process_single_python_file', 'target': 'get_python_file_details', 'target_inputs': ['python_file_path']}, {'source': 'process_single_python_file', 'target': 'logging.error', 'target_inputs': [\"f'Failed to get file details for {python_file_path}'\", \"f'Failed to get instruct data for {python_file_path}'\"]}, {'source': 'process_single_python_file', 'target': 'get_python_datasets', 'target_inputs': ['python_file_path', 'file_details', 'base_name', 'questions_dict', 'model_config', 'detailed_analysis']}, {'source': 'process_single_python_file', 'target': 'save_python_data', 'target_inputs': ['file_details', 'instruct_data', 'base_name', 'relative_path', 'output_dir']}, {'source': 'py2dataset', 'target': 'logging.getLogger().setLevel', 'target_inputs': ['logging.WARNING', 'logging.INFO']}, {'source': 'py2dataset', 'target': 'logging.getLogger', 'target_inputs': []}, {'source': 'py2dataset', 'target': 'sys.setrecursionlimit', 'target_inputs': ['3000']}, {'source': 'py2dataset', 'target': 'get_start_dir', 'target_inputs': ['start_dir']}, {'source': 'py2dataset', 'target': 'get_output_dir', 'target_inputs': ['output_dir']}, {'source': 'py2dataset', 'target': 'get_questions', 'target_inputs': ['questions_pathname']}, {'source': 'py2dataset', 'target': 'get_model', 'target_inputs': ['model_config_pathname']}, {'source': 'py2dataset', 'target': 'Path(start_dir).rglob', 'target_inputs': [\"'[!_]*.py'\"]}, {'source': 'py2dataset', 'target': 'Path', 'target_inputs': ['start_dir']}, {'source': 'py2dataset', 'target': 'process_single_python_file', 'target_inputs': ['python_file_path', 'start_dir', 'model_config_pathname', 'questions_dict', 'use_llm', 'output_dir', 'model_config', 'single_process', 'detailed'], 'target_returns': ['None']}, {'source': 'py2dataset', 'target': 'Process', 'target_inputs': []}, {'source': 'py2dataset', 'target': 'proc.start', 'target_inputs': []}, {'source': 'py2dataset', 'target': 'proc.join', 'target_inputs': []}, {'source': 'py2dataset', 'target': 'combine_json_files', 'target_inputs': ['output_dir', 'html']}, {'source': 'clone_github_repo', 'target': 'shlex.quote', 'target_inputs': ['url']}, {'source': 'clone_github_repo', 'target': 'subprocess.run', 'target_inputs': ['command']}, {'source': 'clone_github_repo', 'target': 'print', 'target_inputs': [\"f'Invalid or inaccessible repository: {url}'\"]}, {'source': 'clone_github_repo', 'target': 'url.split', 'target_inputs': [\"'/'\"]}, {'source': 'clone_github_repo', 'target': 'os.path.join', 'target_inputs': ['os.getcwd()', \"'githubrepos'\", 'githubrepos_dir', 'repo_name']}, {'source': 'clone_github_repo', 'target': 'os.getcwd', 'target_inputs': []}, {'source': 'clone_github_repo', 'target': 'os.makedirs', 'target_inputs': ['githubrepos_dir']}, {'source': 'clone_github_repo', 'target': 'os.path.exists', 'target_inputs': ['path']}, {'source': 'clone_github_repo', 'target': 'git.Repo.clone_from', 'target_inputs': ['url', 'path']}, {'source': 'clone_github_repo', 'target': 'git.Repo', 'target_inputs': ['path']}, {'source': 'clone_github_repo', 'target': 'repo.git.custom_environment', 'target_inputs': []}, {'source': 'clone_github_repo', 'target': 'repo.git.fetch', 'target_inputs': []}, {'source': 'clone_github_repo', 'target': 'repo.head.reference.tracking_branch', 'target_inputs': []}, {'source': 'clone_github_repo', 'target': 'repo.git.reset', 'target_inputs': [\"'--hard'\", 'default_branch']}, {'source': 'main', 'target': 'values.strip(\\'\"\\').strip', 'target_inputs': ['\"\\'\"']}, {'source': 'main', 'target': 'values.strip', 'target_inputs': ['\\'\"\\'']}, {'source': 'main', 'target': 'setattr', 'target_inputs': ['namespace', 'self.dest', 'cleaned_value', 'args', 'param', \"input(f'{param} [{getattr(args, param)}]: ') or getattr(args, param)\", 'args', 'param', \"get_bool_from_input(input(f'{param} [{getattr(args, param)}] (t or f): '), getattr(args, param))\"]}, {'source': 'main', 'target': 'input_str.lower', 'target_inputs': []}, {'source': 'main', 'target': 'argparse.ArgumentParser', 'target_inputs': []}, {'source': 'main', 'target': 'parser.add_argument', 'target_inputs': [\"'--start'\", \"'--output_dir'\", \"'--questions_pathname'\", \"'--model_config_pathname'\", \"'--use_llm'\", \"'--quiet'\", \"'--single_process'\", \"'--detailed'\", \"'--html'\", \"'--I'\", \"'--interactive'\"]}, {'source': 'main', 'target': 'parser.parse_args', 'target_inputs': []}, {'source': 'main', 'target': 'print', 'target_inputs': [\"'Input new value or press enter to keep current value.'\", 'f\"Invalid start directory \\'{args.start}\\'. Using current working directory.\"']}, {'source': 'main', 'target': 'input', 'target_inputs': [\"f'{param} [{getattr(args, param)}]: '\", \"f'{param} [{getattr(args, param)}] (t or f): '\"]}, {'source': 'main', 'target': 'getattr', 'target_inputs': ['args', 'param', 'args', 'param', 'args', 'param', 'args', 'param']}, {'source': 'main', 'target': 'get_bool_from_input', 'target_inputs': [\"input(f'{param} [{getattr(args, param)}] (t or f): ')\", 'getattr(args, param)'], 'target_returns': [\"True if input_str.lower() in ['t', 'true'] else False if input_str.lower() in ['f', 'false'] else current_value\"]}, {'source': 'main', 'target': 'os.path.isdir', 'target_inputs': ['args.start']}, {'source': 'main', 'target': 'args.start.startswith', 'target_inputs': [\"'https://github.com/'\", \"'https://github.com/'\"]}, {'source': 'main', 'target': 'os.getcwd', 'target_inputs': []}, {'source': 'main', 'target': 'clone_github_repo', 'target_inputs': ['args.start'], 'target_returns': [\"''\", 'path']}, {'source': 'main', 'target': 'py2dataset', 'target_inputs': [], 'target_returns': ['combine_json_files(output_dir, html)']}, {'source': 'get_bool_from_input', 'target': 'input_str.lower', 'target_inputs': []}]}"
    },
    {
        "instruction": "Functions defined in Python file: `py2dataset.py2dataset.py`?",
        "input": "```python\n\"\"\"\nFor each Python file within a given directory, this module is designed to generate, save, \nand return datasets that include responses to questions about the code. \nRequirements:\n[req00] The process_single_python_file function shall:\n    a. Accept parameters for Python file path, start directory, model config \n       pathname, questions dict, use of LLM, and output dir.\n    b. If 'use_llm' is True, use 'get_model' to instantiate LLM model config.\n    c. Use 'get_python_file_details' to retrieve Python file info.\n    d. Use 'get_python_datasets' to acquire instruct.json datasets.\n    e. Use 'save_python_data' to store file details and instruct.json data.\n[req01] The py2dataset function shall:\n    a. Accept parameters for start directory, output dir, questions path, model\n       config path, use of LLM, and quiet mode.\n    b. Adjust logging level based on 'quiet'.\n    c. Use current working dir if no valid start dir is provided.\n    d. Get output dir with 'get_output_dir'.\n    e. Retrieve questions dict with 'get_questions'.\n    f. Search for Python files using 'rglob', excluding those starting with \"_\".\n    g. For each Python file, spawn a child process with 'process_single_python_file'\n       to get file details and instruct.json data, if single_process is False.\n    h. Combine instruct.json files with 'combine_json_files'.\n    i. Return datasets.\n[req02] The main function shall:\n    a. Accept and process command-line args.\n    b. Determine py2dataset parameters based on processed arguments.\n    c. Call py2dataset with derived parameters.\n\"\"\"\nimport sys\nimport argparse\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom multiprocessing import Process\nimport subprocess\nimport os\nimport git\nimport shlex\n\nfrom get_python_file_details import get_python_file_details\nfrom get_python_datasets import get_python_datasets\nfrom get_params import (\n    get_questions,\n    get_model,\n    get_output_dir,\n    get_start_dir,\n)\nfrom save_output import combine_json_files, save_python_data\n\n\ndef process_single_python_file(\n    python_file_path: str,\n    start_dir: str,\n    model_config_path: str,\n    questions_dict: Dict,\n    use_llm: bool,\n    output_dir: str,\n    model_config: Dict = None,\n    single_process_mode: bool = False,\n    detailed_analysis: bool = False,\n) -> None:\n    \"\"\"\n    Processes a single Python file to generate question-answer pairs and instructions.\n\n    Args:\n        python_file_path (str): Path to the Python file.\n        start_dir (str): Starting directory to search for Python files.\n        model_config_path (str): Path to the model configuration file.\n        questions_dict (Dict): Dictionary of questions to answer about the Python file.\n        use_llm (bool): If True, use a Large Language Model for generating JSON answers.\n        output_dir (str): Directory to save the output files.\n        model_config (Dict): Configuration dictionary for the LLM.\n        single_process_mode (bool): Use a single process if True. Defaults to False.\n        detailed_analysis (bool): Perform detailed analysis if True. Defaults to False.\n    \"\"\"\n    logging.info(f\"Processing file: {python_file_path}\")\n\n    # need to define relative path as the having at least one parent directory\n    # e.g relative patch should be equal to <last start_directory directory> / file name without path from python_file_path\n    # Get the last directory in start_dir\n    parent_dir = os.path.dirname(start_dir)\n    relative_path = os.path.relpath(python_file_path, parent_dir)\n    relative_path = Path(relative_path)\n    base_name = \".\".join(relative_path.parts)\n\n    if not single_process_mode and use_llm:\n        model_config = get_model(model_config_path)\n\n    file_details = get_python_file_details(python_file_path)\n    if not file_details:\n        logging.error(f\"Failed to get file details for {python_file_path}\")\n        return\n\n    instruct_data = get_python_datasets(\n        python_file_path,\n        file_details,\n        base_name,\n        questions_dict,\n        model_config,\n        detailed_analysis,\n    )\n\n    if instruct_data:\n        save_python_data(\n            file_details, instruct_data, base_name, relative_path, output_dir\n        )\n    else:\n        logging.error(f\"Failed to get instruct data for {python_file_path}\")\n\n\ndef py2dataset(\n    start_dir: str = \"\",\n    output_dir: str = \"\",\n    questions_pathname: str = \"\",\n    model_config_pathname: str = \"\",\n    use_llm: bool = False,\n    quiet: bool = False,\n    single_process: bool = False,\n    detailed: bool = False,\n    html: bool = False,\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start_dir (str): Starting directory for Python files. Defaults to current directory.\n        output_dir (str): Directory to save the output files.\n        questions_pathname (str): Path and filename of the questions file.\n        model_config_pathname (str): Path and filename of the model configuration file.\n        use_llm (bool): If True, use a Large Language Model for generating answers. Defaults to False.\n        quiet_mode (bool): Reduce logging output if True. Defaults to False.\n        single_process (bool): Use a single process for file processing if use_llm. Defaults to False.\n        detailed (bool): Include detailed analysis if True. Defaults to False.\n        html (bool): Generate HTML outputs if True. Defaults to False.\n    Returns:\n        Dict[str, List[Dict]]: Dictionary of generated datasets.\n    \"\"\"\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n\n    sys.setrecursionlimit(3000)  # Set recursion limit higher for AST parsing\n\n    start_dir = get_start_dir(start_dir)\n    output_dir = get_output_dir(output_dir)\n    questions_dict = get_questions(questions_pathname)\n\n    # Load model configuration if LLM is used and single process mode is enabled\n    model_config = (\n        get_model(model_config_pathname) if use_llm and single_process else None\n    )\n\n    if not use_llm:\n        single_process = True\n\n    # Process each Python file in the directory\n    for python_file_path in Path(start_dir).rglob(\"[!_]*.py\"):\n        # if use_llm is false or single_process is true then process the file within the current process\n\n        if not use_llm and single_process:\n            process_single_python_file(\n                python_file_path,\n                start_dir,\n                model_config_pathname,\n                questions_dict,\n                use_llm,\n                output_dir,\n                model_config,\n                single_process,\n                detailed,\n            )\n        else:\n            # Spawn new process for each file to manage memory and performance\n            proc = Process(\n                target=process_single_python_file,\n                args=(\n                    python_file_path,\n                    start_dir,\n                    model_config_pathname,\n                    questions_dict,\n                    use_llm,\n                    output_dir,\n                    None,\n                    single_process,\n                    detailed,\n                ),\n            )\n            proc.start()\n            proc.join()\n\n    # Combine all the individual datasets into a single dictionary\n    return combine_json_files(output_dir, html)\n\n\ndef clone_github_repo(url: str) -> str:\n    \"\"\"\n    Clone repository or pull the latest changes and return local repository path.\n    Args:\n        url (str): The url of the github repository.\n    Returns:\n        str: The path to the cloned repository.\n    \"\"\"\n    # Check valid Git repository\n    try:\n        command = f\"git ls-remote {shlex.quote(url)}\"\n        subprocess.run(\n            command,\n            shell=True,\n            check=True,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n        )\n    except subprocess.CalledProcessError:\n        print(f\"Invalid or inaccessible repository: {url}\")\n        return \"\"\n\n    # Proceed with cloning or fetching\n    repo_name = url.split(\"/\")[-1]\n    githubrepos_dir = os.path.join(os.getcwd(), \"githubrepos\")\n    os.makedirs(githubrepos_dir, exist_ok=True)\n    path = os.path.join(githubrepos_dir, repo_name)\n    if not os.path.exists(path):\n        git.Repo.clone_from(url, path)\n    else:\n        repo = git.Repo(path)\n        with repo.git.custom_environment(\n            GIT_SSH_COMMAND=\"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no\"\n        ):\n            repo.git.fetch()\n            default_branch = repo.head.reference.tracking_branch().remote_head\n            repo.git.reset(\"--hard\", default_branch)\n    return path\n\n\ndef main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Optional command-line arguments:\n    --start (str, optional): Starting directory for Python files or GitHub repository Python files. Defaults to current working directory.\n    --output_dir (str, optional): Directory to write the output files. Defaults to ./dataset/.\n    --questions_pathname (str, optional): Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\n    --model_config_pathname (str, optional): Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\n    --use_llm (bool, optional): Use llm for generating JSON answers. Defaults to False.\n    --quiet (bool, optional): Limit logging output. Defaults to False.\n    --single_process (bool, optional): If True, only a single process will be used to process Python files. Defaults to False.\n    --detailed (bool, optional): Include detailed analysis if True. Defaults to False.\n    --html (bool, optional): Generate HTML output if True. Defaults to False.\n    --I (str, optional): Interactive mode. Defaults to False.\n    \"\"\"\n\n    class PathArgument(argparse.Action):\n        def __call__(self, parser, namespace, values, option_string=None):\n            cleaned_value = values.strip('\"').strip(\"'\")\n            setattr(namespace, self.dest, cleaned_value)\n\n    def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n        \"\"\"\n        Get a boolean value from user input or retain the current value if the input is invalid.\n        \"\"\"\n        return (\n            True\n            if input_str.lower() in [\"t\", \"true\"]\n            else False\n            if input_str.lower() in [\"f\", \"false\"]\n            else current_value\n        )\n\n    # parse each command line entry\n    parser = argparse.ArgumentParser(\n        description=\"Process Python files to generate datasets.\"\n    )\n    parser.add_argument(\n        \"--start\",\n        default=\".\",\n        action=PathArgument,\n        help=\"Starting directory for Python files. Defaults to current working directory.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        default=\"./dataset/\",\n        action=PathArgument,\n        help=\"Directory to write the output files. Defaults to ./dataset/.\",\n    )\n    parser.add_argument(\n        \"--questions_pathname\",\n        default=\"./py2dataset_questions.json\",\n        action=PathArgument,\n        help=\"Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\",\n    )\n    parser.add_argument(\n        \"--model_config_pathname\",\n        default=\"./py2dataset_model_config.yaml\",\n        action=PathArgument,\n        help=\"Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\",\n    )\n    parser.add_argument(\n        \"--use_llm\",\n        action=\"store_true\",\n        help=\"Use LLM for generating JSON answers. Defaults to False.\",\n    )\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Limit logging output.\")\n    parser.add_argument(\n        \"--single_process\",\n        action=\"store_true\",\n        help=\"Use a single process for processing Python files. Defaults to False.\",\n    )\n    parser.add_argument(\n        \"--detailed\", action=\"store_true\", help=\"Include detailed analysis if True.\"\n    )\n    parser.add_argument(\n        \"--html\", action=\"store_true\", help=\"Generate HTML output if True.\"\n    )\n    parser.add_argument(\n        \"--I\",\n        \"--interactive\",\n        action=\"store_true\",\n        dest=\"interactive\",\n        help=\"Interactive mode.\",\n    )\n    args = parser.parse_args()\n\n    # Interactive mode adjustments\n    if args.interactive:\n        print(\"Input new value or press enter to keep current value.\")\n        pathname_params = [\n            \"start\",\n            \"output_dir\",\n            \"questions_pathname\",\n            \"model_config_pathname\",\n        ]\n        bool_params = [\"use_llm\", \"quiet\", \"single_process\", \"detailed\", \"html\"]\n        for param in pathname_params:\n            setattr(\n                args,\n                param,\n                input(f\"{param} [{getattr(args, param)}]: \") or getattr(args, param),\n            )\n        for param in bool_params:\n            setattr(\n                args,\n                param,\n                get_bool_from_input(\n                    input(f\"{param} [{getattr(args, param)}] (t or f): \"),\n                    getattr(args, param),\n                ),\n            )\n\n    # Validate the start directory\n    if not (os.path.isdir(args.start) or args.start.startswith(\"https://github.com/\")):\n        print(\n            f\"Invalid start directory '{args.start}'. Using current working directory.\"\n        )\n        args.start = os.getcwd()\n\n    # if the start directory is a github repository, clone it and change the start directory to the local repository\n    if args.start.startswith(\"https://github.com/\"):\n        args.start = clone_github_repo(args.start)\n\n    # Call py2dataset with args\n    py2dataset(\n        start_dir=args.start,\n        output_dir=args.output_dir,\n        questions_pathname=args.questions_pathname,\n        model_config_pathname=args.model_config_pathname,\n        use_llm=args.use_llm,\n        quiet=args.quiet,\n        single_process=args.single_process,\n        detailed=args.detailed,\n        html=args.html,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n\n```",
        "output": "Functions defined:process_single_python_file, py2dataset, clone_github_repo, main, get_bool_from_input\n\nPurpose and Significance:\nThe `process_single_python_file` function is used to process a single Python file and generate the datasets by analyzing the code, the function accepts several parameters including the Python file path, starting directory, model configuration file pathname, questions dictionary, use of LLM, output directory, and other optional parameters. The `py2dataset` function is used to generate datasets for all the Python files within a specified directory or GitHub repository. It takes in various command-line arguments such as start directory, output directory, questions path, model configuration path, use of LLM, quiet mode, single process mode, and detailed analysis mode. The `clone_github_repo` function is used to clone the specified Git repository or fetch the latest changes if it already exists locally. The `main` function handles the command-line arguments and calls the `py2dataset` function with the derived parameters based on user input. "
    },
    {
        "instruction": "Classes defined in Python file: `py2dataset.py2dataset.py`?",
        "input": "```python\n\"\"\"\nFor each Python file within a given directory, this module is designed to generate, save, \nand return datasets that include responses to questions about the code. \nRequirements:\n[req00] The process_single_python_file function shall:\n    a. Accept parameters for Python file path, start directory, model config \n       pathname, questions dict, use of LLM, and output dir.\n    b. If 'use_llm' is True, use 'get_model' to instantiate LLM model config.\n    c. Use 'get_python_file_details' to retrieve Python file info.\n    d. Use 'get_python_datasets' to acquire instruct.json datasets.\n    e. Use 'save_python_data' to store file details and instruct.json data.\n[req01] The py2dataset function shall:\n    a. Accept parameters for start directory, output dir, questions path, model\n       config path, use of LLM, and quiet mode.\n    b. Adjust logging level based on 'quiet'.\n    c. Use current working dir if no valid start dir is provided.\n    d. Get output dir with 'get_output_dir'.\n    e. Retrieve questions dict with 'get_questions'.\n    f. Search for Python files using 'rglob', excluding those starting with \"_\".\n    g. For each Python file, spawn a child process with 'process_single_python_file'\n       to get file details and instruct.json data, if single_process is False.\n    h. Combine instruct.json files with 'combine_json_files'.\n    i. Return datasets.\n[req02] The main function shall:\n    a. Accept and process command-line args.\n    b. Determine py2dataset parameters based on processed arguments.\n    c. Call py2dataset with derived parameters.\n\"\"\"\nimport sys\nimport argparse\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom multiprocessing import Process\nimport subprocess\nimport os\nimport git\nimport shlex\n\nfrom get_python_file_details import get_python_file_details\nfrom get_python_datasets import get_python_datasets\nfrom get_params import (\n    get_questions,\n    get_model,\n    get_output_dir,\n    get_start_dir,\n)\nfrom save_output import combine_json_files, save_python_data\n\n\ndef process_single_python_file(\n    python_file_path: str,\n    start_dir: str,\n    model_config_path: str,\n    questions_dict: Dict,\n    use_llm: bool,\n    output_dir: str,\n    model_config: Dict = None,\n    single_process_mode: bool = False,\n    detailed_analysis: bool = False,\n) -> None:\n    \"\"\"\n    Processes a single Python file to generate question-answer pairs and instructions.\n\n    Args:\n        python_file_path (str): Path to the Python file.\n        start_dir (str): Starting directory to search for Python files.\n        model_config_path (str): Path to the model configuration file.\n        questions_dict (Dict): Dictionary of questions to answer about the Python file.\n        use_llm (bool): If True, use a Large Language Model for generating JSON answers.\n        output_dir (str): Directory to save the output files.\n        model_config (Dict): Configuration dictionary for the LLM.\n        single_process_mode (bool): Use a single process if True. Defaults to False.\n        detailed_analysis (bool): Perform detailed analysis if True. Defaults to False.\n    \"\"\"\n    logging.info(f\"Processing file: {python_file_path}\")\n\n    # need to define relative path as the having at least one parent directory\n    # e.g relative patch should be equal to <last start_directory directory> / file name without path from python_file_path\n    # Get the last directory in start_dir\n    parent_dir = os.path.dirname(start_dir)\n    relative_path = os.path.relpath(python_file_path, parent_dir)\n    relative_path = Path(relative_path)\n    base_name = \".\".join(relative_path.parts)\n\n    if not single_process_mode and use_llm:\n        model_config = get_model(model_config_path)\n\n    file_details = get_python_file_details(python_file_path)\n    if not file_details:\n        logging.error(f\"Failed to get file details for {python_file_path}\")\n        return\n\n    instruct_data = get_python_datasets(\n        python_file_path,\n        file_details,\n        base_name,\n        questions_dict,\n        model_config,\n        detailed_analysis,\n    )\n\n    if instruct_data:\n        save_python_data(\n            file_details, instruct_data, base_name, relative_path, output_dir\n        )\n    else:\n        logging.error(f\"Failed to get instruct data for {python_file_path}\")\n\n\ndef py2dataset(\n    start_dir: str = \"\",\n    output_dir: str = \"\",\n    questions_pathname: str = \"\",\n    model_config_pathname: str = \"\",\n    use_llm: bool = False,\n    quiet: bool = False,\n    single_process: bool = False,\n    detailed: bool = False,\n    html: bool = False,\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start_dir (str): Starting directory for Python files. Defaults to current directory.\n        output_dir (str): Directory to save the output files.\n        questions_pathname (str): Path and filename of the questions file.\n        model_config_pathname (str): Path and filename of the model configuration file.\n        use_llm (bool): If True, use a Large Language Model for generating answers. Defaults to False.\n        quiet_mode (bool): Reduce logging output if True. Defaults to False.\n        single_process (bool): Use a single process for file processing if use_llm. Defaults to False.\n        detailed (bool): Include detailed analysis if True. Defaults to False.\n        html (bool): Generate HTML outputs if True. Defaults to False.\n    Returns:\n        Dict[str, List[Dict]]: Dictionary of generated datasets.\n    \"\"\"\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n\n    sys.setrecursionlimit(3000)  # Set recursion limit higher for AST parsing\n\n    start_dir = get_start_dir(start_dir)\n    output_dir = get_output_dir(output_dir)\n    questions_dict = get_questions(questions_pathname)\n\n    # Load model configuration if LLM is used and single process mode is enabled\n    model_config = (\n        get_model(model_config_pathname) if use_llm and single_process else None\n    )\n\n    if not use_llm:\n        single_process = True\n\n    # Process each Python file in the directory\n    for python_file_path in Path(start_dir).rglob(\"[!_]*.py\"):\n        # if use_llm is false or single_process is true then process the file within the current process\n\n        if not use_llm and single_process:\n            process_single_python_file(\n                python_file_path,\n                start_dir,\n                model_config_pathname,\n                questions_dict,\n                use_llm,\n                output_dir,\n                model_config,\n                single_process,\n                detailed,\n            )\n        else:\n            # Spawn new process for each file to manage memory and performance\n            proc = Process(\n                target=process_single_python_file,\n                args=(\n                    python_file_path,\n                    start_dir,\n                    model_config_pathname,\n                    questions_dict,\n                    use_llm,\n                    output_dir,\n                    None,\n                    single_process,\n                    detailed,\n                ),\n            )\n            proc.start()\n            proc.join()\n\n    # Combine all the individual datasets into a single dictionary\n    return combine_json_files(output_dir, html)\n\n\ndef clone_github_repo(url: str) -> str:\n    \"\"\"\n    Clone repository or pull the latest changes and return local repository path.\n    Args:\n        url (str): The url of the github repository.\n    Returns:\n        str: The path to the cloned repository.\n    \"\"\"\n    # Check valid Git repository\n    try:\n        command = f\"git ls-remote {shlex.quote(url)}\"\n        subprocess.run(\n            command,\n            shell=True,\n            check=True,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n        )\n    except subprocess.CalledProcessError:\n        print(f\"Invalid or inaccessible repository: {url}\")\n        return \"\"\n\n    # Proceed with cloning or fetching\n    repo_name = url.split(\"/\")[-1]\n    githubrepos_dir = os.path.join(os.getcwd(), \"githubrepos\")\n    os.makedirs(githubrepos_dir, exist_ok=True)\n    path = os.path.join(githubrepos_dir, repo_name)\n    if not os.path.exists(path):\n        git.Repo.clone_from(url, path)\n    else:\n        repo = git.Repo(path)\n        with repo.git.custom_environment(\n            GIT_SSH_COMMAND=\"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no\"\n        ):\n            repo.git.fetch()\n            default_branch = repo.head.reference.tracking_branch().remote_head\n            repo.git.reset(\"--hard\", default_branch)\n    return path\n\n\ndef main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Optional command-line arguments:\n    --start (str, optional): Starting directory for Python files or GitHub repository Python files. Defaults to current working directory.\n    --output_dir (str, optional): Directory to write the output files. Defaults to ./dataset/.\n    --questions_pathname (str, optional): Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\n    --model_config_pathname (str, optional): Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\n    --use_llm (bool, optional): Use llm for generating JSON answers. Defaults to False.\n    --quiet (bool, optional): Limit logging output. Defaults to False.\n    --single_process (bool, optional): If True, only a single process will be used to process Python files. Defaults to False.\n    --detailed (bool, optional): Include detailed analysis if True. Defaults to False.\n    --html (bool, optional): Generate HTML output if True. Defaults to False.\n    --I (str, optional): Interactive mode. Defaults to False.\n    \"\"\"\n\n    class PathArgument(argparse.Action):\n        def __call__(self, parser, namespace, values, option_string=None):\n            cleaned_value = values.strip('\"').strip(\"'\")\n            setattr(namespace, self.dest, cleaned_value)\n\n    def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n        \"\"\"\n        Get a boolean value from user input or retain the current value if the input is invalid.\n        \"\"\"\n        return (\n            True\n            if input_str.lower() in [\"t\", \"true\"]\n            else False\n            if input_str.lower() in [\"f\", \"false\"]\n            else current_value\n        )\n\n    # parse each command line entry\n    parser = argparse.ArgumentParser(\n        description=\"Process Python files to generate datasets.\"\n    )\n    parser.add_argument(\n        \"--start\",\n        default=\".\",\n        action=PathArgument,\n        help=\"Starting directory for Python files. Defaults to current working directory.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        default=\"./dataset/\",\n        action=PathArgument,\n        help=\"Directory to write the output files. Defaults to ./dataset/.\",\n    )\n    parser.add_argument(\n        \"--questions_pathname\",\n        default=\"./py2dataset_questions.json\",\n        action=PathArgument,\n        help=\"Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\",\n    )\n    parser.add_argument(\n        \"--model_config_pathname\",\n        default=\"./py2dataset_model_config.yaml\",\n        action=PathArgument,\n        help=\"Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\",\n    )\n    parser.add_argument(\n        \"--use_llm\",\n        action=\"store_true\",\n        help=\"Use LLM for generating JSON answers. Defaults to False.\",\n    )\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Limit logging output.\")\n    parser.add_argument(\n        \"--single_process\",\n        action=\"store_true\",\n        help=\"Use a single process for processing Python files. Defaults to False.\",\n    )\n    parser.add_argument(\n        \"--detailed\", action=\"store_true\", help=\"Include detailed analysis if True.\"\n    )\n    parser.add_argument(\n        \"--html\", action=\"store_true\", help=\"Generate HTML output if True.\"\n    )\n    parser.add_argument(\n        \"--I\",\n        \"--interactive\",\n        action=\"store_true\",\n        dest=\"interactive\",\n        help=\"Interactive mode.\",\n    )\n    args = parser.parse_args()\n\n    # Interactive mode adjustments\n    if args.interactive:\n        print(\"Input new value or press enter to keep current value.\")\n        pathname_params = [\n            \"start\",\n            \"output_dir\",\n            \"questions_pathname\",\n            \"model_config_pathname\",\n        ]\n        bool_params = [\"use_llm\", \"quiet\", \"single_process\", \"detailed\", \"html\"]\n        for param in pathname_params:\n            setattr(\n                args,\n                param,\n                input(f\"{param} [{getattr(args, param)}]: \") or getattr(args, param),\n            )\n        for param in bool_params:\n            setattr(\n                args,\n                param,\n                get_bool_from_input(\n                    input(f\"{param} [{getattr(args, param)}] (t or f): \"),\n                    getattr(args, param),\n                ),\n            )\n\n    # Validate the start directory\n    if not (os.path.isdir(args.start) or args.start.startswith(\"https://github.com/\")):\n        print(\n            f\"Invalid start directory '{args.start}'. Using current working directory.\"\n        )\n        args.start = os.getcwd()\n\n    # if the start directory is a github repository, clone it and change the start directory to the local repository\n    if args.start.startswith(\"https://github.com/\"):\n        args.start = clone_github_repo(args.start)\n\n    # Call py2dataset with args\n    py2dataset(\n        start_dir=args.start,\n        output_dir=args.output_dir,\n        questions_pathname=args.questions_pathname,\n        model_config_pathname=args.model_config_pathname,\n        use_llm=args.use_llm,\n        quiet=args.quiet,\n        single_process=args.single_process,\n        detailed=args.detailed,\n        html=args.html,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n\n```",
        "output": "Classes defined:PathArgument\n\nPurpose and Significance:\nThe `PathArgument` class is a custom action for handling pathnames as command-line arguments. It ensures that any paths provided as arguments are stripped of quotes to avoid errors when using them later in the code. "
    },
    {
        "instruction": "Inputs to `process_single_python_file` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def process_single_python_file(python_file_path: str, start_dir: str, model_config_path: str, questions_dict: Dict, use_llm: bool, output_dir: str, model_config: Dict=None, single_process_mode: bool=False, detailed_analysis: bool=False) -> None:\n    \"\"\"\n    Processes a single Python file to generate question-answer pairs and instructions.\n\n    Args:\n        python_file_path (str): Path to the Python file.\n        start_dir (str): Starting directory to search for Python files.\n        model_config_path (str): Path to the model configuration file.\n        questions_dict (Dict): Dictionary of questions to answer about the Python file.\n        use_llm (bool): If True, use a Large Language Model for generating JSON answers.\n        output_dir (str): Directory to save the output files.\n        model_config (Dict): Configuration dictionary for the LLM.\n        single_process_mode (bool): Use a single process if True. Defaults to False.\n        detailed_analysis (bool): Perform detailed analysis if True. Defaults to False.\n    \"\"\"\n    logging.info(f'Processing file: {python_file_path}')\n    parent_dir = os.path.dirname(start_dir)\n    relative_path = os.path.relpath(python_file_path, parent_dir)\n    relative_path = Path(relative_path)\n    base_name = '.'.join(relative_path.parts)\n    if not single_process_mode and use_llm:\n        model_config = get_model(model_config_path)\n    file_details = get_python_file_details(python_file_path)\n    if not file_details:\n        logging.error(f'Failed to get file details for {python_file_path}')\n        return\n    instruct_data = get_python_datasets(python_file_path, file_details, base_name, questions_dict, model_config, detailed_analysis)\n    if instruct_data:\n        save_python_data(file_details, instruct_data, base_name, relative_path, output_dir)\n    else:\n        logging.error(f'Failed to get instruct data for {python_file_path}')",
        "output": "Inputs to `process_single_python_file`:python_file_path, start_dir, model_config_path, questions_dict, use_llm, output_dir, model_config, single_process_mode, detailed_analysis\n\nPurpose and Significance:\nThe function 'process_single_python_file' accepts parameters for Python file path, start directory, model config pathname, questions dictionary, the option of whether or not to use an LLM and where the generated output will be stored. These objects are important to run because without them there wouldn't be anywhere for the datasets generated within that specific function call to go. The 'model_config' parameter is only used if 'use_llm' is True, as it is a dictionary of configuration settings for the LLM model. If not using the LLM, single process mode will run and ignore the model_config variable, as we do not need to create a separate model config dictionary.\r\n\n'save_python_data' function: This function takes in the file details, instruct.json data, and the base name of the Python file being analyzed, then saves it to the specified output directory with a unique filename for each file analyzed. \r\n\n'py2dataset' function: The main function accepts command-line arguments to determine py2dataset parameters based on processed arguments. It will check if we should be running in quiet mode and adjusts the logging level accordingly. We also define a couple helper functions, get_output_dir and get_start_dir which will return the output directory and starting directory respectively.\r\n\nThe function then uses 'rglob' to search for all Python files within the specified start directory (or current working directory if none is provided), excluding those that begin with an underscore. It will then either run each file in a separate process or not depending on whether or not we are using single_process mode and whether or not we are using the LLM.\r\n\nIf we are not using the LLM, it will simply call 'process_single_python_file' for each Python file found. If we are using the LLM, it will spawn a new process for each file to manage memory and performance.\r\n\nFinally, the function calls 'combine_json_files' to combine all of the individual datasets into one dictionary before returning them. "
    },
    {
        "instruction": "Inputs to `py2dataset` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def py2dataset(start_dir: str='', output_dir: str='', questions_pathname: str='', model_config_pathname: str='', use_llm: bool=False, quiet: bool=False, single_process: bool=False, detailed: bool=False, html: bool=False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start_dir (str): Starting directory for Python files. Defaults to current directory.\n        output_dir (str): Directory to save the output files.\n        questions_pathname (str): Path and filename of the questions file.\n        model_config_pathname (str): Path and filename of the model configuration file.\n        use_llm (bool): If True, use a Large Language Model for generating answers. Defaults to False.\n        quiet_mode (bool): Reduce logging output if True. Defaults to False.\n        single_process (bool): Use a single process for file processing if use_llm. Defaults to False.\n        detailed (bool): Include detailed analysis if True. Defaults to False.\n        html (bool): Generate HTML outputs if True. Defaults to False.\n    Returns:\n        Dict[str, List[Dict]]: Dictionary of generated datasets.\n    \"\"\"\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n    sys.setrecursionlimit(3000)\n    start_dir = get_start_dir(start_dir)\n    output_dir = get_output_dir(output_dir)\n    questions_dict = get_questions(questions_pathname)\n    model_config = get_model(model_config_pathname) if use_llm and single_process else None\n    if not use_llm:\n        single_process = True\n    for python_file_path in Path(start_dir).rglob('[!_]*.py'):\n        if not use_llm and single_process:\n            process_single_python_file(python_file_path, start_dir, model_config_pathname, questions_dict, use_llm, output_dir, model_config, single_process, detailed)\n        else:\n            proc = Process(target=process_single_python_file, args=(python_file_path, start_dir, model_config_pathname, questions_dict, use_llm, output_dir, None, single_process, detailed))\n            proc.start()\n            proc.join()\n    return combine_json_files(output_dir, html)",
        "output": "Inputs to `py2dataset`:start_dir, output_dir, questions_pathname, model_config_pathname, use_llm, quiet, single_process, detailed, html\n\nPurpose and Significance:\nThe `py2dataset` function is used for processing Python files within a specified directory to generate datasets by getting file details and answering the given questions using a Linguistic Language Model or a parser-based approach. The start directory parameter defines where to look for Python files, while the output_dir parameter specifies where to save the generated datasets. The `questions_pathname` parameter is used to specify the path of the JSON file containing the list of questions that are to be answered by the LLM or parser, and `model_config_pathname` defines the configuration file for the LLM. If `use_llm` is True, it uses the provided model (as defined in `py2dataset_model_config.yaml` file) otherwise a custom implementation will be used. `single_process` determines whether to use only one process or multiple processes to process Python files, and `detailed` is used to enable detailed analysis of each Python file.\r\n\nThe main function parses the command line arguments to obtain the appropriate values for the py2dataset parameters based on the input arguments provided. "
    },
    {
        "instruction": "Inputs to `clone_github_repo` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def clone_github_repo(url: str) -> str:\n    \"\"\"\n    Clone repository or pull the latest changes and return local repository path.\n    Args:\n        url (str): The url of the github repository.\n    Returns:\n        str: The path to the cloned repository.\n    \"\"\"\n    try:\n        command = f'git ls-remote {shlex.quote(url)}'\n        subprocess.run(command, shell=True, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    except subprocess.CalledProcessError:\n        print(f'Invalid or inaccessible repository: {url}')\n        return ''\n    repo_name = url.split('/')[-1]\n    githubrepos_dir = os.path.join(os.getcwd(), 'githubrepos')\n    os.makedirs(githubrepos_dir, exist_ok=True)\n    path = os.path.join(githubrepos_dir, repo_name)\n    if not os.path.exists(path):\n        git.Repo.clone_from(url, path)\n    else:\n        repo = git.Repo(path)\n        with repo.git.custom_environment(GIT_SSH_COMMAND='ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no'):\n            repo.git.fetch()\n            default_branch = repo.head.reference.tracking_branch().remote_head\n            repo.git.reset('--hard', default_branch)\n    return path",
        "output": "Inputs to `clone_github_repo`:url\n\nPurpose and Significance:\nThe `clone_github_repo()` function takes in a url for a GitHub repository, validates that it is valid using git's `git ls-remote` command and clones or fetches the latest changes based on whether the repository has already been cloned. It returns the path to the local repository if successful, otherwise an empty string. "
    },
    {
        "instruction": "Inputs to `get_bool_from_input` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n    \"\"\"\n        Get a boolean value from user input or retain the current value if the input is invalid.\n        \"\"\"\n    return True if input_str.lower() in ['t', 'true'] else False if input_str.lower() in ['f', 'false'] else current_value",
        "output": "Inputs to `get_bool_from_input`:input_str, current_value\n\nPurpose and Significance:\nThe `get_bool_from_input` function is used in the `main` function for interactive mode to validate the input provided by the user and return a boolean value based on their input. The `input_str` parameter represents the user's input as a string, while `current_value` is the current value of the corresponding argument before being updated with the user's input if it is valid. If the user provides an invalid input (i.e., neither \"true\" nor \"false\"), then the function returns the original `current_value`. "
    },
    {
        "instruction": "Docstring of `process_single_python_file` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def process_single_python_file(python_file_path: str, start_dir: str, model_config_path: str, questions_dict: Dict, use_llm: bool, output_dir: str, model_config: Dict=None, single_process_mode: bool=False, detailed_analysis: bool=False) -> None:\n    \"\"\"\n    Processes a single Python file to generate question-answer pairs and instructions.\n\n    Args:\n        python_file_path (str): Path to the Python file.\n        start_dir (str): Starting directory to search for Python files.\n        model_config_path (str): Path to the model configuration file.\n        questions_dict (Dict): Dictionary of questions to answer about the Python file.\n        use_llm (bool): If True, use a Large Language Model for generating JSON answers.\n        output_dir (str): Directory to save the output files.\n        model_config (Dict): Configuration dictionary for the LLM.\n        single_process_mode (bool): Use a single process if True. Defaults to False.\n        detailed_analysis (bool): Perform detailed analysis if True. Defaults to False.\n    \"\"\"\n    logging.info(f'Processing file: {python_file_path}')\n    parent_dir = os.path.dirname(start_dir)\n    relative_path = os.path.relpath(python_file_path, parent_dir)\n    relative_path = Path(relative_path)\n    base_name = '.'.join(relative_path.parts)\n    if not single_process_mode and use_llm:\n        model_config = get_model(model_config_path)\n    file_details = get_python_file_details(python_file_path)\n    if not file_details:\n        logging.error(f'Failed to get file details for {python_file_path}')\n        return\n    instruct_data = get_python_datasets(python_file_path, file_details, base_name, questions_dict, model_config, detailed_analysis)\n    if instruct_data:\n        save_python_data(file_details, instruct_data, base_name, relative_path, output_dir)\n    else:\n        logging.error(f'Failed to get instruct data for {python_file_path}')",
        "output": "Processes a single Python file to generate question-answer pairs and instructions.\n\n    Args:\n        python_file_path (str): Path to the Python file.\n        start_dir (str): Starting directory to search for Python files.\n        model_config_path (str): Path to the model configuration file.\n        questions_dict (Dict): Dictionary of questions to answer about the Python file.\n        use_llm (bool): If True, use a Large Language Model for generating JSON answers.\n        output_dir (str): Directory to save the output files.\n        model_config (Dict): Configuration dictionary for the LLM.\n        single_process_mode (bool): Use a single process if True. Defaults to False.\n        detailed_analysis (bool): Perform detailed analysis if True. Defaults to False."
    },
    {
        "instruction": "Docstring of `py2dataset` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def py2dataset(start_dir: str='', output_dir: str='', questions_pathname: str='', model_config_pathname: str='', use_llm: bool=False, quiet: bool=False, single_process: bool=False, detailed: bool=False, html: bool=False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start_dir (str): Starting directory for Python files. Defaults to current directory.\n        output_dir (str): Directory to save the output files.\n        questions_pathname (str): Path and filename of the questions file.\n        model_config_pathname (str): Path and filename of the model configuration file.\n        use_llm (bool): If True, use a Large Language Model for generating answers. Defaults to False.\n        quiet_mode (bool): Reduce logging output if True. Defaults to False.\n        single_process (bool): Use a single process for file processing if use_llm. Defaults to False.\n        detailed (bool): Include detailed analysis if True. Defaults to False.\n        html (bool): Generate HTML outputs if True. Defaults to False.\n    Returns:\n        Dict[str, List[Dict]]: Dictionary of generated datasets.\n    \"\"\"\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n    sys.setrecursionlimit(3000)\n    start_dir = get_start_dir(start_dir)\n    output_dir = get_output_dir(output_dir)\n    questions_dict = get_questions(questions_pathname)\n    model_config = get_model(model_config_pathname) if use_llm and single_process else None\n    if not use_llm:\n        single_process = True\n    for python_file_path in Path(start_dir).rglob('[!_]*.py'):\n        if not use_llm and single_process:\n            process_single_python_file(python_file_path, start_dir, model_config_pathname, questions_dict, use_llm, output_dir, model_config, single_process, detailed)\n        else:\n            proc = Process(target=process_single_python_file, args=(python_file_path, start_dir, model_config_pathname, questions_dict, use_llm, output_dir, None, single_process, detailed))\n            proc.start()\n            proc.join()\n    return combine_json_files(output_dir, html)",
        "output": "Generates datasets by processing Python files within a specified directory.\n    Args:\n        start_dir (str): Starting directory for Python files. Defaults to current directory.\n        output_dir (str): Directory to save the output files.\n        questions_pathname (str): Path and filename of the questions file.\n        model_config_pathname (str): Path and filename of the model configuration file.\n        use_llm (bool): If True, use a Large Language Model for generating answers. Defaults to False.\n        quiet_mode (bool): Reduce logging output if True. Defaults to False.\n        single_process (bool): Use a single process for file processing if use_llm. Defaults to False.\n        detailed (bool): Include detailed analysis if True. Defaults to False.\n        html (bool): Generate HTML outputs if True. Defaults to False.\n    Returns:\n        Dict[str, List[Dict]]: Dictionary of generated datasets."
    },
    {
        "instruction": "Docstring of `clone_github_repo` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def clone_github_repo(url: str) -> str:\n    \"\"\"\n    Clone repository or pull the latest changes and return local repository path.\n    Args:\n        url (str): The url of the github repository.\n    Returns:\n        str: The path to the cloned repository.\n    \"\"\"\n    try:\n        command = f'git ls-remote {shlex.quote(url)}'\n        subprocess.run(command, shell=True, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    except subprocess.CalledProcessError:\n        print(f'Invalid or inaccessible repository: {url}')\n        return ''\n    repo_name = url.split('/')[-1]\n    githubrepos_dir = os.path.join(os.getcwd(), 'githubrepos')\n    os.makedirs(githubrepos_dir, exist_ok=True)\n    path = os.path.join(githubrepos_dir, repo_name)\n    if not os.path.exists(path):\n        git.Repo.clone_from(url, path)\n    else:\n        repo = git.Repo(path)\n        with repo.git.custom_environment(GIT_SSH_COMMAND='ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no'):\n            repo.git.fetch()\n            default_branch = repo.head.reference.tracking_branch().remote_head\n            repo.git.reset('--hard', default_branch)\n    return path",
        "output": "Clone repository or pull the latest changes and return local repository path.\n    Args:\n        url (str): The url of the github repository.\n    Returns:\n        str: The path to the cloned repository."
    },
    {
        "instruction": "Docstring of `main` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Optional command-line arguments:\n    --start (str, optional): Starting directory for Python files or GitHub repository Python files. Defaults to current working directory.\n    --output_dir (str, optional): Directory to write the output files. Defaults to ./dataset/.\n    --questions_pathname (str, optional): Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\n    --model_config_pathname (str, optional): Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\n    --use_llm (bool, optional): Use llm for generating JSON answers. Defaults to False.\n    --quiet (bool, optional): Limit logging output. Defaults to False.\n    --single_process (bool, optional): If True, only a single process will be used to process Python files. Defaults to False.\n    --detailed (bool, optional): Include detailed analysis if True. Defaults to False.\n    --html (bool, optional): Generate HTML output if True. Defaults to False.\n    --I (str, optional): Interactive mode. Defaults to False.\n    \"\"\"\n\n    class PathArgument(argparse.Action):\n\n        def __call__(self, parser, namespace, values, option_string=None):\n            cleaned_value = values.strip('\"').strip(\"'\")\n            setattr(namespace, self.dest, cleaned_value)\n\n    def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n        \"\"\"\n        Get a boolean value from user input or retain the current value if the input is invalid.\n        \"\"\"\n        return True if input_str.lower() in ['t', 'true'] else False if input_str.lower() in ['f', 'false'] else current_value\n    parser = argparse.ArgumentParser(description='Process Python files to generate datasets.')\n    parser.add_argument('--start', default='.', action=PathArgument, help='Starting directory for Python files. Defaults to current working directory.')\n    parser.add_argument('--output_dir', default='./dataset/', action=PathArgument, help='Directory to write the output files. Defaults to ./dataset/.')\n    parser.add_argument('--questions_pathname', default='./py2dataset_questions.json', action=PathArgument, help='Path and filename of the questions file. Defaults to ./py2dataset_questions.json.')\n    parser.add_argument('--model_config_pathname', default='./py2dataset_model_config.yaml', action=PathArgument, help='Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.')\n    parser.add_argument('--use_llm', action='store_true', help='Use LLM for generating JSON answers. Defaults to False.')\n    parser.add_argument('--quiet', action='store_true', help='Limit logging output.')\n    parser.add_argument('--single_process', action='store_true', help='Use a single process for processing Python files. Defaults to False.')\n    parser.add_argument('--detailed', action='store_true', help='Include detailed analysis if True.')\n    parser.add_argument('--html', action='store_true', help='Generate HTML output if True.')\n    parser.add_argument('--I', '--interactive', action='store_true', dest='interactive', help='Interactive mode.')\n    args = parser.parse_args()\n    if args.interactive:\n        print('Input new value or press enter to keep current value.')\n        pathname_params = ['start', 'output_dir', 'questions_pathname', 'model_config_pathname']\n        bool_params = ['use_llm', 'quiet', 'single_process', 'detailed', 'html']\n        for param in pathname_params:\n            setattr(args, param, input(f'{param} [{getattr(args, param)}]: ') or getattr(args, param))\n        for param in bool_params:\n            setattr(args, param, get_bool_from_input(input(f'{param} [{getattr(args, param)}] (t or f): '), getattr(args, param)))\n    if not (os.path.isdir(args.start) or args.start.startswith('https://github.com/')):\n        print(f\"Invalid start directory '{args.start}'. Using current working directory.\")\n        args.start = os.getcwd()\n    if args.start.startswith('https://github.com/'):\n        args.start = clone_github_repo(args.start)\n    py2dataset(start_dir=args.start, output_dir=args.output_dir, questions_pathname=args.questions_pathname, model_config_pathname=args.model_config_pathname, use_llm=args.use_llm, quiet=args.quiet, single_process=args.single_process, detailed=args.detailed, html=args.html)",
        "output": "Command-line entry point for processing Python files and generating datasets.\n    Optional command-line arguments:\n    --start (str, optional): Starting directory for Python files or GitHub repository Python files. Defaults to current working directory.\n    --output_dir (str, optional): Directory to write the output files. Defaults to ./dataset/.\n    --questions_pathname (str, optional): Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\n    --model_config_pathname (str, optional): Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\n    --use_llm (bool, optional): Use llm for generating JSON answers. Defaults to False.\n    --quiet (bool, optional): Limit logging output. Defaults to False.\n    --single_process (bool, optional): If True, only a single process will be used to process Python files. Defaults to False.\n    --detailed (bool, optional): Include detailed analysis if True. Defaults to False.\n    --html (bool, optional): Generate HTML output if True. Defaults to False.\n    --I (str, optional): Interactive mode. Defaults to False."
    },
    {
        "instruction": "Docstring of `get_bool_from_input` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n    \"\"\"\n        Get a boolean value from user input or retain the current value if the input is invalid.\n        \"\"\"\n    return True if input_str.lower() in ['t', 'true'] else False if input_str.lower() in ['f', 'false'] else current_value",
        "output": "Get a boolean value from user input or retain the current value if the input is invalid."
    },
    {
        "instruction": "Calls made in `process_single_python_file` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def process_single_python_file(python_file_path: str, start_dir: str, model_config_path: str, questions_dict: Dict, use_llm: bool, output_dir: str, model_config: Dict=None, single_process_mode: bool=False, detailed_analysis: bool=False) -> None:\n    \"\"\"\n    Processes a single Python file to generate question-answer pairs and instructions.\n\n    Args:\n        python_file_path (str): Path to the Python file.\n        start_dir (str): Starting directory to search for Python files.\n        model_config_path (str): Path to the model configuration file.\n        questions_dict (Dict): Dictionary of questions to answer about the Python file.\n        use_llm (bool): If True, use a Large Language Model for generating JSON answers.\n        output_dir (str): Directory to save the output files.\n        model_config (Dict): Configuration dictionary for the LLM.\n        single_process_mode (bool): Use a single process if True. Defaults to False.\n        detailed_analysis (bool): Perform detailed analysis if True. Defaults to False.\n    \"\"\"\n    logging.info(f'Processing file: {python_file_path}')\n    parent_dir = os.path.dirname(start_dir)\n    relative_path = os.path.relpath(python_file_path, parent_dir)\n    relative_path = Path(relative_path)\n    base_name = '.'.join(relative_path.parts)\n    if not single_process_mode and use_llm:\n        model_config = get_model(model_config_path)\n    file_details = get_python_file_details(python_file_path)\n    if not file_details:\n        logging.error(f'Failed to get file details for {python_file_path}')\n        return\n    instruct_data = get_python_datasets(python_file_path, file_details, base_name, questions_dict, model_config, detailed_analysis)\n    if instruct_data:\n        save_python_data(file_details, instruct_data, base_name, relative_path, output_dir)\n    else:\n        logging.error(f'Failed to get instruct data for {python_file_path}')",
        "output": "Calls made in `process_single_python_file`:logging.info, os.path.dirname, os.path.relpath, Path, .'.join, get_model, get_python_file_details, logging.error, get_python_datasets, save_python_data\n\nPurpose and Significance:\nThe purpose of these objects within the code is to log information, retrieve parent directory path, relative path, join file paths and names, instantiate a LLM model if `use_llm` is True, process Python files, retrieve file details, get questions dictionary, save the output in JSON format, and combine multiple JSON files into one. "
    },
    {
        "instruction": "Calls made in `py2dataset` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def py2dataset(start_dir: str='', output_dir: str='', questions_pathname: str='', model_config_pathname: str='', use_llm: bool=False, quiet: bool=False, single_process: bool=False, detailed: bool=False, html: bool=False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start_dir (str): Starting directory for Python files. Defaults to current directory.\n        output_dir (str): Directory to save the output files.\n        questions_pathname (str): Path and filename of the questions file.\n        model_config_pathname (str): Path and filename of the model configuration file.\n        use_llm (bool): If True, use a Large Language Model for generating answers. Defaults to False.\n        quiet_mode (bool): Reduce logging output if True. Defaults to False.\n        single_process (bool): Use a single process for file processing if use_llm. Defaults to False.\n        detailed (bool): Include detailed analysis if True. Defaults to False.\n        html (bool): Generate HTML outputs if True. Defaults to False.\n    Returns:\n        Dict[str, List[Dict]]: Dictionary of generated datasets.\n    \"\"\"\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n    sys.setrecursionlimit(3000)\n    start_dir = get_start_dir(start_dir)\n    output_dir = get_output_dir(output_dir)\n    questions_dict = get_questions(questions_pathname)\n    model_config = get_model(model_config_pathname) if use_llm and single_process else None\n    if not use_llm:\n        single_process = True\n    for python_file_path in Path(start_dir).rglob('[!_]*.py'):\n        if not use_llm and single_process:\n            process_single_python_file(python_file_path, start_dir, model_config_pathname, questions_dict, use_llm, output_dir, model_config, single_process, detailed)\n        else:\n            proc = Process(target=process_single_python_file, args=(python_file_path, start_dir, model_config_pathname, questions_dict, use_llm, output_dir, None, single_process, detailed))\n            proc.start()\n            proc.join()\n    return combine_json_files(output_dir, html)",
        "output": "Calls made in `py2dataset`:logging.getLogger().setLevel, logging.getLogger, sys.setrecursionlimit, get_start_dir, get_output_dir, get_questions, get_model, Path(start_dir).rglob, Path, process_single_python_file, Process, proc.start, proc.join, combine_json_files\n\nPurpose and Significance:\nThe purpose of the code is to generate datasets for Python files within a specified directory or GitHub repository. The significance of these objects in `py2dataset` function are as follows:\r\n- `logging.getLogger().setLevel`: This sets the logging level based on whether quiet mode is enabled or not.\r\n- `logging.getLogger`: This gets the logger object for logging purposes.\r\n- `sys.setrecursionlimit`: This sets the recursion limit higher to handle large ASTs while parsing Python files.\r\n- `get_start_dir`: This function retrieves the starting directory from command line arguments or current working directory if no argument is provided\r\n- `get_output_dir`: This gets an output directory where generated dataset JSON and HTML will be stored based on input directory provided\r\n- `Path(start_dir).rglob(\"[!_]*.py\")`: This searches for all Python files in the specified directory excluding those starting with \"_\"\r\n- `process_single_python_file`: This function is called to process each Python file individually and generate datasets.\r\n- `Process`: This class is used to spawn a new process for each Python file to manage memory usage, it sets `use_llm`, single process, etc based on provided inputs. This avoids errors of the previous LLM generating the output.\r\n- `proc.start` and `proc.join`: These methods are used to start and join the child processes respectively.\r\n- `combine_json_files`: This function combines all the individual datasets into a single dictionary and returns it as an output. "
    },
    {
        "instruction": "Calls made in `clone_github_repo` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def clone_github_repo(url: str) -> str:\n    \"\"\"\n    Clone repository or pull the latest changes and return local repository path.\n    Args:\n        url (str): The url of the github repository.\n    Returns:\n        str: The path to the cloned repository.\n    \"\"\"\n    try:\n        command = f'git ls-remote {shlex.quote(url)}'\n        subprocess.run(command, shell=True, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    except subprocess.CalledProcessError:\n        print(f'Invalid or inaccessible repository: {url}')\n        return ''\n    repo_name = url.split('/')[-1]\n    githubrepos_dir = os.path.join(os.getcwd(), 'githubrepos')\n    os.makedirs(githubrepos_dir, exist_ok=True)\n    path = os.path.join(githubrepos_dir, repo_name)\n    if not os.path.exists(path):\n        git.Repo.clone_from(url, path)\n    else:\n        repo = git.Repo(path)\n        with repo.git.custom_environment(GIT_SSH_COMMAND='ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no'):\n            repo.git.fetch()\n            default_branch = repo.head.reference.tracking_branch().remote_head\n            repo.git.reset('--hard', default_branch)\n    return path",
        "output": "Calls made in `clone_github_repo`:shlex.quote, subprocess.run, print, url.split, os.path.join, os.getcwd, os.makedirs, os.path.exists, git.Repo.clone_from, git.Repo, repo.git.custom_environment, repo.git.fetch, repo.head.reference.tracking_branch, repo.git.reset\n\nPurpose and Significance:\nThe `shlex.quote` function is used to safely quote a value for use in shell command line invocations by adding single quotes if necessary and escaping any other special characters that could cause issues when parsed as a command line argument. \n\nThe `subprocess.run` function is used to run the `git ls-remote` command to check if the provided URL is valid and accessible. If it's not, it returns a non-zero exit code, which triggers a CalledProcessError that exits with an error message. This allows for user validation of the provided URL.\n\nThe `os.path.join` function joins paths together to create new pathnames for saving cloned repositories.\n\nThe `os.getcwd` function gets the current working directory so we can save the cloned repository in a subdirectory called \"githubrepos\" within it. \n\nThe `git.Repo.clone_from` function is used to clone the git repository into the local file system.\n\nThe `repo.git.custom_environment` sets up the necessary environment variables for accessing the remote GitHub repository over SSH, since some hosts may not be configured with a known host key. \n\nThe `repo.head.reference.tracking_branch()` function gets the default branch of the cloned repository, which is then used to fetch changes and reset the local copy to that branch using `repo.git.fetch` and `repo.git.reset`. This ensures that we have the latest version of the code in case it has been updated since the last time it was cloned. \n\nThe `get_bool_from_input` function is used to convert user input into a boolean value, handling cases where they enter \"true\" or \"false\". If the input is not recognized as either of these values, it returns the current value provided in the code. This is important for maintaining backwards compatibility with the previous argument parser options that used boolean values, and making it easy for users to continue using those values without modification. "
    },
    {
        "instruction": "Calls made in `main` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Optional command-line arguments:\n    --start (str, optional): Starting directory for Python files or GitHub repository Python files. Defaults to current working directory.\n    --output_dir (str, optional): Directory to write the output files. Defaults to ./dataset/.\n    --questions_pathname (str, optional): Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\n    --model_config_pathname (str, optional): Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\n    --use_llm (bool, optional): Use llm for generating JSON answers. Defaults to False.\n    --quiet (bool, optional): Limit logging output. Defaults to False.\n    --single_process (bool, optional): If True, only a single process will be used to process Python files. Defaults to False.\n    --detailed (bool, optional): Include detailed analysis if True. Defaults to False.\n    --html (bool, optional): Generate HTML output if True. Defaults to False.\n    --I (str, optional): Interactive mode. Defaults to False.\n    \"\"\"\n\n    class PathArgument(argparse.Action):\n\n        def __call__(self, parser, namespace, values, option_string=None):\n            cleaned_value = values.strip('\"').strip(\"'\")\n            setattr(namespace, self.dest, cleaned_value)\n\n    def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n        \"\"\"\n        Get a boolean value from user input or retain the current value if the input is invalid.\n        \"\"\"\n        return True if input_str.lower() in ['t', 'true'] else False if input_str.lower() in ['f', 'false'] else current_value\n    parser = argparse.ArgumentParser(description='Process Python files to generate datasets.')\n    parser.add_argument('--start', default='.', action=PathArgument, help='Starting directory for Python files. Defaults to current working directory.')\n    parser.add_argument('--output_dir', default='./dataset/', action=PathArgument, help='Directory to write the output files. Defaults to ./dataset/.')\n    parser.add_argument('--questions_pathname', default='./py2dataset_questions.json', action=PathArgument, help='Path and filename of the questions file. Defaults to ./py2dataset_questions.json.')\n    parser.add_argument('--model_config_pathname', default='./py2dataset_model_config.yaml', action=PathArgument, help='Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.')\n    parser.add_argument('--use_llm', action='store_true', help='Use LLM for generating JSON answers. Defaults to False.')\n    parser.add_argument('--quiet', action='store_true', help='Limit logging output.')\n    parser.add_argument('--single_process', action='store_true', help='Use a single process for processing Python files. Defaults to False.')\n    parser.add_argument('--detailed', action='store_true', help='Include detailed analysis if True.')\n    parser.add_argument('--html', action='store_true', help='Generate HTML output if True.')\n    parser.add_argument('--I', '--interactive', action='store_true', dest='interactive', help='Interactive mode.')\n    args = parser.parse_args()\n    if args.interactive:\n        print('Input new value or press enter to keep current value.')\n        pathname_params = ['start', 'output_dir', 'questions_pathname', 'model_config_pathname']\n        bool_params = ['use_llm', 'quiet', 'single_process', 'detailed', 'html']\n        for param in pathname_params:\n            setattr(args, param, input(f'{param} [{getattr(args, param)}]: ') or getattr(args, param))\n        for param in bool_params:\n            setattr(args, param, get_bool_from_input(input(f'{param} [{getattr(args, param)}] (t or f): '), getattr(args, param)))\n    if not (os.path.isdir(args.start) or args.start.startswith('https://github.com/')):\n        print(f\"Invalid start directory '{args.start}'. Using current working directory.\")\n        args.start = os.getcwd()\n    if args.start.startswith('https://github.com/'):\n        args.start = clone_github_repo(args.start)\n    py2dataset(start_dir=args.start, output_dir=args.output_dir, questions_pathname=args.questions_pathname, model_config_pathname=args.model_config_pathname, use_llm=args.use_llm, quiet=args.quiet, single_process=args.single_process, detailed=args.detailed, html=args.html)",
        "output": "Calls made in `main`:values.strip(\\'\"\\').strip, values.strip, setattr, input_str.lower, argparse.ArgumentParser, parser.add_argument, parser.parse_args, print, input, getattr, get_bool_from_input, os.path.isdir, args.start.startswith, os.getcwd, clone_github_repo, py2dataset\n\nPurpose and Significance:\nThe purpose of this code is to generate datasets for Python files within a specified directory or GitHub repository by answering questions about the code and storing them in JSON format. The significance of these objects are as follows: \n\n1. `values.strip('\"').strip(\"'\")` - This function is used to remove quotes from user inputted arguments such as start, output_dir, questions_pathname, model_config_pathname. \n2. `setattr` - This function sets the value of an attribute of a class or an instance based on the specified name and value. It is used to set the values for each argument in the command-line arguments. \n3. `input_str.lower()` - This function converts the input string to lowercase for easy comparison with True/False strings. \n4. `argparse.ArgumentParser` - A simple command-line parsing utility that automatically generates help messages, argument error handling, and outputs in the correct format when passed incorrect arguments. \n5. `parser.add_argument` - This function adds a new command line option to the parser object. It takes an option name as well as any number of optional keyword arguments to specify additional features for this argument. The optional arguments are: action, help, default and required. Action is used to specify the type of input that is expected, such as boolean or string. \n6. `parser.parse_args` - This function parses the command-line options passed to the script and returns a Namespace object containing attributes for each option. \n7. `print` - Used to print messages to the console. \n8. `getattr` - Gets an attribute of an object by its name, which is used to get the current value of the argument. \n9. `args.start.startswith(\"https://github.com/\")` - This function checks if the start directory provided in the command-line arguments starts with \"https://github.com/\" and returns True or False accordingly. \n10. `os.path.isdir` - Checks whether a path is an existing directory or not. \n11. `args.start = os.getcwd()` - This sets the start directory to the current working directory if it's invalid. \n12. `clone_github_repo` - Used to clone GitHub repositories and returns the local repository path. \n13. `py2dataset` - The main function that generates datasets by processing Python files within a specified directory or GitHub repository. "
    },
    {
        "instruction": "Calls made in `get_bool_from_input` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n    \"\"\"\n        Get a boolean value from user input or retain the current value if the input is invalid.\n        \"\"\"\n    return True if input_str.lower() in ['t', 'true'] else False if input_str.lower() in ['f', 'false'] else current_value",
        "output": "Calls made in `get_bool_from_input`:input_str.lower\n\nPurpose and Significance:\nThe function `get_bool_from_input` is used to get a boolean value from user input or retain the current value if the input is invalid. It takes two arguments, `input_str`, which is the string entered by the user, and `current_value`, which is the default value for the parameter being checked. The function returns True if the input is \"t\" or \"true\", False if it's \"f\" or \"false\", and the current value otherwise.\r\n\nHere's how this code works: \r\n- `argparse` library allows command-line arguments to be easily added and processed. This makes it easier for users to call `py2dataset` from their terminals.\r\n- `get_bool_from_input` function checks whether the user input is either \"true\" or \"false\". If yes, then the respective boolean value will be returned based on that. Otherwise, it returns the current value of the parameter being checked. This helps to maintain the previous default value if the user enters an invalid input.\r\n- `get_questions` function reads the questions from a JSON file and returns them as a dictionary.\r\n- `get_model` function loads the LLM model configuration from a YAML file and returns it as a dictionary.\r\n- `get_output_dir` function creates the output directory if it doesn't exist. If no path is provided, it will use the current working directory.\r\n- `process_single_python_file` function processes each Python file to generate question-answer pairs and instructions for that file using `get_python_file_details`, `get_python_datasets`, and `save_python_data`. It returns `None` if no answer data can be acquired, else the full output. It will set recursion depth as 300 in order to accommodate most complex files. "
    },
    {
        "instruction": "Variables defined in `process_single_python_file` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def process_single_python_file(python_file_path: str, start_dir: str, model_config_path: str, questions_dict: Dict, use_llm: bool, output_dir: str, model_config: Dict=None, single_process_mode: bool=False, detailed_analysis: bool=False) -> None:\n    \"\"\"\n    Processes a single Python file to generate question-answer pairs and instructions.\n\n    Args:\n        python_file_path (str): Path to the Python file.\n        start_dir (str): Starting directory to search for Python files.\n        model_config_path (str): Path to the model configuration file.\n        questions_dict (Dict): Dictionary of questions to answer about the Python file.\n        use_llm (bool): If True, use a Large Language Model for generating JSON answers.\n        output_dir (str): Directory to save the output files.\n        model_config (Dict): Configuration dictionary for the LLM.\n        single_process_mode (bool): Use a single process if True. Defaults to False.\n        detailed_analysis (bool): Perform detailed analysis if True. Defaults to False.\n    \"\"\"\n    logging.info(f'Processing file: {python_file_path}')\n    parent_dir = os.path.dirname(start_dir)\n    relative_path = os.path.relpath(python_file_path, parent_dir)\n    relative_path = Path(relative_path)\n    base_name = '.'.join(relative_path.parts)\n    if not single_process_mode and use_llm:\n        model_config = get_model(model_config_path)\n    file_details = get_python_file_details(python_file_path)\n    if not file_details:\n        logging.error(f'Failed to get file details for {python_file_path}')\n        return\n    instruct_data = get_python_datasets(python_file_path, file_details, base_name, questions_dict, model_config, detailed_analysis)\n    if instruct_data:\n        save_python_data(file_details, instruct_data, base_name, relative_path, output_dir)\n    else:\n        logging.error(f'Failed to get instruct data for {python_file_path}')",
        "output": "Variables defined in `process_single_python_file`:relative_path, file_details, base_name, parent_dir, model_config, instruct_data\n\nPurpose and Significance:\nThe `process_single_python_file` function is responsible for processing a single Python file and generating the datasets based on the instructions specified in the question JSON. "
    },
    {
        "instruction": "Variables defined in `py2dataset` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def py2dataset(start_dir: str='', output_dir: str='', questions_pathname: str='', model_config_pathname: str='', use_llm: bool=False, quiet: bool=False, single_process: bool=False, detailed: bool=False, html: bool=False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start_dir (str): Starting directory for Python files. Defaults to current directory.\n        output_dir (str): Directory to save the output files.\n        questions_pathname (str): Path and filename of the questions file.\n        model_config_pathname (str): Path and filename of the model configuration file.\n        use_llm (bool): If True, use a Large Language Model for generating answers. Defaults to False.\n        quiet_mode (bool): Reduce logging output if True. Defaults to False.\n        single_process (bool): Use a single process for file processing if use_llm. Defaults to False.\n        detailed (bool): Include detailed analysis if True. Defaults to False.\n        html (bool): Generate HTML outputs if True. Defaults to False.\n    Returns:\n        Dict[str, List[Dict]]: Dictionary of generated datasets.\n    \"\"\"\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n    sys.setrecursionlimit(3000)\n    start_dir = get_start_dir(start_dir)\n    output_dir = get_output_dir(output_dir)\n    questions_dict = get_questions(questions_pathname)\n    model_config = get_model(model_config_pathname) if use_llm and single_process else None\n    if not use_llm:\n        single_process = True\n    for python_file_path in Path(start_dir).rglob('[!_]*.py'):\n        if not use_llm and single_process:\n            process_single_python_file(python_file_path, start_dir, model_config_pathname, questions_dict, use_llm, output_dir, model_config, single_process, detailed)\n        else:\n            proc = Process(target=process_single_python_file, args=(python_file_path, start_dir, model_config_pathname, questions_dict, use_llm, output_dir, None, single_process, detailed))\n            proc.start()\n            proc.join()\n    return combine_json_files(output_dir, html)",
        "output": "Variables defined in `py2dataset`:single_process, model_config, output_dir, start_dir, questions_dict, proc\n\nPurpose and Significance:\nIn `def py2dataset`, these are some important variables used in this function for processing Python files and generating datasets. \n- `single_process`: A boolean flag to use a single process or not. If True, only one process will be used to process all the Python files. Otherwise, multiple processes will be spawned for each file. This is useful when using LLM because it can consume a lot of memory and time to generate JSON answers for each file.\n- `model_config`: A dictionary containing the configuration for the LLM model if `use_llm` is True. It is loaded only once, not for each file, which saves time and memory usage.\n- `output_dir`: The directory where all the output files will be saved. This can be changed via the CLI option or is a default in current directory under './dataset' name.\r\n- `start_dir`: Directory from where we'll look for Python files. We'll search recursively within this directory for `.py` files, excluding those starting with an underscore `_`.\r\n- `questions_dict`: A dictionary of questions to answer about the Python file. This is loaded from a JSON file specified by the CLI option `--questions_pathname`.\r\n- `proc`: An instance of the `Process` class used for spawning child processes when `single_process` is False and `use_llm` is True.\r\n\nIn `def process_single_python_file`, these are some important variables:\r\n- `python_file_path`: The path to the Python file we're currently processing.\r\n- `start_dir`: Starting directory for searching Python files. This is used to generate a relative path for saving output files.\r\n- `model_config_pathname`: Path and filename of the model configuration file, which is loaded only if `use_llm` is True and `single_process` is False.\r\n- `questions_dict`: Dictionary of questions to answer about the Python file. This is loaded from a JSON file specified by the CLI option `--questions_pathname`.\r\n- `use_llm`: A boolean flag to use LLM or not. If True, we'll generate JSON answers for each file using the LLM model.\r\n- `output_dir`: Directory where output files will be saved.\r\n- `model_config`: Configuration dictionary for the LLM. This is loaded only if `use_llm` is True and `single_process` is False.\r\n- `detailed_analysis`: A boolean flag to perform detailed analysis or not. If True, we'll include more information in the output files such as function names, variable names, etc. "
    },
    {
        "instruction": "Variables defined in `clone_github_repo` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def clone_github_repo(url: str) -> str:\n    \"\"\"\n    Clone repository or pull the latest changes and return local repository path.\n    Args:\n        url (str): The url of the github repository.\n    Returns:\n        str: The path to the cloned repository.\n    \"\"\"\n    try:\n        command = f'git ls-remote {shlex.quote(url)}'\n        subprocess.run(command, shell=True, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    except subprocess.CalledProcessError:\n        print(f'Invalid or inaccessible repository: {url}')\n        return ''\n    repo_name = url.split('/')[-1]\n    githubrepos_dir = os.path.join(os.getcwd(), 'githubrepos')\n    os.makedirs(githubrepos_dir, exist_ok=True)\n    path = os.path.join(githubrepos_dir, repo_name)\n    if not os.path.exists(path):\n        git.Repo.clone_from(url, path)\n    else:\n        repo = git.Repo(path)\n        with repo.git.custom_environment(GIT_SSH_COMMAND='ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no'):\n            repo.git.fetch()\n            default_branch = repo.head.reference.tracking_branch().remote_head\n            repo.git.reset('--hard', default_branch)\n    return path",
        "output": "Variables defined in `clone_github_repo`:repo_name, path, default_branch, githubrepos_dir, command, repo\n\nPurpose and Significance:\nThe purpose of this object is to clone a GitHub repository or pull the latest changes and return the local repository path. The significance of these variables are as follows:\r\n- `repo_name`: This variable stores the name of the cloned repository.\r\n- `path`: This variable stores the path where the cloned repository will be saved locally.\r\n- `default_branch`: This variable is used to fetch and reset changes in the local repository to the default branch. It represents the current tracking branch that is checked out on the local machine.\r\n- `githubrepos_dir`: This variable is a directory where all the cloned repositories will be saved locally.\r\n- `command`: This variable stores the command used to clone or fetch changes from a GitHub repository.\r\n- `repo`: This variable represents the Git repository object that is created using the `git.Repo` class, which allows us to interact with the local repository.\r\n\nThe function uses these variables to clone a GitHub repository and reset the changes made in it to the default branch. "
    },
    {
        "instruction": "Variables defined in `main` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Optional command-line arguments:\n    --start (str, optional): Starting directory for Python files or GitHub repository Python files. Defaults to current working directory.\n    --output_dir (str, optional): Directory to write the output files. Defaults to ./dataset/.\n    --questions_pathname (str, optional): Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\n    --model_config_pathname (str, optional): Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\n    --use_llm (bool, optional): Use llm for generating JSON answers. Defaults to False.\n    --quiet (bool, optional): Limit logging output. Defaults to False.\n    --single_process (bool, optional): If True, only a single process will be used to process Python files. Defaults to False.\n    --detailed (bool, optional): Include detailed analysis if True. Defaults to False.\n    --html (bool, optional): Generate HTML output if True. Defaults to False.\n    --I (str, optional): Interactive mode. Defaults to False.\n    \"\"\"\n\n    class PathArgument(argparse.Action):\n\n        def __call__(self, parser, namespace, values, option_string=None):\n            cleaned_value = values.strip('\"').strip(\"'\")\n            setattr(namespace, self.dest, cleaned_value)\n\n    def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n        \"\"\"\n        Get a boolean value from user input or retain the current value if the input is invalid.\n        \"\"\"\n        return True if input_str.lower() in ['t', 'true'] else False if input_str.lower() in ['f', 'false'] else current_value\n    parser = argparse.ArgumentParser(description='Process Python files to generate datasets.')\n    parser.add_argument('--start', default='.', action=PathArgument, help='Starting directory for Python files. Defaults to current working directory.')\n    parser.add_argument('--output_dir', default='./dataset/', action=PathArgument, help='Directory to write the output files. Defaults to ./dataset/.')\n    parser.add_argument('--questions_pathname', default='./py2dataset_questions.json', action=PathArgument, help='Path and filename of the questions file. Defaults to ./py2dataset_questions.json.')\n    parser.add_argument('--model_config_pathname', default='./py2dataset_model_config.yaml', action=PathArgument, help='Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.')\n    parser.add_argument('--use_llm', action='store_true', help='Use LLM for generating JSON answers. Defaults to False.')\n    parser.add_argument('--quiet', action='store_true', help='Limit logging output.')\n    parser.add_argument('--single_process', action='store_true', help='Use a single process for processing Python files. Defaults to False.')\n    parser.add_argument('--detailed', action='store_true', help='Include detailed analysis if True.')\n    parser.add_argument('--html', action='store_true', help='Generate HTML output if True.')\n    parser.add_argument('--I', '--interactive', action='store_true', dest='interactive', help='Interactive mode.')\n    args = parser.parse_args()\n    if args.interactive:\n        print('Input new value or press enter to keep current value.')\n        pathname_params = ['start', 'output_dir', 'questions_pathname', 'model_config_pathname']\n        bool_params = ['use_llm', 'quiet', 'single_process', 'detailed', 'html']\n        for param in pathname_params:\n            setattr(args, param, input(f'{param} [{getattr(args, param)}]: ') or getattr(args, param))\n        for param in bool_params:\n            setattr(args, param, get_bool_from_input(input(f'{param} [{getattr(args, param)}] (t or f): '), getattr(args, param)))\n    if not (os.path.isdir(args.start) or args.start.startswith('https://github.com/')):\n        print(f\"Invalid start directory '{args.start}'. Using current working directory.\")\n        args.start = os.getcwd()\n    if args.start.startswith('https://github.com/'):\n        args.start = clone_github_repo(args.start)\n    py2dataset(start_dir=args.start, output_dir=args.output_dir, questions_pathname=args.questions_pathname, model_config_pathname=args.model_config_pathname, use_llm=args.use_llm, quiet=args.quiet, single_process=args.single_process, detailed=args.detailed, html=args.html)",
        "output": "Variables defined in `main`:args, bool_params, cleaned_value, pathname_params, parser\n\nPurpose and Significance:\nThe `parser` object is an instance of the `argparse.ArgumentParser()` class that allows us to define and parse command-line arguments for our program. The `bool_params` list contains the names of boolean parameters that we want to ask the user to input interactively if they choose to use `--interactive` mode. The `pathname_params` list contains the names of parameters that should be treated as paths, and `cleaned_value` is a helper function used to remove any leading or trailing quotes from user inputs. Finally, `args` is an instance of `Namespace` which holds all the command-line arguments passed by the user. "
    },
    {
        "instruction": "Returns from `py2dataset` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def py2dataset(start_dir: str='', output_dir: str='', questions_pathname: str='', model_config_pathname: str='', use_llm: bool=False, quiet: bool=False, single_process: bool=False, detailed: bool=False, html: bool=False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start_dir (str): Starting directory for Python files. Defaults to current directory.\n        output_dir (str): Directory to save the output files.\n        questions_pathname (str): Path and filename of the questions file.\n        model_config_pathname (str): Path and filename of the model configuration file.\n        use_llm (bool): If True, use a Large Language Model for generating answers. Defaults to False.\n        quiet_mode (bool): Reduce logging output if True. Defaults to False.\n        single_process (bool): Use a single process for file processing if use_llm. Defaults to False.\n        detailed (bool): Include detailed analysis if True. Defaults to False.\n        html (bool): Generate HTML outputs if True. Defaults to False.\n    Returns:\n        Dict[str, List[Dict]]: Dictionary of generated datasets.\n    \"\"\"\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n    sys.setrecursionlimit(3000)\n    start_dir = get_start_dir(start_dir)\n    output_dir = get_output_dir(output_dir)\n    questions_dict = get_questions(questions_pathname)\n    model_config = get_model(model_config_pathname) if use_llm and single_process else None\n    if not use_llm:\n        single_process = True\n    for python_file_path in Path(start_dir).rglob('[!_]*.py'):\n        if not use_llm and single_process:\n            process_single_python_file(python_file_path, start_dir, model_config_pathname, questions_dict, use_llm, output_dir, model_config, single_process, detailed)\n        else:\n            proc = Process(target=process_single_python_file, args=(python_file_path, start_dir, model_config_pathname, questions_dict, use_llm, output_dir, None, single_process, detailed))\n            proc.start()\n            proc.join()\n    return combine_json_files(output_dir, html)",
        "output": "Returns from `py2dataset`:combine_json_files(output_dir, html)\n\nPurpose and Significance:\nThe `py2dataset` function is responsible for generating datasets by processing Python files within a specified directory. It takes in various parameters such as starting directory, output directory, questions path, model config path, use of LLM, quiet mode, and single process mode. The `process_single_python_file` function is called to generate question-answer pairs and instructions for each Python file found within the given directory. If `use_llm` is True and `single_process` is False, a new process is spawned for each file to manage memory and performance. Finally, all individual datasets are combined into a single dictionary using the `combine_json_files` function with HTML output generated if `html` is True.\r\n\nThe 'Returns from `py2dataset`: `combine_json_files(output_dir, html)`' object returns a dictionary of generated datasets. "
    },
    {
        "instruction": "Returns from `clone_github_repo` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def clone_github_repo(url: str) -> str:\n    \"\"\"\n    Clone repository or pull the latest changes and return local repository path.\n    Args:\n        url (str): The url of the github repository.\n    Returns:\n        str: The path to the cloned repository.\n    \"\"\"\n    try:\n        command = f'git ls-remote {shlex.quote(url)}'\n        subprocess.run(command, shell=True, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n    except subprocess.CalledProcessError:\n        print(f'Invalid or inaccessible repository: {url}')\n        return ''\n    repo_name = url.split('/')[-1]\n    githubrepos_dir = os.path.join(os.getcwd(), 'githubrepos')\n    os.makedirs(githubrepos_dir, exist_ok=True)\n    path = os.path.join(githubrepos_dir, repo_name)\n    if not os.path.exists(path):\n        git.Repo.clone_from(url, path)\n    else:\n        repo = git.Repo(path)\n        with repo.git.custom_environment(GIT_SSH_COMMAND='ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no'):\n            repo.git.fetch()\n            default_branch = repo.head.reference.tracking_branch().remote_head\n            repo.git.reset('--hard', default_branch)\n    return path",
        "output": "Returns from `clone_github_repo`:path\n\nPurpose and Significance:\nThe purpose of the clone_github_repo function is to clones or fetches a Git repository specified by URL and returns the path to its local location. It's significant because it allows users to provide a GitHub repository URL as an input parameter instead of a directory path, which makes it easier for them to run py2dataset on code that they don't have locally. The function is called from main() when the user provides a GitHub repository URL as the start_dir argument and returns the local path to the cloned repo or fetches any updates if the repo already exists.\r\n\nDescribe how this object can be used: 'The function `clone_github_repo` can be used by providing a GitHub repository URL in the --start command line parameter when running py2dataset.' "
    },
    {
        "instruction": "Returns from `main` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Optional command-line arguments:\n    --start (str, optional): Starting directory for Python files or GitHub repository Python files. Defaults to current working directory.\n    --output_dir (str, optional): Directory to write the output files. Defaults to ./dataset/.\n    --questions_pathname (str, optional): Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\n    --model_config_pathname (str, optional): Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\n    --use_llm (bool, optional): Use llm for generating JSON answers. Defaults to False.\n    --quiet (bool, optional): Limit logging output. Defaults to False.\n    --single_process (bool, optional): If True, only a single process will be used to process Python files. Defaults to False.\n    --detailed (bool, optional): Include detailed analysis if True. Defaults to False.\n    --html (bool, optional): Generate HTML output if True. Defaults to False.\n    --I (str, optional): Interactive mode. Defaults to False.\n    \"\"\"\n\n    class PathArgument(argparse.Action):\n\n        def __call__(self, parser, namespace, values, option_string=None):\n            cleaned_value = values.strip('\"').strip(\"'\")\n            setattr(namespace, self.dest, cleaned_value)\n\n    def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n        \"\"\"\n        Get a boolean value from user input or retain the current value if the input is invalid.\n        \"\"\"\n        return True if input_str.lower() in ['t', 'true'] else False if input_str.lower() in ['f', 'false'] else current_value\n    parser = argparse.ArgumentParser(description='Process Python files to generate datasets.')\n    parser.add_argument('--start', default='.', action=PathArgument, help='Starting directory for Python files. Defaults to current working directory.')\n    parser.add_argument('--output_dir', default='./dataset/', action=PathArgument, help='Directory to write the output files. Defaults to ./dataset/.')\n    parser.add_argument('--questions_pathname', default='./py2dataset_questions.json', action=PathArgument, help='Path and filename of the questions file. Defaults to ./py2dataset_questions.json.')\n    parser.add_argument('--model_config_pathname', default='./py2dataset_model_config.yaml', action=PathArgument, help='Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.')\n    parser.add_argument('--use_llm', action='store_true', help='Use LLM for generating JSON answers. Defaults to False.')\n    parser.add_argument('--quiet', action='store_true', help='Limit logging output.')\n    parser.add_argument('--single_process', action='store_true', help='Use a single process for processing Python files. Defaults to False.')\n    parser.add_argument('--detailed', action='store_true', help='Include detailed analysis if True.')\n    parser.add_argument('--html', action='store_true', help='Generate HTML output if True.')\n    parser.add_argument('--I', '--interactive', action='store_true', dest='interactive', help='Interactive mode.')\n    args = parser.parse_args()\n    if args.interactive:\n        print('Input new value or press enter to keep current value.')\n        pathname_params = ['start', 'output_dir', 'questions_pathname', 'model_config_pathname']\n        bool_params = ['use_llm', 'quiet', 'single_process', 'detailed', 'html']\n        for param in pathname_params:\n            setattr(args, param, input(f'{param} [{getattr(args, param)}]: ') or getattr(args, param))\n        for param in bool_params:\n            setattr(args, param, get_bool_from_input(input(f'{param} [{getattr(args, param)}] (t or f): '), getattr(args, param)))\n    if not (os.path.isdir(args.start) or args.start.startswith('https://github.com/')):\n        print(f\"Invalid start directory '{args.start}'. Using current working directory.\")\n        args.start = os.getcwd()\n    if args.start.startswith('https://github.com/'):\n        args.start = clone_github_repo(args.start)\n    py2dataset(start_dir=args.start, output_dir=args.output_dir, questions_pathname=args.questions_pathname, model_config_pathname=args.model_config_pathname, use_llm=args.use_llm, quiet=args.quiet, single_process=args.single_process, detailed=args.detailed, html=args.html)",
        "output": "Returns from `main`:True if input_str.lower() in ['t, true'] else False if input_str.lower() in ['f, false'] else current_value\n\nPurpose and Significance:\nThe given object is a lambda function that checks if the input value `input_str` matches any of 'true', 'false', 'True', or 'False' (ignoring case) and returns True for 't' or 'true' and False for 'f' or 'false'. If it doesn't match either, it returns the current value. This is used to convert a string input into a boolean value. "
    },
    {
        "instruction": "Returns from `get_bool_from_input` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n    \"\"\"\n        Get a boolean value from user input or retain the current value if the input is invalid.\n        \"\"\"\n    return True if input_str.lower() in ['t', 'true'] else False if input_str.lower() in ['f', 'false'] else current_value",
        "output": "Returns from `get_bool_from_input`:True if input_str.lower() in ['t, true'] else False if input_str.lower() in ['f, false'] else current_value\n\nPurpose and Significance:\nThe purpose of this object is to get a boolean value from user input or retain the current value if the input is invalid. "
    },
    {
        "instruction": "Methods defined in `PathArgument` in Python file: `py2dataset.py2dataset.py`?",
        "input": "class PathArgument(argparse.Action):\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        cleaned_value = values.strip('\"').strip(\"'\")\n        setattr(namespace, self.dest, cleaned_value)",
        "output": "Methods defined in `PathArgument`:__call__\n\nPurpose and Significance:\nThe `PathArgument` class is a custom action for the `argparse` module that allows us to strip quotes from the input path arguments. The `__call__` method is called when an argument with this action is parsed, and it takes in three parameters: `parser`, `namespace`, and `values`. The `parser` parameter refers to the parser object itself, while `namespace` is a namespace object that contains all of the arguments passed to the script. `values` is the value of the current argument being processed. We use this method to remove any quotes from the input path by calling `.strip('\"').strip(\"'\")`. "
    },
    {
        "instruction": "Variables defined in `PathArgument` in Python file: `py2dataset.py2dataset.py`?",
        "input": "class PathArgument(argparse.Action):\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        cleaned_value = values.strip('\"').strip(\"'\")\n        setattr(namespace, self.dest, cleaned_value)",
        "output": "Variables defined in `PathArgument`:cleaned_value\n\nPurpose and Significance:\nThe purpose of defining the PathArgument class is to clean user input strings before they are passed as command-line arguments. It takes the user input string, removes any single or double quotes that might have been added to it accidentally, and returns the cleaned value. This ensures that the program can handle both quoted and unquoted paths correctly.\r\n\nThe significance of this class is that it allows users to provide either a quoted or unquoted path as an input for `start`, `output_dir`, `questions_pathname`, and `model_config_pathname` parameters in the command line arguments without worrying about extra characters added due to misinterpreted escaping.\r\n\nAlso, `PathArgument` class inherits from argparse.Action which is a built-in action for adding user-defined actions to the parser. It allows us to define custom behavior when parsing command-line arguments. "
    },
    {
        "instruction": "Inheritance of `PathArgument` in Python file: `py2dataset.py2dataset.py`?",
        "input": "class PathArgument(argparse.Action):\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        cleaned_value = values.strip('\"').strip(\"'\")\n        setattr(namespace, self.dest, cleaned_value)",
        "output": "Inheritance of `PathArgument`:argparse.Action\n\nPurpose and Significance:\nThe purpose of PathArgument is to create a custom Action that strips out quotes before processing a string pathname parameter.\r\n\nExample usage:\r\n```python\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--path\", action=PathArgument)\r\nargs = parser.parse_args([\"--path\", \"\\\"/path/to/file\\\"\"])\r\nprint(args.path)  # Outputs: /path/to/file \r\n``` "
    },
    {
        "instruction": "Inputs to `__call__` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def __call__(self, parser, namespace, values, option_string=None):\n    cleaned_value = values.strip('\"').strip(\"'\")\n    setattr(namespace, self.dest, cleaned_value)",
        "output": "Inputs to `__call__`:self, parser, namespace, values, option_string\n\nPurpose and Significance:\nThere is one argument to consider. Here is the code again for easier reading with line number comments to mark important lines:\r\n\n```python\r\nclass PathArgument(argparse.Action):\r\n    def __call__(self, parser, namespace, values, option_string=None):\r\n        cleaned_value = values.strip('\"').strip(\"'\")  # [1]\r\n        setattr(namespace, self.dest, cleaned_value)  # [2]\r\n```\r\n\nThis is a custom action for the `argparse` module that takes a string argument and cleans it by removing any leading or trailing quotes (`\"` or `'`) before setting it as an attribute of the namespace object.\r\n\n- Line 1: We call `.strip('\"').strip(\"'\")` to remove any double-quote character `\"` from either end, if it is present.\r\n- Line 2: We use `setattr` to set the value of the destination attribute (`self.dest`) in the namespace object with the cleaned value.\r\n\nThis class is used when adding arguments for `--start`, `--output_dir`, and other similar arguments that are expected to receive a path as input. This way, users can specify paths within single or double quotes, and these quotes will be removed automatically by this custom action. "
    },
    {
        "instruction": "Calls made in `__call__` in Python file: `py2dataset.py2dataset.py`?",
        "input": "def __call__(self, parser, namespace, values, option_string=None):\n    cleaned_value = values.strip('\"').strip(\"'\")\n    setattr(namespace, self.dest, cleaned_value)",
        "output": "Calls made in `__call__`:values.strip(\\'\"\\').strip, values.strip, setattr\n\nPurpose and Significance:\nThe `PathArgument` class is used to process user input for the `start`, `output_dir`, and other directory paths by stripping away any leading or trailing quotes around the input value and setting it as an attribute in the namespace. The `get_bool_from_input` function takes a string input from the user and returns a boolean value based on their choice of 't' or 'f'. It is used to set the values for the `use_llm`, `quiet`, `single_process`, and `detailed` arguments. The `main` function is the entry point for the program, where it processes command-line arguments and sets up the necessary parameters for calling the `py2dataset` function. "
    },
    {
        "instruction": "1) Describe the Purpose and Processing summary of Python file: `py2dataset.py2dataset.py`; 2) Summarize the Significance of applicable Function, Class, and Method; 3) Explain what each Input, Output, and Variable does in the code.",
        "input": "```python\n\"\"\"\nFor each Python file within a given directory, this module is designed to generate, save, \nand return datasets that include responses to questions about the code. \nRequirements:\n[req00] The process_single_python_file function shall:\n    a. Accept parameters for Python file path, start directory, model config \n       pathname, questions dict, use of LLM, and output dir.\n    b. If 'use_llm' is True, use 'get_model' to instantiate LLM model config.\n    c. Use 'get_python_file_details' to retrieve Python file info.\n    d. Use 'get_python_datasets' to acquire instruct.json datasets.\n    e. Use 'save_python_data' to store file details and instruct.json data.\n[req01] The py2dataset function shall:\n    a. Accept parameters for start directory, output dir, questions path, model\n       config path, use of LLM, and quiet mode.\n    b. Adjust logging level based on 'quiet'.\n    c. Use current working dir if no valid start dir is provided.\n    d. Get output dir with 'get_output_dir'.\n    e. Retrieve questions dict with 'get_questions'.\n    f. Search for Python files using 'rglob', excluding those starting with \"_\".\n    g. For each Python file, spawn a child process with 'process_single_python_file'\n       to get file details and instruct.json data, if single_process is False.\n    h. Combine instruct.json files with 'combine_json_files'.\n    i. Return datasets.\n[req02] The main function shall:\n    a. Accept and process command-line args.\n    b. Determine py2dataset parameters based on processed arguments.\n    c. Call py2dataset with derived parameters.\n\"\"\"\nimport sys\nimport argparse\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom multiprocessing import Process\nimport subprocess\nimport os\nimport git\nimport shlex\n\nfrom get_python_file_details import get_python_file_details\nfrom get_python_datasets import get_python_datasets\nfrom get_params import (\n    get_questions,\n    get_model,\n    get_output_dir,\n    get_start_dir,\n)\nfrom save_output import combine_json_files, save_python_data\n\n\ndef process_single_python_file(\n    python_file_path: str,\n    start_dir: str,\n    model_config_path: str,\n    questions_dict: Dict,\n    use_llm: bool,\n    output_dir: str,\n    model_config: Dict = None,\n    single_process_mode: bool = False,\n    detailed_analysis: bool = False,\n) -> None:\n    \"\"\"\n    Processes a single Python file to generate question-answer pairs and instructions.\n\n    Args:\n        python_file_path (str): Path to the Python file.\n        start_dir (str): Starting directory to search for Python files.\n        model_config_path (str): Path to the model configuration file.\n        questions_dict (Dict): Dictionary of questions to answer about the Python file.\n        use_llm (bool): If True, use a Large Language Model for generating JSON answers.\n        output_dir (str): Directory to save the output files.\n        model_config (Dict): Configuration dictionary for the LLM.\n        single_process_mode (bool): Use a single process if True. Defaults to False.\n        detailed_analysis (bool): Perform detailed analysis if True. Defaults to False.\n    \"\"\"\n    logging.info(f\"Processing file: {python_file_path}\")\n\n    # need to define relative path as the having at least one parent directory\n    # e.g relative patch should be equal to <last start_directory directory> / file name without path from python_file_path\n    # Get the last directory in start_dir\n    parent_dir = os.path.dirname(start_dir)\n    relative_path = os.path.relpath(python_file_path, parent_dir)\n    relative_path = Path(relative_path)\n    base_name = \".\".join(relative_path.parts)\n\n    if not single_process_mode and use_llm:\n        model_config = get_model(model_config_path)\n\n    file_details = get_python_file_details(python_file_path)\n    if not file_details:\n        logging.error(f\"Failed to get file details for {python_file_path}\")\n        return\n\n    instruct_data = get_python_datasets(\n        python_file_path,\n        file_details,\n        base_name,\n        questions_dict,\n        model_config,\n        detailed_analysis,\n    )\n\n    if instruct_data:\n        save_python_data(\n            file_details, instruct_data, base_name, relative_path, output_dir\n        )\n    else:\n        logging.error(f\"Failed to get instruct data for {python_file_path}\")\n\n\ndef py2dataset(\n    start_dir: str = \"\",\n    output_dir: str = \"\",\n    questions_pathname: str = \"\",\n    model_config_pathname: str = \"\",\n    use_llm: bool = False,\n    quiet: bool = False,\n    single_process: bool = False,\n    detailed: bool = False,\n    html: bool = False,\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start_dir (str): Starting directory for Python files. Defaults to current directory.\n        output_dir (str): Directory to save the output files.\n        questions_pathname (str): Path and filename of the questions file.\n        model_config_pathname (str): Path and filename of the model configuration file.\n        use_llm (bool): If True, use a Large Language Model for generating answers. Defaults to False.\n        quiet_mode (bool): Reduce logging output if True. Defaults to False.\n        single_process (bool): Use a single process for file processing if use_llm. Defaults to False.\n        detailed (bool): Include detailed analysis if True. Defaults to False.\n        html (bool): Generate HTML outputs if True. Defaults to False.\n    Returns:\n        Dict[str, List[Dict]]: Dictionary of generated datasets.\n    \"\"\"\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n\n    sys.setrecursionlimit(3000)  # Set recursion limit higher for AST parsing\n\n    start_dir = get_start_dir(start_dir)\n    output_dir = get_output_dir(output_dir)\n    questions_dict = get_questions(questions_pathname)\n\n    # Load model configuration if LLM is used and single process mode is enabled\n    model_config = (\n        get_model(model_config_pathname) if use_llm and single_process else None\n    )\n\n    if not use_llm:\n        single_process = True\n\n    # Process each Python file in the directory\n    for python_file_path in Path(start_dir).rglob(\"[!_]*.py\"):\n        # if use_llm is false or single_process is true then process the file within the current process\n\n        if not use_llm and single_process:\n            process_single_python_file(\n                python_file_path,\n                start_dir,\n                model_config_pathname,\n                questions_dict,\n                use_llm,\n                output_dir,\n                model_config,\n                single_process,\n                detailed,\n            )\n        else:\n            # Spawn new process for each file to manage memory and performance\n            proc = Process(\n                target=process_single_python_file,\n                args=(\n                    python_file_path,\n                    start_dir,\n                    model_config_pathname,\n                    questions_dict,\n                    use_llm,\n                    output_dir,\n                    None,\n                    single_process,\n                    detailed,\n                ),\n            )\n            proc.start()\n            proc.join()\n\n    # Combine all the individual datasets into a single dictionary\n    return combine_json_files(output_dir, html)\n\n\ndef clone_github_repo(url: str) -> str:\n    \"\"\"\n    Clone repository or pull the latest changes and return local repository path.\n    Args:\n        url (str): The url of the github repository.\n    Returns:\n        str: The path to the cloned repository.\n    \"\"\"\n    # Check valid Git repository\n    try:\n        command = f\"git ls-remote {shlex.quote(url)}\"\n        subprocess.run(\n            command,\n            shell=True,\n            check=True,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n        )\n    except subprocess.CalledProcessError:\n        print(f\"Invalid or inaccessible repository: {url}\")\n        return \"\"\n\n    # Proceed with cloning or fetching\n    repo_name = url.split(\"/\")[-1]\n    githubrepos_dir = os.path.join(os.getcwd(), \"githubrepos\")\n    os.makedirs(githubrepos_dir, exist_ok=True)\n    path = os.path.join(githubrepos_dir, repo_name)\n    if not os.path.exists(path):\n        git.Repo.clone_from(url, path)\n    else:\n        repo = git.Repo(path)\n        with repo.git.custom_environment(\n            GIT_SSH_COMMAND=\"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no\"\n        ):\n            repo.git.fetch()\n            default_branch = repo.head.reference.tracking_branch().remote_head\n            repo.git.reset(\"--hard\", default_branch)\n    return path\n\n\ndef main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Optional command-line arguments:\n    --start (str, optional): Starting directory for Python files or GitHub repository Python files. Defaults to current working directory.\n    --output_dir (str, optional): Directory to write the output files. Defaults to ./dataset/.\n    --questions_pathname (str, optional): Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\n    --model_config_pathname (str, optional): Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\n    --use_llm (bool, optional): Use llm for generating JSON answers. Defaults to False.\n    --quiet (bool, optional): Limit logging output. Defaults to False.\n    --single_process (bool, optional): If True, only a single process will be used to process Python files. Defaults to False.\n    --detailed (bool, optional): Include detailed analysis if True. Defaults to False.\n    --html (bool, optional): Generate HTML output if True. Defaults to False.\n    --I (str, optional): Interactive mode. Defaults to False.\n    \"\"\"\n\n    class PathArgument(argparse.Action):\n        def __call__(self, parser, namespace, values, option_string=None):\n            cleaned_value = values.strip('\"').strip(\"'\")\n            setattr(namespace, self.dest, cleaned_value)\n\n    def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n        \"\"\"\n        Get a boolean value from user input or retain the current value if the input is invalid.\n        \"\"\"\n        return (\n            True\n            if input_str.lower() in [\"t\", \"true\"]\n            else False\n            if input_str.lower() in [\"f\", \"false\"]\n            else current_value\n        )\n\n    # parse each command line entry\n    parser = argparse.ArgumentParser(\n        description=\"Process Python files to generate datasets.\"\n    )\n    parser.add_argument(\n        \"--start\",\n        default=\".\",\n        action=PathArgument,\n        help=\"Starting directory for Python files. Defaults to current working directory.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        default=\"./dataset/\",\n        action=PathArgument,\n        help=\"Directory to write the output files. Defaults to ./dataset/.\",\n    )\n    parser.add_argument(\n        \"--questions_pathname\",\n        default=\"./py2dataset_questions.json\",\n        action=PathArgument,\n        help=\"Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\",\n    )\n    parser.add_argument(\n        \"--model_config_pathname\",\n        default=\"./py2dataset_model_config.yaml\",\n        action=PathArgument,\n        help=\"Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\",\n    )\n    parser.add_argument(\n        \"--use_llm\",\n        action=\"store_true\",\n        help=\"Use LLM for generating JSON answers. Defaults to False.\",\n    )\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Limit logging output.\")\n    parser.add_argument(\n        \"--single_process\",\n        action=\"store_true\",\n        help=\"Use a single process for processing Python files. Defaults to False.\",\n    )\n    parser.add_argument(\n        \"--detailed\", action=\"store_true\", help=\"Include detailed analysis if True.\"\n    )\n    parser.add_argument(\n        \"--html\", action=\"store_true\", help=\"Generate HTML output if True.\"\n    )\n    parser.add_argument(\n        \"--I\",\n        \"--interactive\",\n        action=\"store_true\",\n        dest=\"interactive\",\n        help=\"Interactive mode.\",\n    )\n    args = parser.parse_args()\n\n    # Interactive mode adjustments\n    if args.interactive:\n        print(\"Input new value or press enter to keep current value.\")\n        pathname_params = [\n            \"start\",\n            \"output_dir\",\n            \"questions_pathname\",\n            \"model_config_pathname\",\n        ]\n        bool_params = [\"use_llm\", \"quiet\", \"single_process\", \"detailed\", \"html\"]\n        for param in pathname_params:\n            setattr(\n                args,\n                param,\n                input(f\"{param} [{getattr(args, param)}]: \") or getattr(args, param),\n            )\n        for param in bool_params:\n            setattr(\n                args,\n                param,\n                get_bool_from_input(\n                    input(f\"{param} [{getattr(args, param)}] (t or f): \"),\n                    getattr(args, param),\n                ),\n            )\n\n    # Validate the start directory\n    if not (os.path.isdir(args.start) or args.start.startswith(\"https://github.com/\")):\n        print(\n            f\"Invalid start directory '{args.start}'. Using current working directory.\"\n        )\n        args.start = os.getcwd()\n\n    # if the start directory is a github repository, clone it and change the start directory to the local repository\n    if args.start.startswith(\"https://github.com/\"):\n        args.start = clone_github_repo(args.start)\n\n    # Call py2dataset with args\n    py2dataset(\n        start_dir=args.start,\n        output_dir=args.output_dir,\n        questions_pathname=args.questions_pathname,\n        model_config_pathname=args.model_config_pathname,\n        use_llm=args.use_llm,\n        quiet=args.quiet,\n        single_process=args.single_process,\n        detailed=args.detailed,\n        html=args.html,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n\n```",
        "output": "The purpose of this module is to generate, save, and return datasets that include responses to questions about Python files. The processing summary includes the following functions: `process_single_python_file`, `py2dataset`, `clone_github_repo`, and `main`. \n\n1. `process_single_python_file` function: This function is responsible for processing a single Python file to generate question-answer pairs and instructions. It accepts parameters for Python file path, start directory, model config pathname, questions dict, use of LLM, and output dir. If 'use_llm' is True, it uses the `get_model` function to instantiate an LLM model configuration. It then retrieves the file details using the `get_python_file_details` function and acquires instruct.json datasets using the `get_python_datasets` function. Finally, it saves the file details and instruct.json data using the `save_python_data` function. \n2. `py2dataset` function: This function is responsible for generating datasets by processing Python files within a specified directory. It accepts parameters for start directory, output dir, questions path, model config path, use of LLM, and quiet mode. Based on the command-line arguments provided, it adjusts logging level based on 'quiet', determines an output dir with `get_output_dir`, retrieves a question dictionary with `get_questions`. It then uses the `Path(start_dir).rglob` method to search for Python files excluding those starting with \"_\". For each Python file, it spawns a child process using `process_single_python_file` to get file details and instruct.json data if 'single_process' is False. If 'use_llm' is True and single_process is False, it loads the LLM model configuration in the main process. After processing all Python files, it combines the individual datasets using `combine_json_files` function and returns a dictionary of generated datasets. \n3. `clone_github_repo` function: This function clones a GitHub repository or pulls the latest changes based on the provided URL. It first checks if the URL is valid by running `git ls-remote` command, then proceeds to clone or fetch depending on whether it's a new repository or an existing one. \n4. `main` function: This function processes command-line arguments and determines py2dataset parameters based on processed arguments. It calls the `py2dataset` with the derived parameters including logging, search parameters (path/repository) based on `args.interactive` (switched True for interactive mode). Additionally, it checks if 'use_llm' is set to False or not and sets a recursion limit of 3000 for AST parsing. \n5. `PathArgument` class: This class inherits from the `argparse.Action` class and overrides its `__call__` method to clean up input values by removing quotes and whitespaces. It is used in the command-line arguments to ensure that paths are passed as strings without any extra characters. \n6. Variables defined in `process_single_python_file`: 'relative_path', 'file_details', 'base_name', 'parent_dir', 'model_config', and 'instruct_data'. These variables are used to generate the relative path, retrieve file details, create a base name for the output files, get the parent directory of start dir, define model configuration based on use_llm and single_process, acquire instructions from the `get_python_datasets` function. \n7. Variables defined in `py2dataset`: 'single_process', 'model_config', 'output_dir', 'start_dir', and 'questions_dict'. These variables are used to determine if a single process is used for file processing, define the output directory using `get_output_dir`, get the question dictionary from `get_questions` function, search for Python files using `Path(start_dir).rglob`, spawn child processes with `process_single_python_file` based on 'use_llm' and 'single_process', and combine all the individual datasets into a single dictionary. \n8. Variables defined in `clone_github_repo`: 'repo_name', 'path', and 'default_branch'. These variables are used to clone a repository or fetch the latest changes, set the repo path based on 'gitrepos' directory, get the parent directory of start dir, and run commands with git. \n9. Variables defined in `main`: 'args', 'bool_params', and 'cleaned_value'. These variables are used to handle command-line arguments, define boolean parameters for interactive mode, and clean up input values. \n10. Calls made in `process_single_python_file` include: logging.info, os.path.dirname, os.path.relpath, Path, .join, get_model, logging.error, get_python_datasets, save_python_data. \n11. Calls made in `py2dataset` include: logging.getLogger().setLevel, logging.getLogger, sys.setrecursionlimit, get_start_dir, get_output_dir, get_questions, Path(start_dir).rglob, Process, proc.start, and proc.join. \n12. Calls made in `clone_github_repo` include: shlex.quote, subprocess.run, url.split, os.path.join, os.getcwd, git.Repo.clone_from, repo.git.fetch, and repo.git.reset \n13. Inputs to the class 'PathArgument': \"Path name input in quotations. Default values for optional parameters\"\n14. Returns from `py2dataset` are: The combined datasets using `combine_json_files(output_dir, html)` function. \n15. Returns from `clone_github_repo` are: The path to the cloned repository. \n16. Returns from `main` are: True if input_str is 't' or 'true', False if input_str is 'f' or 'false', and retains current value for any other inputs. \n{\n    \"Code Elements\": {\n        \"Classes defined\": \"PathArgument\",\n        \"Dependencies\": \"pathlib, os, get_params, shlex, save_output, argparse, sys, get_python_file_details, logging, git, typing, subprocess, multiprocessing, get_python_datasets\",\n        \"Functions defined\": \"process_single_python_file, py2dataset, clone_github_repo, main, get_bool_from_input\",\n        \"`PathArgument` Inheritance\": \"argparse.Action\",\n        \"`PathArgument` Methods\": \"__call__\",\n        \"`PathArgument` Variables\": \"cleaned_value\",\n        \"`__call__` Calls\": \"values.strip(\\\\'\\\"\\\\').strip, values.strip, setattr\",\n        \"`__call__` Inputs\": \"self, parser, namespace, values, option_string\",\n        \"`clone_github_repo` Calls\": \"shlex.quote, subprocess.run, print, url.split, os.path.join, os.getcwd, os.makedirs, os.path.exists, git.Repo.clone_from, git.Repo, repo.git.custom_environment, repo.git.fetch, repo.head.reference.tracking_branch, repo.git.reset\",\n        \"`clone_github_repo` Inputs\": \"url\",\n        \"`clone_github_repo` Returns\": \"path\",\n        \"`clone_github_repo` Variables\": \"repo_name, path, default_branch, githubrepos_dir, command, repo\",\n        \"`get_bool_from_input` Calls\": \"input_str.lower\",\n        \"`get_bool_from_input` Inputs\": \"input_str, current_value\",\n        \"`get_bool_from_input` Returns\": \"True if input_str.lower() in ['t, true'] else False if input_str.lower() in ['f, false'] else current_value\",\n        \"`main` Calls\": \"values.strip(\\\\'\\\"\\\\').strip, values.strip, setattr, input_str.lower, argparse.ArgumentParser, parser.add_argument, parser.parse_args, print, input, getattr, get_bool_from_input, os.path.isdir, args.start.startswith, os.getcwd, clone_github_repo, py2dataset\",\n        \"`main` Returns\": \"True if input_str.lower() in ['t, true'] else False if input_str.lower() in ['f, false'] else current_value\",\n        \"`main` Variables\": \"args, bool_params, cleaned_value, pathname_params, parser\",\n        \"`process_single_python_file` Calls\": \"logging.info, os.path.dirname, os.path.relpath, Path, .'.join, get_model, get_python_file_details, logging.error, get_python_datasets, save_python_data\",\n        \"`process_single_python_file` Inputs\": \"python_file_path, start_dir, model_config_path, questions_dict, use_llm, output_dir, model_config, single_process_mode, detailed_analysis\",\n        \"`process_single_python_file` Variables\": \"relative_path, file_details, base_name, parent_dir, model_config, instruct_data\",\n        \"`py2dataset` Calls\": \"logging.getLogger().setLevel, logging.getLogger, sys.setrecursionlimit, get_start_dir, get_output_dir, get_questions, get_model, Path(start_dir).rglob, Path, process_single_python_file, Process, proc.start, proc.join, combine_json_files\",\n        \"`py2dataset` Inputs\": \"start_dir, output_dir, questions_pathname, model_config_pathname, use_llm, quiet, single_process, detailed, html\",\n        \"`py2dataset` Returns\": \"combine_json_files(output_dir, html)\",\n        \"`py2dataset` Variables\": \"single_process, model_config, output_dir, start_dir, questions_dict, proc\"\n    }\n}"
    }
]