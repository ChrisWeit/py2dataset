file_info:
  file_code: "\"\"\"\nGenerates JSON format question-answer pairs and instructions\
    \ for a Python file\nRequirements:\n[req01] The `DatasetGenerator` class shall:\n\
    \        a. Accept a Python file path (str), file details (Dict), base name (str),\
    \ list of questions (List[Dict]), and model configuration (Dict) as input during\
    \ instantiation.\n        b. Initialize and store the Python file path, file details,\
    \ base name, list of questions, language model, and use_llm flag as class attributes.\n\
    \        c. Provide the `clean_and_get_unique_elements` method to clean an input\
    \ string (str) and return a string of unique elements.\n        d. Provide the\
    \ `get_response_from_llm` method to retrieve a response from the language model\
    \ based on a query and context.\n        e. Provide the `process_question` method\
    \ to process a question based on its type and generate a corresponding response\
    \ to add to the instruct_list.\n        f. Provide the `process_question_type`\
    \ method to process questions related to a file, function, class, or method.\n\
    \        g. Provide the `generate` method to generate responses for all questions\
    \ in the list and return the instruct_list.\n        h. Internally manage a question\
    \ mapping to correlate question types to keys in file details.\n\n[req02] The\
    \ `get_python_datasets` function shall:\n        a. Accept a Python file path\
    \ (str), file details (Dict), base name (str), list of questions (List[Dict]),\
    \ and model configuration (Dict) as input.\n        b. Instantiate an object of\
    \ the `DatasetGenerator` class using the provided input.\n        c. Generate\
    \ question-answer pairs and instructions using the `generate` method of the `DatasetGenerator`\
    \ instance.\n        d. Return the generated `instruct_list`.\n\"\"\"\nimport\
    \ logging\nimport re\nimport math\nfrom typing import Dict, List, Tuple\n\n# Set\
    \ up logging\nlogging.basicConfig(\n    format='%(asctime)s - %(levelname)s -\
    \ %(message)s', \n    level=logging.INFO\n)\nlogger = logging.getLogger(__name__)\n\
    \n\nclass DatasetGenerator:\n    \"\"\"\n    Generate JSON formatted dictionary\
    \ outputs for a Python file.\n    Attributes:\n        file_path (str): The path\
    \ to the Python file.\n        file_details (Dict[str, Any]): Details of the Python\
    \ file.\n        base_name (str): The base name of the Python file.\n        questions\
    \ (List[Dict[str, str]]): Questions for generating responses.\n        instruct_list\
    \ (List[Dict[str, str]]): Storage for generated instructions.\n        question_mapping\
    \ (Dict[str, str]): Mapping of question types to keys in file details.\n     \
    \   use_llm (bool): Flag indicating if a language model should be used.\n    \
    \    llm (object): The language model for generating responses.\n        prompt\
    \ (str): The prompt format for querying the language model.\n    Methods:\n  \
    \      clean_and_get_unique_elements(input_str: str) -> str: \n            Clean\
    \ and return unique elements from an input string.\n        add_to_list(list_to_update:\
    \ List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:\
    \ \n            Add response to the instruct list.\n        get_response_from_llm(query:\
    \ str, context: str) -> str:\n            Get language model response to query\
    \ for given context.\n        process_question(question_type: str, question_id:\
    \ str, query: str, context: str, info: Dict) -> None:\n            Process question\
    \ and add generated response to the instruct_list.\n        process_question_type(question_type:\
    \ str, question_id: str, question_text: str) -> None:\n            Process question\
    \ related to file, function, class, or method.\n        generate() -> Tuple[List[Dict],\
    \ List[Dict]]:\n            Generate responses for all the questions and return\
    \ the instruct_list.\n    \"\"\"\n    def __init__(self, file_path: str, file_details:\
    \ Dict, base_name: str, questions: List[Dict], model_config: Dict) -> None:\n\
    \        self.file_path = file_path\n        self.file_details = file_details\n\
    \        self.base_name = base_name\n        self.questions = questions\n    \
    \    self.model_config = model_config\n        self.llm = model_config['model']\n\
    \        if self.llm is None:\n            self.use_llm = False\n        else:\n\
    \            self.use_llm = True\n        self.instruct_list = []\n        self.question_mapping\
    \ = {\n            'file': 'file',\n            'function': 'functions',\n   \
    \         'class': 'classes',\n            'method': 'classes'\n        }\n\n\
    \    @staticmethod\n    def clean_and_get_unique_elements(input_str: str) -> str:\n\
    \        \"\"\"\n        Clean input string and return string of unique elements.\n\
    \        Args:\n            input_str (str): The input string to be cleaned.\n\
    \        Returns:\n            str: The cleaned string.\n        \"\"\"\n    \
    \    cleaned_elements = set(re.sub(r'[^\\w\\-_>\\s:/.]', '', element.strip())\n\
    \                               for element in re.sub(r'\\s+', ' ', input_str).split(','))\n\
    \        return ', '.join(cleaned_elements)\n\n    def get_response_from_llm(self,\
    \ query: str, context: str) -> str:\n        \"\"\"\n        Get language model\
    \ response to query for given context.\n        Args:\n            query (str):\
    \ The query to be used for generating the response.\n            context (str):\
    \ The context to be used for generating the response.\n        Returns:\n    \
    \        str: The generated response.\n        \"\"\"\n\n        def get_context_and_prompt(query,\
    \ context, code_qa):\n            full_context = f\"{context}\\nCODE Q and A:\\\
    n{code_qa}\"\n            prompt = self.model_config['prompt_template'].format(context=full_context,\
    \ query=query)\n            context_size = len(self.llm.tokenize(prompt))\n  \
    \          return full_context, prompt, context_size\n\n        max_context_length\
    \ = self.model_config['inference_model']['model_params']['context_length']\n \
    \       excluded_instructions = [\"Call code graph\", \"Docstring\"]\n       \
    \ code_qa = \"\\n\".join([f\"Q: {item['instruction']} \\nA: {item['output']}\"\
    \ for item in self.instruct_list if not any(item['instruction'].startswith(prefix)\
    \ for prefix in excluded_instructions)])\n\n        # manage context length for\
    \ LLM using different strategies starting with the longest and most comprehensive\n\
    \        context_strategies = [\n            lambda: '```python\\n' + str(context)\
    \ + '\\n```',\n            lambda: '```python\\n' + str(self.file_details['file_info']['file_code_simplified'])\
    \ + '\\n```',\n            lambda: self.get_string_from_info(self.file_details['file_info'],\
    \ 'file_summary'),\n            lambda: ''\n        ]\n        for strategy in\
    \ context_strategies:\n            context = strategy()\n            full_context,\
    \ prompt, context_size = get_context_and_prompt(query, context, code_qa)\n   \
    \         if context_size <= 0.70 * max_context_length:\n                break\n\
    \        else:\n            logger.error(f'Failed to generate model response,\
    \ adjust context_length > {math.ceil(context_size/0.70)} in py2dataset_model_config.yaml')\n\
    \            return ''\n\n        try:\n            response = re.sub(r'\\n\\\
    s*\\n', '\\n\\n', self.llm(prompt))\n            logging.info(f'Response: {response}')\n\
    \        except:\n            logger.error('Failed to generate model response')\n\
    \            response = ''\n        return response\n\n    def process_question(self,\
    \ question_type: str, question_id: str, query: str, context: str, info: Dict)\
    \ -> None:\n        \"\"\"\n        Process question and add the generated response\
    \ to the instruct_list.\n        Args:\n            question_type (str): The type\
    \ of question to be processed.\n            question_id (str): The ID of the question\
    \ to be processed.\n            query (str): The query to be processed.\n    \
    \        context (str): The context to be used for generating the response.\n\
    \            info (Dict): The information of the Python file.\n        Returns:\n\
    \            None\n        \"\"\"\n        if question_id.endswith('code_graph')\
    \ or question_id.endswith('docstring'):\n            response = info.get(question_id,\
    \ {})\n        else:\n            response = self.get_response_from_llm(query,\
    \ context) if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id,\
    \ '')))\n        if question_type == 'file':\n            context = self.file_details['file_info']['file_code']\n\
    \        if response and response != 'None':\n            response_str = str(response).strip()\n\
    \            if response_str:\n                self.instruct_list.append({'instruction':\
    \ query, 'input': context, 'output': response_str})\n\n    @staticmethod\n   \
    \ def get_string_from_info(info, item_type):\n        if info[item_type]:\n  \
    \          items = [item.strip() for item in str(info[item_type]).split(',') if\
    \ item]\n            return ', '.join(items)\n        return ''\n\n    def process_question_type(self,\
    \ question_type: str, question_id: str, question_text: str) -> None:\n       \
    \ \"\"\"\n        Process questions related to a file, function, class, or method.\n\
    \        Args:\n            question_type (str): The type of question to be processed.\n\
    \            question_id (str): The ID of the question to be processed.\n    \
    \        question_text (str): The text of the question to be processed.\n    \
    \    Returns:\n            None\n        \"\"\"\n        if question_type == 'file':\n\
    \            query = question_text.format(filename=self.base_name)\n         \
    \   info = self.file_details['file_info']\n            context = self.file_details['file_info']['file_code']\n\
    \            self.process_question(question_type, question_id, query, context,\
    \ info)\n        elif question_type == 'method':  \n            for class_name,\
    \ class_info in self.file_details['classes'].items():\n                for key,\
    \ method_info in class_info.items():\n                    if key.startswith('class_method_'):\n\
    \                        method_name = key[len('class_method_'):]\n          \
    \              context = method_info['method_code']\n                        mapping\
    \ = {'class_name': class_name, 'method_name': method_name}\n                 \
    \       query = question_text.format(filename=self.base_name, **mapping)\n   \
    \                     self.process_question(question_type, question_id, query,\
    \ context, method_info)\n        else:  # if question_type == 'function' or question_type\
    \ == 'class'\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n\
    \                context = info[f'{question_type}_code']\n                mapping\
    \ = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_purpose'\
    \ and self.use_llm:\n                    variables_string = self.get_string_from_info(info,\
    \ f'{question_type}_variables')\n                    inputs_string = self.get_string_from_info(info,\
    \ f'{question_type}_inputs')\n                    combined_string = ', '.join([s\
    \ for s in [variables_string, inputs_string] if s])\n                    mapping[f'{question_type}_variables']\
    \ = self.clean_and_get_unique_elements(combined_string)\n                    #\
    \ get methods to include in mapping for query\n                    if question_type\
    \ == 'class':\n                        methods_string = self.get_string_from_info(info,\
    \ f'{question_type}_methods')\n                        mapping[f'{question_type}_methods']\
    \ = methods_string\n\n                query = question_text.format(filename=self.base_name,\
    \ **mapping)\n                self.process_question(question_type, question_id,\
    \ query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n\
    \n        \"\"\"\n        Generate responses for all the questions and returns\
    \ the instruct_list.\n        Args:\n            None\n        Returns:\n    \
    \        Tuple[List[Dict], List[Dict]]: The generated question-answer pairs and\
    \ instructions.\n        \"\"\"\n        for question in self.questions:\n   \
    \         self.process_question_type(question['type'], question['id'], question['text'])\n\
    \        return self.instruct_list\n\n\ndef get_python_datasets(file_path: str,\
    \ file_details: Dict, base_name: str, questions: List[Dict], \n              \
    \          model_config: Dict) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\"\n\
    \    Extract information from a Python file and return it in JSON format.\n  \
    \  Args:\n        file_path (str): The path to the Python file.\n        file_details\
    \ (Dict): The details of the file.\n        base_name (str): The base Python code\
    \ filename.\n        questions (List[Dict]): The list of questions.\n        llm\
    \ (object): The language model to be used for generating responses.\n        prompt\
    \ (str): The prompt to be used for generating responses.\n    Returns:\n     \
    \   Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n   \
    \ \"\"\"\n    generator = DatasetGenerator(file_path, file_details, base_name,\
    \ questions, model_config)\n    return generator.generate()\n"
  file_ast: 'Module(body=[Expr(value=Constant(value=''\nGenerates JSON format question-answer
    pairs and instructions for a Python file\nRequirements:\n[req01] The `DatasetGenerator`
    class shall:\n        a. Accept a Python file path (str), file details (Dict),
    base name (str), list of questions (List[Dict]), and model configuration (Dict)
    as input during instantiation.\n        b. Initialize and store the Python file
    path, file details, base name, list of questions, language model, and use_llm
    flag as class attributes.\n        c. Provide the `clean_and_get_unique_elements`
    method to clean an input string (str) and return a string of unique elements.\n        d.
    Provide the `get_response_from_llm` method to retrieve a response from the language
    model based on a query and context.\n        e. Provide the `process_question`
    method to process a question based on its type and generate a corresponding response
    to add to the instruct_list.\n        f. Provide the `process_question_type` method
    to process questions related to a file, function, class, or method.\n        g.
    Provide the `generate` method to generate responses for all questions in the list
    and return the instruct_list.\n        h. Internally manage a question mapping
    to correlate question types to keys in file details.\n\n[req02] The `get_python_datasets`
    function shall:\n        a. Accept a Python file path (str), file details (Dict),
    base name (str), list of questions (List[Dict]), and model configuration (Dict)
    as input.\n        b. Instantiate an object of the `DatasetGenerator` class using
    the provided input.\n        c. Generate question-answer pairs and instructions
    using the `generate` method of the `DatasetGenerator` instance.\n        d. Return
    the generated `instruct_list`.\n'')), Import(names=[alias(name=''logging'')]),
    Import(names=[alias(name=''re'')]), Import(names=[alias(name=''math'')]), ImportFrom(module=''typing'',
    names=[alias(name=''Dict''), alias(name=''List''), alias(name=''Tuple'')], level=0),
    Expr(value=Call(func=Attribute(value=Name(id=''logging'', ctx=Load()), attr=''basicConfig'',
    ctx=Load()), args=[], keywords=[keyword(arg=''format'', value=Constant(value=''%(asctime)s
    - %(levelname)s - %(message)s'')), keyword(arg=''level'', value=Attribute(value=Name(id=''logging'',
    ctx=Load()), attr=''INFO'', ctx=Load()))])), Assign(targets=[Name(id=''logger'',
    ctx=Store())], value=Call(func=Attribute(value=Name(id=''logging'', ctx=Load()),
    attr=''getLogger'', ctx=Load()), args=[Name(id=''__name__'', ctx=Load())], keywords=[])),
    ClassDef(name=''DatasetGenerator'', bases=[], keywords=[], body=[Expr(value=Constant(value=''\n    Generate
    JSON formatted dictionary outputs for a Python file.\n    Attributes:\n        file_path
    (str): The path to the Python file.\n        file_details (Dict[str, Any]): Details
    of the Python file.\n        base_name (str): The base name of the Python file.\n        questions
    (List[Dict[str, str]]): Questions for generating responses.\n        instruct_list
    (List[Dict[str, str]]): Storage for generated instructions.\n        question_mapping
    (Dict[str, str]): Mapping of question types to keys in file details.\n        use_llm
    (bool): Flag indicating if a language model should be used.\n        llm (object):
    The language model for generating responses.\n        prompt (str): The prompt
    format for querying the language model.\n    Methods:\n        clean_and_get_unique_elements(input_str:
    str) -> str: \n            Clean and return unique elements from an input string.\n        add_to_list(list_to_update:
    List[Dict], query: str, response: str, additional_field=None) -> List[Dict]: \n            Add
    response to the instruct list.\n        get_response_from_llm(query: str, context:
    str) -> str:\n            Get language model response to query for given context.\n        process_question(question_type:
    str, question_id: str, query: str, context: str, info: Dict) -> None:\n            Process
    question and add generated response to the instruct_list.\n        process_question_type(question_type:
    str, question_id: str, question_text: str) -> None:\n            Process question
    related to file, function, class, or method.\n        generate() -> Tuple[List[Dict],
    List[Dict]]:\n            Generate responses for all the questions and return
    the instruct_list.\n    '')), FunctionDef(name=''__init__'', args=arguments(posonlyargs=[],
    args=[arg(arg=''self''), arg(arg=''file_path'', annotation=Name(id=''str'', ctx=Load())),
    arg(arg=''file_details'', annotation=Name(id=''Dict'', ctx=Load())), arg(arg=''base_name'',
    annotation=Name(id=''str'', ctx=Load())), arg(arg=''questions'', annotation=Subscript(value=Name(id=''List'',
    ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load())), arg(arg=''model_config'',
    annotation=Name(id=''Dict'', ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]),
    body=[Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''file_path'',
    ctx=Store())], value=Name(id=''file_path'', ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Store())], value=Name(id=''file_details'',
    ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'',
    ctx=Store())], value=Name(id=''base_name'', ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''questions'', ctx=Store())], value=Name(id=''questions'', ctx=Load())),
    Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''model_config'',
    ctx=Store())], value=Name(id=''model_config'', ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''llm'', ctx=Store())], value=Subscript(value=Name(id=''model_config'',
    ctx=Load()), slice=Constant(value=''model''), ctx=Load())), If(test=Compare(left=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''llm'', ctx=Load()), ops=[Is()], comparators=[Constant(value=None)]),
    body=[Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''use_llm'',
    ctx=Store())], value=Constant(value=False))], orelse=[Assign(targets=[Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''use_llm'', ctx=Store())], value=Constant(value=True))]), Assign(targets=[Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''instruct_list'', ctx=Store())], value=List(elts=[], ctx=Load())),
    Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''question_mapping'',
    ctx=Store())], value=Dict(keys=[Constant(value=''file''), Constant(value=''function''),
    Constant(value=''class''), Constant(value=''method'')], values=[Constant(value=''file''),
    Constant(value=''functions''), Constant(value=''classes''), Constant(value=''classes'')]))],
    decorator_list=[], returns=Constant(value=None)), FunctionDef(name=''clean_and_get_unique_elements'',
    args=arguments(posonlyargs=[], args=[arg(arg=''input_str'', annotation=Name(id=''str'',
    ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Clean
    input string and return string of unique elements.\n        Args:\n            input_str
    (str): The input string to be cleaned.\n        Returns:\n            str: The
    cleaned string.\n        '')), Assign(targets=[Name(id=''cleaned_elements'', ctx=Store())],
    value=Call(func=Name(id=''set'', ctx=Load()), args=[GeneratorExp(elt=Call(func=Attribute(value=Name(id=''re'',
    ctx=Load()), attr=''sub'', ctx=Load()), args=[Constant(value=''[^\\w\\-_>\\s:/.]''),
    Constant(value=''''), Call(func=Attribute(value=Name(id=''element'', ctx=Load()),
    attr=''strip'', ctx=Load()), args=[], keywords=[])], keywords=[]), generators=[comprehension(target=Name(id=''element'',
    ctx=Store()), iter=Call(func=Attribute(value=Call(func=Attribute(value=Name(id=''re'',
    ctx=Load()), attr=''sub'', ctx=Load()), args=[Constant(value=''\\s+''), Constant(value=''
    ''), Name(id=''input_str'', ctx=Load())], keywords=[]), attr=''split'', ctx=Load()),
    args=[Constant(value='','')], keywords=[]), ifs=[], is_async=0)])], keywords=[])),
    Return(value=Call(func=Attribute(value=Constant(value='', ''), attr=''join'',
    ctx=Load()), args=[Name(id=''cleaned_elements'', ctx=Load())], keywords=[]))],
    decorator_list=[Name(id=''staticmethod'', ctx=Load())], returns=Name(id=''str'',
    ctx=Load())), FunctionDef(name=''get_response_from_llm'', args=arguments(posonlyargs=[],
    args=[arg(arg=''self''), arg(arg=''query'', annotation=Name(id=''str'', ctx=Load())),
    arg(arg=''context'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
    kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Get
    language model response to query for given context.\n        Args:\n            query
    (str): The query to be used for generating the response.\n            context
    (str): The context to be used for generating the response.\n        Returns:\n            str:
    The generated response.\n        '')), FunctionDef(name=''get_context_and_prompt'',
    args=arguments(posonlyargs=[], args=[arg(arg=''query''), arg(arg=''context''),
    arg(arg=''code_qa'')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id=''full_context'',
    ctx=Store())], value=JoinedStr(values=[FormattedValue(value=Name(id=''context'',
    ctx=Load()), conversion=-1), Constant(value=''\nCODE Q and A:\n''), FormattedValue(value=Name(id=''code_qa'',
    ctx=Load()), conversion=-1)])), Assign(targets=[Name(id=''prompt'', ctx=Store())],
    value=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''model_config'', ctx=Load()), slice=Constant(value=''prompt_template''),
    ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''context'',
    value=Name(id=''full_context'', ctx=Load())), keyword(arg=''query'', value=Name(id=''query'',
    ctx=Load()))])), Assign(targets=[Name(id=''context_size'', ctx=Store())], value=Call(func=Name(id=''len'',
    ctx=Load()), args=[Call(func=Attribute(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''llm'', ctx=Load()), attr=''tokenize'', ctx=Load()), args=[Name(id=''prompt'',
    ctx=Load())], keywords=[])], keywords=[])), Return(value=Tuple(elts=[Name(id=''full_context'',
    ctx=Load()), Name(id=''prompt'', ctx=Load()), Name(id=''context_size'', ctx=Load())],
    ctx=Load()))], decorator_list=[]), Assign(targets=[Name(id=''max_context_length'',
    ctx=Store())], value=Subscript(value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''model_config'', ctx=Load()), slice=Constant(value=''inference_model''),
    ctx=Load()), slice=Constant(value=''model_params''), ctx=Load()), slice=Constant(value=''context_length''),
    ctx=Load())), Assign(targets=[Name(id=''excluded_instructions'', ctx=Store())],
    value=List(elts=[Constant(value=''Call code graph''), Constant(value=''Docstring'')],
    ctx=Load())), Assign(targets=[Name(id=''code_qa'', ctx=Store())], value=Call(func=Attribute(value=Constant(value=''\n''),
    attr=''join'', ctx=Load()), args=[ListComp(elt=JoinedStr(values=[Constant(value=''Q:
    ''), FormattedValue(value=Subscript(value=Name(id=''item'', ctx=Load()), slice=Constant(value=''instruction''),
    ctx=Load()), conversion=-1), Constant(value='' \nA: ''), FormattedValue(value=Subscript(value=Name(id=''item'',
    ctx=Load()), slice=Constant(value=''output''), ctx=Load()), conversion=-1)]),
    generators=[comprehension(target=Name(id=''item'', ctx=Store()), iter=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''instruct_list'', ctx=Load()), ifs=[UnaryOp(op=Not(), operand=Call(func=Name(id=''any'',
    ctx=Load()), args=[GeneratorExp(elt=Call(func=Attribute(value=Subscript(value=Name(id=''item'',
    ctx=Load()), slice=Constant(value=''instruction''), ctx=Load()), attr=''startswith'',
    ctx=Load()), args=[Name(id=''prefix'', ctx=Load())], keywords=[]), generators=[comprehension(target=Name(id=''prefix'',
    ctx=Store()), iter=Name(id=''excluded_instructions'', ctx=Load()), ifs=[], is_async=0)])],
    keywords=[]))], is_async=0)])], keywords=[])), Assign(targets=[Name(id=''context_strategies'',
    ctx=Store())], value=List(elts=[Lambda(args=arguments(posonlyargs=[], args=[],
    kwonlyargs=[], kw_defaults=[], defaults=[]), body=BinOp(left=BinOp(left=Constant(value=''```python\n''),
    op=Add(), right=Call(func=Name(id=''str'', ctx=Load()), args=[Name(id=''context'',
    ctx=Load())], keywords=[])), op=Add(), right=Constant(value=''\n```''))), Lambda(args=arguments(posonlyargs=[],
    args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=BinOp(left=BinOp(left=Constant(value=''```python\n''),
    op=Add(), right=Call(func=Name(id=''str'', ctx=Load()), args=[Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
    ctx=Load()), slice=Constant(value=''file_code_simplified''), ctx=Load())], keywords=[])),
    op=Add(), right=Constant(value=''\n```''))), Lambda(args=arguments(posonlyargs=[],
    args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=Call(func=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''get_string_from_info'', ctx=Load()), args=[Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
    ctx=Load()), Constant(value=''file_summary'')], keywords=[])), Lambda(args=arguments(posonlyargs=[],
    args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=Constant(value=''''))],
    ctx=Load())), For(target=Name(id=''strategy'', ctx=Store()), iter=Name(id=''context_strategies'',
    ctx=Load()), body=[Assign(targets=[Name(id=''context'', ctx=Store())], value=Call(func=Name(id=''strategy'',
    ctx=Load()), args=[], keywords=[])), Assign(targets=[Tuple(elts=[Name(id=''full_context'',
    ctx=Store()), Name(id=''prompt'', ctx=Store()), Name(id=''context_size'', ctx=Store())],
    ctx=Store())], value=Call(func=Name(id=''get_context_and_prompt'', ctx=Load()),
    args=[Name(id=''query'', ctx=Load()), Name(id=''context'', ctx=Load()), Name(id=''code_qa'',
    ctx=Load())], keywords=[])), If(test=Compare(left=Name(id=''context_size'', ctx=Load()),
    ops=[LtE()], comparators=[BinOp(left=Constant(value=0.7), op=Mult(), right=Name(id=''max_context_length'',
    ctx=Load()))]), body=[Break()], orelse=[])], orelse=[Expr(value=Call(func=Attribute(value=Name(id=''logger'',
    ctx=Load()), attr=''error'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Failed
    to generate model response, adjust context_length > ''), FormattedValue(value=Call(func=Attribute(value=Name(id=''math'',
    ctx=Load()), attr=''ceil'', ctx=Load()), args=[BinOp(left=Name(id=''context_size'',
    ctx=Load()), op=Div(), right=Constant(value=0.7))], keywords=[]), conversion=-1),
    Constant(value='' in py2dataset_model_config.yaml'')])], keywords=[])), Return(value=Constant(value=''''))]),
    Try(body=[Assign(targets=[Name(id=''response'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''re'',
    ctx=Load()), attr=''sub'', ctx=Load()), args=[Constant(value=''\\n\\s*\\n''),
    Constant(value=''\n\n''), Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
    attr=''llm'', ctx=Load()), args=[Name(id=''prompt'', ctx=Load())], keywords=[])],
    keywords=[])), Expr(value=Call(func=Attribute(value=Name(id=''logging'', ctx=Load()),
    attr=''info'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Response:
    ''), FormattedValue(value=Name(id=''response'', ctx=Load()), conversion=-1)])],
    keywords=[]))], handlers=[ExceptHandler(body=[Expr(value=Call(func=Attribute(value=Name(id=''logger'',
    ctx=Load()), attr=''error'', ctx=Load()), args=[Constant(value=''Failed to generate
    model response'')], keywords=[])), Assign(targets=[Name(id=''response'', ctx=Store())],
    value=Constant(value=''''))])], orelse=[], finalbody=[]), Return(value=Name(id=''response'',
    ctx=Load()))], decorator_list=[], returns=Name(id=''str'', ctx=Load())), FunctionDef(name=''process_question'',
    args=arguments(posonlyargs=[], args=[arg(arg=''self''), arg(arg=''question_type'',
    annotation=Name(id=''str'', ctx=Load())), arg(arg=''question_id'', annotation=Name(id=''str'',
    ctx=Load())), arg(arg=''query'', annotation=Name(id=''str'', ctx=Load())), arg(arg=''context'',
    annotation=Name(id=''str'', ctx=Load())), arg(arg=''info'', annotation=Name(id=''Dict'',
    ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Process
    question and add the generated response to the instruct_list.\n        Args:\n            question_type
    (str): The type of question to be processed.\n            question_id (str): The
    ID of the question to be processed.\n            query (str): The query to be
    processed.\n            context (str): The context to be used for generating the
    response.\n            info (Dict): The information of the Python file.\n        Returns:\n            None\n        '')),
    If(test=BoolOp(op=Or(), values=[Call(func=Attribute(value=Name(id=''question_id'',
    ctx=Load()), attr=''endswith'', ctx=Load()), args=[Constant(value=''code_graph'')],
    keywords=[]), Call(func=Attribute(value=Name(id=''question_id'', ctx=Load()),
    attr=''endswith'', ctx=Load()), args=[Constant(value=''docstring'')], keywords=[])]),
    body=[Assign(targets=[Name(id=''response'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''info'',
    ctx=Load()), attr=''get'', ctx=Load()), args=[Name(id=''question_id'', ctx=Load()),
    Dict(keys=[], values=[])], keywords=[]))], orelse=[Assign(targets=[Name(id=''response'',
    ctx=Store())], value=IfExp(test=BoolOp(op=And(), values=[Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''use_llm'', ctx=Load()), Call(func=Attribute(value=Name(id=''question_id'',
    ctx=Load()), attr=''endswith'', ctx=Load()), args=[Constant(value=''purpose'')],
    keywords=[])]), body=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
    attr=''get_response_from_llm'', ctx=Load()), args=[Name(id=''query'', ctx=Load()),
    Name(id=''context'', ctx=Load())], keywords=[]), orelse=Call(func=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''clean_and_get_unique_elements'', ctx=Load()), args=[Call(func=Name(id=''str'',
    ctx=Load()), args=[Call(func=Attribute(value=Name(id=''info'', ctx=Load()), attr=''get'',
    ctx=Load()), args=[Name(id=''question_id'', ctx=Load()), Constant(value='''')],
    keywords=[])], keywords=[])], keywords=[])))]), If(test=Compare(left=Name(id=''question_type'',
    ctx=Load()), ops=[Eq()], comparators=[Constant(value=''file'')]), body=[Assign(targets=[Name(id=''context'',
    ctx=Store())], value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
    ctx=Load()), slice=Constant(value=''file_code''), ctx=Load()))], orelse=[]), If(test=BoolOp(op=And(),
    values=[Name(id=''response'', ctx=Load()), Compare(left=Name(id=''response'',
    ctx=Load()), ops=[NotEq()], comparators=[Constant(value=''None'')])]), body=[Assign(targets=[Name(id=''response_str'',
    ctx=Store())], value=Call(func=Attribute(value=Call(func=Name(id=''str'', ctx=Load()),
    args=[Name(id=''response'', ctx=Load())], keywords=[]), attr=''strip'', ctx=Load()),
    args=[], keywords=[])), If(test=Name(id=''response_str'', ctx=Load()), body=[Expr(value=Call(func=Attribute(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''instruct_list'', ctx=Load()), attr=''append'', ctx=Load()),
    args=[Dict(keys=[Constant(value=''instruction''), Constant(value=''input''), Constant(value=''output'')],
    values=[Name(id=''query'', ctx=Load()), Name(id=''context'', ctx=Load()), Name(id=''response_str'',
    ctx=Load())])], keywords=[]))], orelse=[])], orelse=[])], decorator_list=[], returns=Constant(value=None)),
    FunctionDef(name=''get_string_from_info'', args=arguments(posonlyargs=[], args=[arg(arg=''info''),
    arg(arg=''item_type'')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[If(test=Subscript(value=Name(id=''info'',
    ctx=Load()), slice=Name(id=''item_type'', ctx=Load()), ctx=Load()), body=[Assign(targets=[Name(id=''items'',
    ctx=Store())], value=ListComp(elt=Call(func=Attribute(value=Name(id=''item'',
    ctx=Load()), attr=''strip'', ctx=Load()), args=[], keywords=[]), generators=[comprehension(target=Name(id=''item'',
    ctx=Store()), iter=Call(func=Attribute(value=Call(func=Name(id=''str'', ctx=Load()),
    args=[Subscript(value=Name(id=''info'', ctx=Load()), slice=Name(id=''item_type'',
    ctx=Load()), ctx=Load())], keywords=[]), attr=''split'', ctx=Load()), args=[Constant(value='','')],
    keywords=[]), ifs=[Name(id=''item'', ctx=Load())], is_async=0)])), Return(value=Call(func=Attribute(value=Constant(value='',
    ''), attr=''join'', ctx=Load()), args=[Name(id=''items'', ctx=Load())], keywords=[]))],
    orelse=[]), Return(value=Constant(value=''''))], decorator_list=[Name(id=''staticmethod'',
    ctx=Load())]), FunctionDef(name=''process_question_type'', args=arguments(posonlyargs=[],
    args=[arg(arg=''self''), arg(arg=''question_type'', annotation=Name(id=''str'',
    ctx=Load())), arg(arg=''question_id'', annotation=Name(id=''str'', ctx=Load())),
    arg(arg=''question_text'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
    kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Process
    questions related to a file, function, class, or method.\n        Args:\n            question_type
    (str): The type of question to be processed.\n            question_id (str): The
    ID of the question to be processed.\n            question_text (str): The text
    of the question to be processed.\n        Returns:\n            None\n        '')),
    If(test=Compare(left=Name(id=''question_type'', ctx=Load()), ops=[Eq()], comparators=[Constant(value=''file'')]),
    body=[Assign(targets=[Name(id=''query'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''question_text'',
    ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''filename'',
    value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'', ctx=Load()))])),
    Assign(targets=[Name(id=''info'', ctx=Store())], value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
    ctx=Load())), Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
    ctx=Load()), slice=Constant(value=''file_code''), ctx=Load())), Expr(value=Call(func=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'',
    ctx=Load()), Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()),
    Name(id=''context'', ctx=Load()), Name(id=''info'', ctx=Load())], keywords=[]))],
    orelse=[If(test=Compare(left=Name(id=''question_type'', ctx=Load()), ops=[Eq()],
    comparators=[Constant(value=''method'')]), body=[For(target=Tuple(elts=[Name(id=''class_name'',
    ctx=Store()), Name(id=''class_info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''classes''),
    ctx=Load()), attr=''items'', ctx=Load()), args=[], keywords=[]), body=[For(target=Tuple(elts=[Name(id=''key'',
    ctx=Store()), Name(id=''method_info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Name(id=''class_info'',
    ctx=Load()), attr=''items'', ctx=Load()), args=[], keywords=[]), body=[If(test=Call(func=Attribute(value=Name(id=''key'',
    ctx=Load()), attr=''startswith'', ctx=Load()), args=[Constant(value=''class_method_'')],
    keywords=[]), body=[Assign(targets=[Name(id=''method_name'', ctx=Store())], value=Subscript(value=Name(id=''key'',
    ctx=Load()), slice=Slice(lower=Call(func=Name(id=''len'', ctx=Load()), args=[Constant(value=''class_method_'')],
    keywords=[])), ctx=Load())), Assign(targets=[Name(id=''context'', ctx=Store())],
    value=Subscript(value=Name(id=''method_info'', ctx=Load()), slice=Constant(value=''method_code''),
    ctx=Load())), Assign(targets=[Name(id=''mapping'', ctx=Store())], value=Dict(keys=[Constant(value=''class_name''),
    Constant(value=''method_name'')], values=[Name(id=''class_name'', ctx=Load()),
    Name(id=''method_name'', ctx=Load())])), Assign(targets=[Name(id=''query'', ctx=Store())],
    value=Call(func=Attribute(value=Name(id=''question_text'', ctx=Load()), attr=''format'',
    ctx=Load()), args=[], keywords=[keyword(arg=''filename'', value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''base_name'', ctx=Load())), keyword(value=Name(id=''mapping'',
    ctx=Load()))])), Expr(value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
    attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'', ctx=Load()),
    Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()), Name(id=''context'',
    ctx=Load()), Name(id=''method_info'', ctx=Load())], keywords=[]))], orelse=[])],
    orelse=[])], orelse=[])], orelse=[For(target=Tuple(elts=[Name(id=''name'', ctx=Store()),
    Name(id=''info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Load()), slice=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''question_mapping'', ctx=Load()), slice=Name(id=''question_type'',
    ctx=Load()), ctx=Load()), ctx=Load()), attr=''items'', ctx=Load()), args=[], keywords=[]),
    body=[Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Name(id=''info'',
    ctx=Load()), slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
    ctx=Load()), conversion=-1), Constant(value=''_code'')]), ctx=Load())), Assign(targets=[Name(id=''mapping'',
    ctx=Store())], value=Dict(keys=[JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
    ctx=Load()), conversion=-1), Constant(value=''_name'')])], values=[Name(id=''name'',
    ctx=Load())])), If(test=BoolOp(op=And(), values=[Compare(left=Name(id=''question_id'',
    ctx=Load()), ops=[Eq()], comparators=[JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
    ctx=Load()), conversion=-1), Constant(value=''_purpose'')])]), Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''use_llm'', ctx=Load())]), body=[Assign(targets=[Name(id=''variables_string'',
    ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
    attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
    JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
    conversion=-1), Constant(value=''_variables'')])], keywords=[])), Assign(targets=[Name(id=''inputs_string'',
    ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
    attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
    JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
    conversion=-1), Constant(value=''_inputs'')])], keywords=[])), Assign(targets=[Name(id=''combined_string'',
    ctx=Store())], value=Call(func=Attribute(value=Constant(value='', ''), attr=''join'',
    ctx=Load()), args=[ListComp(elt=Name(id=''s'', ctx=Load()), generators=[comprehension(target=Name(id=''s'',
    ctx=Store()), iter=List(elts=[Name(id=''variables_string'', ctx=Load()), Name(id=''inputs_string'',
    ctx=Load())], ctx=Load()), ifs=[Name(id=''s'', ctx=Load())], is_async=0)])], keywords=[])),
    Assign(targets=[Subscript(value=Name(id=''mapping'', ctx=Load()), slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
    ctx=Load()), conversion=-1), Constant(value=''_variables'')]), ctx=Store())],
    value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()), attr=''clean_and_get_unique_elements'',
    ctx=Load()), args=[Name(id=''combined_string'', ctx=Load())], keywords=[])), If(test=Compare(left=Name(id=''question_type'',
    ctx=Load()), ops=[Eq()], comparators=[Constant(value=''class'')]), body=[Assign(targets=[Name(id=''methods_string'',
    ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
    attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
    JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
    conversion=-1), Constant(value=''_methods'')])], keywords=[])), Assign(targets=[Subscript(value=Name(id=''mapping'',
    ctx=Load()), slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
    ctx=Load()), conversion=-1), Constant(value=''_methods'')]), ctx=Store())], value=Name(id=''methods_string'',
    ctx=Load()))], orelse=[])], orelse=[]), Assign(targets=[Name(id=''query'', ctx=Store())],
    value=Call(func=Attribute(value=Name(id=''question_text'', ctx=Load()), attr=''format'',
    ctx=Load()), args=[], keywords=[keyword(arg=''filename'', value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''base_name'', ctx=Load())), keyword(value=Name(id=''mapping'',
    ctx=Load()))])), Expr(value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
    attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'', ctx=Load()),
    Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()), Name(id=''context'',
    ctx=Load()), Name(id=''info'', ctx=Load())], keywords=[]))], orelse=[])])])],
    decorator_list=[], returns=Constant(value=None)), FunctionDef(name=''generate'',
    args=arguments(posonlyargs=[], args=[arg(arg=''self'')], kwonlyargs=[], kw_defaults=[],
    defaults=[]), body=[Expr(value=Constant(value=''\n        Generate responses for
    all the questions and returns the instruct_list.\n        Args:\n            None\n        Returns:\n            Tuple[List[Dict],
    List[Dict]]: The generated question-answer pairs and instructions.\n        '')),
    For(target=Name(id=''question'', ctx=Store()), iter=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''questions'', ctx=Load()), body=[Expr(value=Call(func=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''process_question_type'', ctx=Load()), args=[Subscript(value=Name(id=''question'',
    ctx=Load()), slice=Constant(value=''type''), ctx=Load()), Subscript(value=Name(id=''question'',
    ctx=Load()), slice=Constant(value=''id''), ctx=Load()), Subscript(value=Name(id=''question'',
    ctx=Load()), slice=Constant(value=''text''), ctx=Load())], keywords=[]))], orelse=[]),
    Return(value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''instruct_list'',
    ctx=Load()))], decorator_list=[], returns=Subscript(value=Name(id=''Tuple'', ctx=Load()),
    slice=Tuple(elts=[Subscript(value=Name(id=''List'', ctx=Load()), slice=Name(id=''Dict'',
    ctx=Load()), ctx=Load()), Subscript(value=Name(id=''List'', ctx=Load()), slice=Name(id=''Dict'',
    ctx=Load()), ctx=Load())], ctx=Load()), ctx=Load()))], decorator_list=[]), FunctionDef(name=''get_python_datasets'',
    args=arguments(posonlyargs=[], args=[arg(arg=''file_path'', annotation=Name(id=''str'',
    ctx=Load())), arg(arg=''file_details'', annotation=Name(id=''Dict'', ctx=Load())),
    arg(arg=''base_name'', annotation=Name(id=''str'', ctx=Load())), arg(arg=''questions'',
    annotation=Subscript(value=Name(id=''List'', ctx=Load()), slice=Name(id=''Dict'',
    ctx=Load()), ctx=Load())), arg(arg=''model_config'', annotation=Name(id=''Dict'',
    ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n    Extract
    information from a Python file and return it in JSON format.\n    Args:\n        file_path
    (str): The path to the Python file.\n        file_details (Dict): The details
    of the file.\n        base_name (str): The base Python code filename.\n        questions
    (List[Dict]): The list of questions.\n        llm (object): The language model
    to be used for generating responses.\n        prompt (str): The prompt to be used
    for generating responses.\n    Returns:\n        Tuple[List[Dict], List[Dict]]:
    Extracted information in JSON format.\n    '')), Assign(targets=[Name(id=''generator'',
    ctx=Store())], value=Call(func=Name(id=''DatasetGenerator'', ctx=Load()), args=[Name(id=''file_path'',
    ctx=Load()), Name(id=''file_details'', ctx=Load()), Name(id=''base_name'', ctx=Load()),
    Name(id=''questions'', ctx=Load()), Name(id=''model_config'', ctx=Load())], keywords=[])),
    Return(value=Call(func=Attribute(value=Name(id=''generator'', ctx=Load()), attr=''generate'',
    ctx=Load()), args=[], keywords=[]))], decorator_list=[], returns=Subscript(value=Name(id=''Tuple'',
    ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id=''List'', ctx=Load()),
    slice=Name(id=''Dict'', ctx=Load()), ctx=Load()), Subscript(value=Name(id=''List'',
    ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load())], ctx=Load()), ctx=Load()))],
    type_ignores=[])'
  file_dependencies:
  - math
  - logging
  - typing
  - re
  file_functions:
  - get_python_datasets
  file_classes:
  - DatasetGenerator
  file_summary: '{dependencies: [math, logging, typing, re], function_defs: [{get_python_datasets:
    {inputs: [file_path, file_details, base_name, questions, model_config], calls:
    [DatasetGenerator, generator.generate], call_inputs: {DatasetGenerator: [file_path,
    file_details, base_name, questions, model_config], generator.generate: []}, returns:
    [generator.generate()]}}], class_defs: [{DatasetGenerator: {method_defs: {__init__:
    {inputs: [self, file_path, file_details, base_name, questions, model_config],
    calls: [], call_inputs: {}, returns: []}, clean_and_get_unique_elements: {inputs:
    [input_str], calls: [set, re.sub, element.strip, re.sub(''\\\\s+'', '' '', input_str).split,
    '', ''.join], call_inputs: {set: [(re.sub(''[^\\\\w\\\\-_>\\\\s:/.]'', '''', element.strip())
    for element in re.sub(''\\\\s+'', '' '', input_str).split('',''))], re.sub: [''\\\\s+'',
    '' '', input_str], element.strip: [], re.sub(''\\\\s+'', '' '', input_str).split:
    ['',''], '', ''.join: [cleaned_elements]}, returns: ['', ''.join(cleaned_elements)]},
    get_response_from_llm: {inputs: [self, query, context], calls: [self.model_config[''prompt_template''].format,
    len, self.llm.tokenize, ''\\n''.join, any, item[''instruction''].startswith, str,
    self.get_string_from_info, strategy, get_context_and_prompt, logger.error, math.ceil,
    re.sub, self.llm, logging.info], call_inputs: {self.model_config[''prompt_template''].format:
    [], len: [self.llm.tokenize(prompt)], self.llm.tokenize: [prompt], ''\\n''.join:
    [[f\Q: {item[''instruction'']} \\nA: {item[''output'']}\ for item in self.instruct_list
    if not any((item[''instruction''].startswith(prefix) for prefix in excluded_instructions))]],
    any: [(item[''instruction''].startswith(prefix) for prefix in excluded_instructions)],
    item[''instruction''].startswith: [prefix], str: [self.file_details[''file_info''][''file_code_simplified'']],
    self.get_string_from_info: [self.file_details[''file_info''], ''file_summary''],
    strategy: [], get_context_and_prompt: [query, context, code_qa], logger.error:
    [''Failed to generate model response''], math.ceil: [context_size / 0.7], re.sub:
    [''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt)], self.llm: [prompt], logging.info:
    [f''Response: {response}'']}, returns: [response, (full_context, prompt, context_size),
    '''']}, get_context_and_prompt: {inputs: [query, context, code_qa], calls: [self.model_config[''prompt_template''].format,
    len, self.llm.tokenize], call_inputs: {self.model_config[''prompt_template''].format:
    [], len: [self.llm.tokenize(prompt)], self.llm.tokenize: [prompt]}, returns: [(full_context,
    prompt, context_size)]}, process_question: {inputs: [self, question_type, question_id,
    query, context, info], calls: [question_id.endswith, info.get, self.get_response_from_llm,
    self.clean_and_get_unique_elements, str, str(response).strip, self.instruct_list.append],
    call_inputs: {question_id.endswith: [''purpose''], info.get: [question_id, ''''],
    self.get_response_from_llm: [query, context], self.clean_and_get_unique_elements:
    [str(info.get(question_id, ''''))], str: [response], str(response).strip: [],
    self.instruct_list.append: [{''instruction'': query, ''input'': context, ''output'':
    response_str}]}, returns: []}, get_string_from_info: {inputs: [info, item_type],
    calls: [item.strip, str(info[item_type]).split, str, '', ''.join], call_inputs:
    {item.strip: [], str(info[item_type]).split: ['',''], str: [info[item_type]],
    '', ''.join: [items]}, returns: ['''', '', ''.join(items)]}, process_question_type:
    {inputs: [self, question_type, question_id, question_text], calls: [question_text.format,
    self.process_question, self.file_details[''classes''].items, class_info.items,
    key.startswith, len, self.file_details[self.question_mapping[question_type]].items,
    self.get_string_from_info, '', ''.join, self.clean_and_get_unique_elements], call_inputs:
    {question_text.format: [], self.process_question: [question_type, question_id,
    query, context, info], self.file_details[''classes''].items: [], class_info.items:
    [], key.startswith: [''class_method_''], len: [''class_method_''], self.file_details[self.question_mapping[question_type]].items:
    [], self.get_string_from_info: [info, f''{question_type}_methods''], '', ''.join:
    [[s for s in [variables_string, inputs_string] if s]], self.clean_and_get_unique_elements:
    [combined_string]}, returns: []}, generate: {inputs: [self], calls: [self.process_question_type],
    call_inputs: {self.process_question_type: [question[''type''], question[''id''],
    question[''text'']]}, returns: [self.instruct_list]}}}}]}'
  file_code_simplified: "\"\"\"\"\"\"\nimport logging\nimport re\nimport math\nfrom\
    \ typing import Dict, List, Tuple\nlogging.basicConfig(format='%(asctime)s - %(levelname)s\
    \ - %(message)s',\n    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\
    \n\nclass DatasetGenerator:\n    \"\"\"\"\"\"\n\n    def __init__(self, file_path:\
    \ str, file_details: Dict, base_name: str,\n        questions: List[Dict], model_config:\
    \ Dict) ->None:\n        self.file_path = file_path\n        self.file_details\
    \ = file_details\n        self.base_name = base_name\n        self.questions =\
    \ questions\n        self.model_config = model_config\n        self.llm = model_config['model']\n\
    \        if self.llm is None:\n            self.use_llm = False\n        else:\n\
    \            self.use_llm = True\n        self.instruct_list = []\n        self.question_mapping\
    \ = {'file': 'file', 'function': 'functions',\n            'class': 'classes',\
    \ 'method': 'classes'}\n\n    @staticmethod\n    def clean_and_get_unique_elements(input_str:\
    \ str) ->str:\n        \"\"\"\"\"\"\n        cleaned_elements = set(re.sub('[^\\\
    \\w\\\\-_>\\\\s:/.]', '', element.\n            strip()) for element in re.sub('\\\
    \\s+', ' ', input_str).split(','))\n        return ', '.join(cleaned_elements)\n\
    \n    def get_response_from_llm(self, query: str, context: str) ->str:\n     \
    \   \"\"\"\"\"\"\n\n        def get_context_and_prompt(query, context, code_qa):\n\
    \            full_context = f'{context}\\nCODE Q and A:\\n{code_qa}'\n       \
    \     prompt = self.model_config['prompt_template'].format(context=\n        \
    \        full_context, query=query)\n            context_size = len(self.llm.tokenize(prompt))\n\
    \            return full_context, prompt, context_size\n        max_context_length\
    \ = self.model_config['inference_model'][\n            'model_params']['context_length']\n\
    \        excluded_instructions = ['Call code graph', 'Docstring']\n        code_qa\
    \ = '\\n'.join([\n            f\"Q: {item['instruction']} \\nA: {item['output']}\"\
    \ for item in\n            self.instruct_list if not any(item['instruction'].startswith(\n\
    \            prefix) for prefix in excluded_instructions)])\n        context_strategies\
    \ = [lambda : '```python\\n' + str(context) +\n            '\\n```', lambda :\
    \ '```python\\n' + str(self.file_details[\n            'file_info']['file_code_simplified'])\
    \ + '\\n```', lambda : self.\n            get_string_from_info(self.file_details['file_info'],\n\
    \            'file_summary'), lambda : '']\n        for strategy in context_strategies:\n\
    \            context = strategy()\n            full_context, prompt, context_size\
    \ = get_context_and_prompt(query,\n                context, code_qa)\n       \
    \     if context_size <= 0.7 * max_context_length:\n                break\n  \
    \      else:\n            logger.error(\n                f'Failed to generate\
    \ model response, adjust context_length > {math.ceil(context_size / 0.7)} in py2dataset_model_config.yaml'\n\
    \                )\n            return ''\n        try:\n            response\
    \ = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n            logging.info(f'Response:\
    \ {response}')\n        except:\n            logger.error('Failed to generate\
    \ model response')\n            response = ''\n        return response\n\n   \
    \ def process_question(self, question_type: str, question_id: str, query:\n  \
    \      str, context: str, info: Dict) ->None:\n        \"\"\"\"\"\"\n        if\
    \ question_id.endswith('code_graph') or question_id.endswith(\n            'docstring'):\n\
    \            response = info.get(question_id, {})\n        else:\n           \
    \ response = self.get_response_from_llm(query, context\n                ) if self.use_llm\
    \ and question_id.endswith('purpose'\n                ) else self.clean_and_get_unique_elements(str(info.get(\n\
    \                question_id, '')))\n        if question_type == 'file':\n   \
    \         context = self.file_details['file_info']['file_code']\n        if response\
    \ and response != 'None':\n            response_str = str(response).strip()\n\
    \            if response_str:\n                self.instruct_list.append({'instruction':\
    \ query, 'input':\n                    context, 'output': response_str})\n\n \
    \   @staticmethod\n    def get_string_from_info(info, item_type):\n        if\
    \ info[item_type]:\n            items = [item.strip() for item in str(info[item_type]).split(\n\
    \                ',') if item]\n            return ', '.join(items)\n        return\
    \ ''\n\n    def process_question_type(self, question_type: str, question_id: str,\n\
    \        question_text: str) ->None:\n        \"\"\"\"\"\"\n        if question_type\
    \ == 'file':\n            query = question_text.format(filename=self.base_name)\n\
    \            info = self.file_details['file_info']\n            context = self.file_details['file_info']['file_code']\n\
    \            self.process_question(question_type, question_id, query,\n      \
    \          context, info)\n        elif question_type == 'method':\n         \
    \   for class_name, class_info in self.file_details['classes'].items():\n    \
    \            for key, method_info in class_info.items():\n                   \
    \ if key.startswith('class_method_'):\n                        method_name = key[len('class_method_'):]\n\
    \                        context = method_info['method_code']\n              \
    \          mapping = {'class_name': class_name, 'method_name':\n             \
    \               method_name}\n                        query = question_text.format(filename=self.\n\
    \                            base_name, **mapping)\n                        self.process_question(question_type,\
    \ question_id,\n                            query, context, method_info)\n   \
    \     else:\n            for name, info in self.file_details[self.question_mapping[\n\
    \                question_type]].items():\n                context = info[f'{question_type}_code']\n\
    \                mapping = {f'{question_type}_name': name}\n                if\
    \ question_id == f'{question_type}_purpose' and self.use_llm:\n              \
    \      variables_string = self.get_string_from_info(info,\n                  \
    \      f'{question_type}_variables')\n                    inputs_string = self.get_string_from_info(info,\n\
    \                        f'{question_type}_inputs')\n                    combined_string\
    \ = ', '.join([s for s in [\n                        variables_string, inputs_string]\
    \ if s])\n                    mapping[f'{question_type}_variables'\n         \
    \               ] = self.clean_and_get_unique_elements(combined_string)\n    \
    \                if question_type == 'class':\n                        methods_string\
    \ = self.get_string_from_info(info,\n                            f'{question_type}_methods')\n\
    \                        mapping[f'{question_type}_methods'] = methods_string\n\
    \                query = question_text.format(filename=self.base_name, **mapping\n\
    \                    )\n                self.process_question(question_type, question_id,\
    \ query,\n                    context, info)\n\n    def generate(self) ->Tuple[List[Dict],\
    \ List[Dict]]:\n        \"\"\"\"\"\"\n        for question in self.questions:\n\
    \            self.process_question_type(question['type'], question['id'],\n  \
    \              question['text'])\n        return self.instruct_list\n\n\ndef get_python_datasets(file_path:\
    \ str, file_details: Dict, base_name: str,\n    questions: List[Dict], model_config:\
    \ Dict) ->Tuple[List[Dict], List[Dict]]:\n    \"\"\"\"\"\"\n    generator = DatasetGenerator(file_path,\
    \ file_details, base_name,\n        questions, model_config)\n    return generator.generate()"
  entire_code_graph:
    nodes:
    - DatasetGenerator
    - DatasetGenerator.__init__
    - DatasetGenerator.clean_and_get_unique_elements
    - DatasetGenerator.get_response_from_llm
    - DatasetGenerator.get_context_and_prompt
    - DatasetGenerator.process_question
    - DatasetGenerator.get_string_from_info
    - DatasetGenerator.process_question_type
    - DatasetGenerator.generate
    - get_python_datasets
    - generator.generate
    - set
    - re.sub
    - element.strip
    - re.sub('\\s+', ' ', input_str).split
    - ''', ''.join'
    - self.model_config['prompt_template'].format
    - len
    - self.llm.tokenize
    - '''\n''.join'
    - any
    - item['instruction'].startswith
    - str
    - strategy
    - get_context_and_prompt
    - logger.error
    - math.ceil
    - self.llm
    - logging.info
    - question_id.endswith
    - info.get
    - str(response).strip
    - self.instruct_list.append
    - item.strip
    - str(info[item_type]).split
    - question_text.format
    - self.file_details['classes'].items
    - class_info.items
    - key.startswith
    - self.file_details[self.question_mapping[question_type]].items
    edges:
    - source: DatasetGenerator
      target: DatasetGenerator.__init__
      target_inputs:
      - self
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      target_returns: []
    - source: DatasetGenerator
      target: DatasetGenerator.clean_and_get_unique_elements
      target_inputs:
      - input_str
      target_returns:
      - ''', ''.join(cleaned_elements)'
    - source: DatasetGenerator
      target: DatasetGenerator.get_response_from_llm
      target_inputs:
      - self
      - query
      - context
      target_returns:
      - response
      - ''''''
      - (full_context, prompt, context_size)
    - source: DatasetGenerator
      target: DatasetGenerator.get_context_and_prompt
      target_inputs:
      - query
      - context
      - code_qa
      target_returns:
      - (full_context, prompt, context_size)
    - source: DatasetGenerator
      target: DatasetGenerator.process_question
      target_inputs:
      - self
      - question_type
      - question_id
      - query
      - context
      - info
      target_returns: []
    - source: DatasetGenerator
      target: DatasetGenerator.get_string_from_info
      target_inputs:
      - info
      - item_type
      target_returns:
      - ''''''
      - ''', ''.join(items)'
    - source: DatasetGenerator
      target: DatasetGenerator.process_question_type
      target_inputs:
      - self
      - question_type
      - question_id
      - question_text
      target_returns: []
    - source: DatasetGenerator
      target: DatasetGenerator.generate
      target_inputs:
      - self
      target_returns:
      - self.instruct_list
    - source: DatasetGenerator.clean_and_get_unique_elements
      target: set
      target_inputs:
      - (re.sub('[^\\w\\-_>\\s:/.]', '', element.strip()) for element in re.sub('\\s+',
        ' ', input_str).split(','))
    - source: DatasetGenerator.clean_and_get_unique_elements
      target: re.sub
      target_inputs:
      - '''\\s+'''
      - ''' '''
      - input_str
    - source: DatasetGenerator.clean_and_get_unique_elements
      target: element.strip
      target_inputs: []
    - source: DatasetGenerator.clean_and_get_unique_elements
      target: re.sub('\\s+', ' ', input_str).split
      target_inputs:
      - ''','''
    - source: DatasetGenerator.clean_and_get_unique_elements
      target: ''', ''.join'
      target_inputs:
      - cleaned_elements
    - source: DatasetGenerator.get_response_from_llm
      target: self.model_config['prompt_template'].format
      target_inputs: []
    - source: DatasetGenerator.get_response_from_llm
      target: len
      target_inputs:
      - self.llm.tokenize(prompt)
    - source: DatasetGenerator.get_response_from_llm
      target: self.llm.tokenize
      target_inputs:
      - prompt
    - source: DatasetGenerator.get_response_from_llm
      target: '''\n''.join'
      target_inputs:
      - '[f"Q: {item[''instruction'']} \nA: {item[''output'']}" for item in self.instruct_list
        if not any((item[''instruction''].startswith(prefix) for prefix in excluded_instructions))]'
    - source: DatasetGenerator.get_response_from_llm
      target: any
      target_inputs:
      - (item['instruction'].startswith(prefix) for prefix in excluded_instructions)
    - source: DatasetGenerator.get_response_from_llm
      target: item['instruction'].startswith
      target_inputs:
      - prefix
    - source: DatasetGenerator.get_response_from_llm
      target: str
      target_inputs:
      - self.file_details['file_info']['file_code_simplified']
    - source: DatasetGenerator.get_response_from_llm
      target: DatasetGenerator.get_string_from_info
      target_inputs:
      - info
      - item_type
      target_returns:
      - ''''''
      - ''', ''.join(items)'
    - source: DatasetGenerator.get_response_from_llm
      target: strategy
      target_inputs: []
    - source: DatasetGenerator.get_response_from_llm
      target: get_context_and_prompt
      target_inputs:
      - query
      - context
      - code_qa
    - source: DatasetGenerator.get_response_from_llm
      target: logger.error
      target_inputs:
      - '''Failed to generate model response'''
    - source: DatasetGenerator.get_response_from_llm
      target: math.ceil
      target_inputs:
      - context_size / 0.7
    - source: DatasetGenerator.get_response_from_llm
      target: re.sub
      target_inputs:
      - '''\\n\\s*\\n'''
      - '''\n\n'''
      - self.llm(prompt)
    - source: DatasetGenerator.get_response_from_llm
      target: self.llm
      target_inputs:
      - prompt
    - source: DatasetGenerator.get_response_from_llm
      target: logging.info
      target_inputs:
      - 'f''Response: {response}'''
    - source: DatasetGenerator.get_context_and_prompt
      target: self.model_config['prompt_template'].format
      target_inputs: []
    - source: DatasetGenerator.get_context_and_prompt
      target: len
      target_inputs:
      - self.llm.tokenize(prompt)
    - source: DatasetGenerator.get_context_and_prompt
      target: self.llm.tokenize
      target_inputs:
      - prompt
    - source: DatasetGenerator.process_question
      target: question_id.endswith
      target_inputs:
      - '''purpose'''
    - source: DatasetGenerator.process_question
      target: info.get
      target_inputs:
      - question_id
      - ''''''
    - source: DatasetGenerator.process_question
      target: DatasetGenerator.get_response_from_llm
      target_inputs:
      - self
      - query
      - context
      target_returns:
      - response
      - ''''''
      - (full_context, prompt, context_size)
    - source: DatasetGenerator.process_question
      target: DatasetGenerator.clean_and_get_unique_elements
      target_inputs:
      - input_str
      target_returns:
      - ''', ''.join(cleaned_elements)'
    - source: DatasetGenerator.process_question
      target: str
      target_inputs:
      - response
    - source: DatasetGenerator.process_question
      target: str(response).strip
      target_inputs: []
    - source: DatasetGenerator.process_question
      target: self.instruct_list.append
      target_inputs:
      - '{''instruction'': query, ''input'': context, ''output'': response_str}'
    - source: DatasetGenerator.get_string_from_info
      target: item.strip
      target_inputs: []
    - source: DatasetGenerator.get_string_from_info
      target: str(info[item_type]).split
      target_inputs:
      - ''','''
    - source: DatasetGenerator.get_string_from_info
      target: str
      target_inputs:
      - info[item_type]
    - source: DatasetGenerator.get_string_from_info
      target: ''', ''.join'
      target_inputs:
      - items
    - source: DatasetGenerator.process_question_type
      target: question_text.format
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: DatasetGenerator.process_question
      target_inputs:
      - self
      - question_type
      - question_id
      - query
      - context
      - info
      target_returns: []
    - source: DatasetGenerator.process_question_type
      target: self.file_details['classes'].items
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: class_info.items
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: key.startswith
      target_inputs:
      - '''class_method_'''
    - source: DatasetGenerator.process_question_type
      target: len
      target_inputs:
      - '''class_method_'''
    - source: DatasetGenerator.process_question_type
      target: self.file_details[self.question_mapping[question_type]].items
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: DatasetGenerator.get_string_from_info
      target_inputs:
      - info
      - item_type
      target_returns:
      - ''''''
      - ''', ''.join(items)'
    - source: DatasetGenerator.process_question_type
      target: ''', ''.join'
      target_inputs:
      - '[s for s in [variables_string, inputs_string] if s]'
    - source: DatasetGenerator.process_question_type
      target: DatasetGenerator.clean_and_get_unique_elements
      target_inputs:
      - input_str
      target_returns:
      - ''', ''.join(cleaned_elements)'
    - source: DatasetGenerator.generate
      target: DatasetGenerator.process_question_type
      target_inputs:
      - self
      - question_type
      - question_id
      - question_text
      target_returns: []
    - source: get_python_datasets
      target: DatasetGenerator
      target_inputs:
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      target_returns: []
    - source: get_python_datasets
      target: generator.generate
      target_inputs: []
functions:
  get_python_datasets:
    function_name: get_python_datasets
    function_code: "def get_python_datasets(file_path: str, file_details: Dict, base_name:\
      \ str, questions: List[Dict], model_config: Dict) -> Tuple[List[Dict], List[Dict]]:\n\
      \    \"\"\"\n    Extract information from a Python file and return it in JSON\
      \ format.\n    Args:\n        file_path (str): The path to the Python file.\n\
      \        file_details (Dict): The details of the file.\n        base_name (str):\
      \ The base Python code filename.\n        questions (List[Dict]): The list of\
      \ questions.\n        llm (object): The language model to be used for generating\
      \ responses.\n        prompt (str): The prompt to be used for generating responses.\n\
      \    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information\
      \ in JSON format.\n    \"\"\"\n    generator = DatasetGenerator(file_path, file_details,\
      \ base_name, questions, model_config)\n    return generator.generate()"
    function_ast: 'FunctionDef(name=''get_python_datasets'', args=arguments(posonlyargs=[],
      args=[arg(arg=''file_path'', annotation=Name(id=''str'', ctx=Load())), arg(arg=''file_details'',
      annotation=Name(id=''Dict'', ctx=Load())), arg(arg=''base_name'', annotation=Name(id=''str'',
      ctx=Load())), arg(arg=''questions'', annotation=Subscript(value=Name(id=''List'',
      ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load())), arg(arg=''model_config'',
      annotation=Name(id=''Dict'', ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]),
      body=[Expr(value=Constant(value=''\n    Extract information from a Python file
      and return it in JSON format.\n    Args:\n        file_path (str): The path
      to the Python file.\n        file_details (Dict): The details of the file.\n        base_name
      (str): The base Python code filename.\n        questions (List[Dict]): The list
      of questions.\n        llm (object): The language model to be used for generating
      responses.\n        prompt (str): The prompt to be used for generating responses.\n    Returns:\n        Tuple[List[Dict],
      List[Dict]]: Extracted information in JSON format.\n    '')), Assign(targets=[Name(id=''generator'',
      ctx=Store())], value=Call(func=Name(id=''DatasetGenerator'', ctx=Load()), args=[Name(id=''file_path'',
      ctx=Load()), Name(id=''file_details'', ctx=Load()), Name(id=''base_name'', ctx=Load()),
      Name(id=''questions'', ctx=Load()), Name(id=''model_config'', ctx=Load())],
      keywords=[])), Return(value=Call(func=Attribute(value=Name(id=''generator'',
      ctx=Load()), attr=''generate'', ctx=Load()), args=[], keywords=[]))], decorator_list=[],
      returns=Subscript(value=Name(id=''Tuple'', ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id=''List'',
      ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load()), Subscript(value=Name(id=''List'',
      ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load())], ctx=Load()),
      ctx=Load()))'
    function_docstring: "\n    Extract information from a Python file and return it\
      \ in JSON format.\n    Args:\n        file_path (str): The path to the Python\
      \ file.\n        file_details (Dict): The details of the file.\n        base_name\
      \ (str): The base Python code filename.\n        questions (List[Dict]): The\
      \ list of questions.\n        llm (object): The language model to be used for\
      \ generating responses.\n        prompt (str): The prompt to be used for generating\
      \ responses.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted\
      \ information in JSON format.\n    "
    function_inputs:
    - file_path
    - file_details
    - base_name
    - questions
    - model_config
    function_defaults: []
    function_returns:
    - generator.generate()
    function_calls:
    - DatasetGenerator
    - generator.generate
    function_call_inputs:
      DatasetGenerator:
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      generator.generate: []
    function_variables:
    - generator
    function_decorators: []
    function_annotations: []
    function_properties: []
classes:
  DatasetGenerator:
    class_name: DatasetGenerator
    class_code: "class DatasetGenerator:\n    \"\"\"\n    Generate JSON formatted\
      \ dictionary outputs for a Python file.\n    Attributes:\n        file_path\
      \ (str): The path to the Python file.\n        file_details (Dict[str, Any]):\
      \ Details of the Python file.\n        base_name (str): The base name of the\
      \ Python file.\n        questions (List[Dict[str, str]]): Questions for generating\
      \ responses.\n        instruct_list (List[Dict[str, str]]): Storage for generated\
      \ instructions.\n        question_mapping (Dict[str, str]): Mapping of question\
      \ types to keys in file details.\n        use_llm (bool): Flag indicating if\
      \ a language model should be used.\n        llm (object): The language model\
      \ for generating responses.\n        prompt (str): The prompt format for querying\
      \ the language model.\n    Methods:\n        clean_and_get_unique_elements(input_str:\
      \ str) -> str: \n            Clean and return unique elements from an input\
      \ string.\n        add_to_list(list_to_update: List[Dict], query: str, response:\
      \ str, additional_field=None) -> List[Dict]: \n            Add response to the\
      \ instruct list.\n        get_response_from_llm(query: str, context: str) ->\
      \ str:\n            Get language model response to query for given context.\n\
      \        process_question(question_type: str, question_id: str, query: str,\
      \ context: str, info: Dict) -> None:\n            Process question and add generated\
      \ response to the instruct_list.\n        process_question_type(question_type:\
      \ str, question_id: str, question_text: str) -> None:\n            Process question\
      \ related to file, function, class, or method.\n        generate() -> Tuple[List[Dict],\
      \ List[Dict]]:\n            Generate responses for all the questions and return\
      \ the instruct_list.\n    \"\"\"\n\n    def __init__(self, file_path: str, file_details:\
      \ Dict, base_name: str, questions: List[Dict], model_config: Dict) -> None:\n\
      \        self.file_path = file_path\n        self.file_details = file_details\n\
      \        self.base_name = base_name\n        self.questions = questions\n  \
      \      self.model_config = model_config\n        self.llm = model_config['model']\n\
      \        if self.llm is None:\n            self.use_llm = False\n        else:\n\
      \            self.use_llm = True\n        self.instruct_list = []\n        self.question_mapping\
      \ = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method':\
      \ 'classes'}\n\n    @staticmethod\n    def clean_and_get_unique_elements(input_str:\
      \ str) -> str:\n        \"\"\"\n        Clean input string and return string\
      \ of unique elements.\n        Args:\n            input_str (str): The input\
      \ string to be cleaned.\n        Returns:\n            str: The cleaned string.\n\
      \        \"\"\"\n        cleaned_elements = set((re.sub('[^\\\\w\\\\-_>\\\\\
      s:/.]', '', element.strip()) for element in re.sub('\\\\s+', ' ', input_str).split(',')))\n\
      \        return ', '.join(cleaned_elements)\n\n    def get_response_from_llm(self,\
      \ query: str, context: str) -> str:\n        \"\"\"\n        Get language model\
      \ response to query for given context.\n        Args:\n            query (str):\
      \ The query to be used for generating the response.\n            context (str):\
      \ The context to be used for generating the response.\n        Returns:\n  \
      \          str: The generated response.\n        \"\"\"\n\n        def get_context_and_prompt(query,\
      \ context, code_qa):\n            full_context = f'{context}\\nCODE Q and A:\\\
      n{code_qa}'\n            prompt = self.model_config['prompt_template'].format(context=full_context,\
      \ query=query)\n            context_size = len(self.llm.tokenize(prompt))\n\
      \            return (full_context, prompt, context_size)\n        max_context_length\
      \ = self.model_config['inference_model']['model_params']['context_length']\n\
      \        excluded_instructions = ['Call code graph', 'Docstring']\n        code_qa\
      \ = '\\n'.join([f\"Q: {item['instruction']} \\nA: {item['output']}\" for item\
      \ in self.instruct_list if not any((item['instruction'].startswith(prefix) for\
      \ prefix in excluded_instructions))])\n        context_strategies = [lambda:\
      \ '```python\\n' + str(context) + '\\n```', lambda: '```python\\n' + str(self.file_details['file_info']['file_code_simplified'])\
      \ + '\\n```', lambda: self.get_string_from_info(self.file_details['file_info'],\
      \ 'file_summary'), lambda: '']\n        for strategy in context_strategies:\n\
      \            context = strategy()\n            full_context, prompt, context_size\
      \ = get_context_and_prompt(query, context, code_qa)\n            if context_size\
      \ <= 0.7 * max_context_length:\n                break\n        else:\n     \
      \       logger.error(f'Failed to generate model response, adjust context_length\
      \ > {math.ceil(context_size / 0.7)} in py2dataset_model_config.yaml')\n    \
      \        return ''\n        try:\n            response = re.sub('\\\\n\\\\s*\\\
      \\n', '\\n\\n', self.llm(prompt))\n            logging.info(f'Response: {response}')\n\
      \        except:\n            logger.error('Failed to generate model response')\n\
      \            response = ''\n        return response\n\n    def process_question(self,\
      \ question_type: str, question_id: str, query: str, context: str, info: Dict)\
      \ -> None:\n        \"\"\"\n        Process question and add the generated response\
      \ to the instruct_list.\n        Args:\n            question_type (str): The\
      \ type of question to be processed.\n            question_id (str): The ID of\
      \ the question to be processed.\n            query (str): The query to be processed.\n\
      \            context (str): The context to be used for generating the response.\n\
      \            info (Dict): The information of the Python file.\n        Returns:\n\
      \            None\n        \"\"\"\n        if question_id.endswith('code_graph')\
      \ or question_id.endswith('docstring'):\n            response = info.get(question_id,\
      \ {})\n        else:\n            response = self.get_response_from_llm(query,\
      \ context) if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id,\
      \ '')))\n        if question_type == 'file':\n            context = self.file_details['file_info']['file_code']\n\
      \        if response and response != 'None':\n            response_str = str(response).strip()\n\
      \            if response_str:\n                self.instruct_list.append({'instruction':\
      \ query, 'input': context, 'output': response_str})\n\n    @staticmethod\n \
      \   def get_string_from_info(info, item_type):\n        if info[item_type]:\n\
      \            items = [item.strip() for item in str(info[item_type]).split(',')\
      \ if item]\n            return ', '.join(items)\n        return ''\n\n    def\
      \ process_question_type(self, question_type: str, question_id: str, question_text:\
      \ str) -> None:\n        \"\"\"\n        Process questions related to a file,\
      \ function, class, or method.\n        Args:\n            question_type (str):\
      \ The type of question to be processed.\n            question_id (str): The\
      \ ID of the question to be processed.\n            question_text (str): The\
      \ text of the question to be processed.\n        Returns:\n            None\n\
      \        \"\"\"\n        if question_type == 'file':\n            query = question_text.format(filename=self.base_name)\n\
      \            info = self.file_details['file_info']\n            context = self.file_details['file_info']['file_code']\n\
      \            self.process_question(question_type, question_id, query, context,\
      \ info)\n        elif question_type == 'method':\n            for class_name,\
      \ class_info in self.file_details['classes'].items():\n                for key,\
      \ method_info in class_info.items():\n                    if key.startswith('class_method_'):\n\
      \                        method_name = key[len('class_method_'):]\n        \
      \                context = method_info['method_code']\n                    \
      \    mapping = {'class_name': class_name, 'method_name': method_name}\n    \
      \                    query = question_text.format(filename=self.base_name, **mapping)\n\
      \                        self.process_question(question_type, question_id, query,\
      \ context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n\
      \                context = info[f'{question_type}_code']\n                mapping\
      \ = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_purpose'\
      \ and self.use_llm:\n                    variables_string = self.get_string_from_info(info,\
      \ f'{question_type}_variables')\n                    inputs_string = self.get_string_from_info(info,\
      \ f'{question_type}_inputs')\n                    combined_string = ', '.join([s\
      \ for s in [variables_string, inputs_string] if s])\n                    mapping[f'{question_type}_variables']\
      \ = self.clean_and_get_unique_elements(combined_string)\n                  \
      \  if question_type == 'class':\n                        methods_string = self.get_string_from_info(info,\
      \ f'{question_type}_methods')\n                        mapping[f'{question_type}_methods']\
      \ = methods_string\n                query = question_text.format(filename=self.base_name,\
      \ **mapping)\n                self.process_question(question_type, question_id,\
      \ query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n\
      \        \"\"\"\n        Generate responses for all the questions and returns\
      \ the instruct_list.\n        Args:\n            None\n        Returns:\n  \
      \          Tuple[List[Dict], List[Dict]]: The generated question-answer pairs\
      \ and instructions.\n        \"\"\"\n        for question in self.questions:\n\
      \            self.process_question_type(question['type'], question['id'], question['text'])\n\
      \        return self.instruct_list"
    class_ast: 'ClassDef(name=''DatasetGenerator'', bases=[], keywords=[], body=[Expr(value=Constant(value=''\n    Generate
      JSON formatted dictionary outputs for a Python file.\n    Attributes:\n        file_path
      (str): The path to the Python file.\n        file_details (Dict[str, Any]):
      Details of the Python file.\n        base_name (str): The base name of the Python
      file.\n        questions (List[Dict[str, str]]): Questions for generating responses.\n        instruct_list
      (List[Dict[str, str]]): Storage for generated instructions.\n        question_mapping
      (Dict[str, str]): Mapping of question types to keys in file details.\n        use_llm
      (bool): Flag indicating if a language model should be used.\n        llm (object):
      The language model for generating responses.\n        prompt (str): The prompt
      format for querying the language model.\n    Methods:\n        clean_and_get_unique_elements(input_str:
      str) -> str: \n            Clean and return unique elements from an input string.\n        add_to_list(list_to_update:
      List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:
      \n            Add response to the instruct list.\n        get_response_from_llm(query:
      str, context: str) -> str:\n            Get language model response to query
      for given context.\n        process_question(question_type: str, question_id:
      str, query: str, context: str, info: Dict) -> None:\n            Process question
      and add generated response to the instruct_list.\n        process_question_type(question_type:
      str, question_id: str, question_text: str) -> None:\n            Process question
      related to file, function, class, or method.\n        generate() -> Tuple[List[Dict],
      List[Dict]]:\n            Generate responses for all the questions and return
      the instruct_list.\n    '')), FunctionDef(name=''__init__'', args=arguments(posonlyargs=[],
      args=[arg(arg=''self''), arg(arg=''file_path'', annotation=Name(id=''str'',
      ctx=Load())), arg(arg=''file_details'', annotation=Name(id=''Dict'', ctx=Load())),
      arg(arg=''base_name'', annotation=Name(id=''str'', ctx=Load())), arg(arg=''questions'',
      annotation=Subscript(value=Name(id=''List'', ctx=Load()), slice=Name(id=''Dict'',
      ctx=Load()), ctx=Load())), arg(arg=''model_config'', annotation=Name(id=''Dict'',
      ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Assign(targets=[Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_path'', ctx=Store())], value=Name(id=''file_path'',
      ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()),
      attr=''file_details'', ctx=Store())], value=Name(id=''file_details'', ctx=Load())),
      Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'',
      ctx=Store())], value=Name(id=''base_name'', ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''questions'', ctx=Store())], value=Name(id=''questions'',
      ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()),
      attr=''model_config'', ctx=Store())], value=Name(id=''model_config'', ctx=Load())),
      Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''llm'',
      ctx=Store())], value=Subscript(value=Name(id=''model_config'', ctx=Load()),
      slice=Constant(value=''model''), ctx=Load())), If(test=Compare(left=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''llm'', ctx=Load()), ops=[Is()], comparators=[Constant(value=None)]),
      body=[Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''use_llm'',
      ctx=Store())], value=Constant(value=False))], orelse=[Assign(targets=[Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''use_llm'', ctx=Store())], value=Constant(value=True))]),
      Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''instruct_list'',
      ctx=Store())], value=List(elts=[], ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''question_mapping'', ctx=Store())], value=Dict(keys=[Constant(value=''file''),
      Constant(value=''function''), Constant(value=''class''), Constant(value=''method'')],
      values=[Constant(value=''file''), Constant(value=''functions''), Constant(value=''classes''),
      Constant(value=''classes'')]))], decorator_list=[], returns=Constant(value=None)),
      FunctionDef(name=''clean_and_get_unique_elements'', args=arguments(posonlyargs=[],
      args=[arg(arg=''input_str'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
      kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Clean
      input string and return string of unique elements.\n        Args:\n            input_str
      (str): The input string to be cleaned.\n        Returns:\n            str: The
      cleaned string.\n        '')), Assign(targets=[Name(id=''cleaned_elements'',
      ctx=Store())], value=Call(func=Name(id=''set'', ctx=Load()), args=[GeneratorExp(elt=Call(func=Attribute(value=Name(id=''re'',
      ctx=Load()), attr=''sub'', ctx=Load()), args=[Constant(value=''[^\\w\\-_>\\s:/.]''),
      Constant(value=''''), Call(func=Attribute(value=Name(id=''element'', ctx=Load()),
      attr=''strip'', ctx=Load()), args=[], keywords=[])], keywords=[]), generators=[comprehension(target=Name(id=''element'',
      ctx=Store()), iter=Call(func=Attribute(value=Call(func=Attribute(value=Name(id=''re'',
      ctx=Load()), attr=''sub'', ctx=Load()), args=[Constant(value=''\\s+''), Constant(value=''
      ''), Name(id=''input_str'', ctx=Load())], keywords=[]), attr=''split'', ctx=Load()),
      args=[Constant(value='','')], keywords=[]), ifs=[], is_async=0)])], keywords=[])),
      Return(value=Call(func=Attribute(value=Constant(value='', ''), attr=''join'',
      ctx=Load()), args=[Name(id=''cleaned_elements'', ctx=Load())], keywords=[]))],
      decorator_list=[Name(id=''staticmethod'', ctx=Load())], returns=Name(id=''str'',
      ctx=Load())), FunctionDef(name=''get_response_from_llm'', args=arguments(posonlyargs=[],
      args=[arg(arg=''self''), arg(arg=''query'', annotation=Name(id=''str'', ctx=Load())),
      arg(arg=''context'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
      kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Get
      language model response to query for given context.\n        Args:\n            query
      (str): The query to be used for generating the response.\n            context
      (str): The context to be used for generating the response.\n        Returns:\n            str:
      The generated response.\n        '')), FunctionDef(name=''get_context_and_prompt'',
      args=arguments(posonlyargs=[], args=[arg(arg=''query''), arg(arg=''context''),
      arg(arg=''code_qa'')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id=''full_context'',
      ctx=Store())], value=JoinedStr(values=[FormattedValue(value=Name(id=''context'',
      ctx=Load()), conversion=-1), Constant(value=''\nCODE Q and A:\n''), FormattedValue(value=Name(id=''code_qa'',
      ctx=Load()), conversion=-1)])), Assign(targets=[Name(id=''prompt'', ctx=Store())],
      value=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''model_config'', ctx=Load()), slice=Constant(value=''prompt_template''),
      ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''context'',
      value=Name(id=''full_context'', ctx=Load())), keyword(arg=''query'', value=Name(id=''query'',
      ctx=Load()))])), Assign(targets=[Name(id=''context_size'', ctx=Store())], value=Call(func=Name(id=''len'',
      ctx=Load()), args=[Call(func=Attribute(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''llm'', ctx=Load()), attr=''tokenize'', ctx=Load()), args=[Name(id=''prompt'',
      ctx=Load())], keywords=[])], keywords=[])), Return(value=Tuple(elts=[Name(id=''full_context'',
      ctx=Load()), Name(id=''prompt'', ctx=Load()), Name(id=''context_size'', ctx=Load())],
      ctx=Load()))], decorator_list=[]), Assign(targets=[Name(id=''max_context_length'',
      ctx=Store())], value=Subscript(value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''model_config'', ctx=Load()), slice=Constant(value=''inference_model''),
      ctx=Load()), slice=Constant(value=''model_params''), ctx=Load()), slice=Constant(value=''context_length''),
      ctx=Load())), Assign(targets=[Name(id=''excluded_instructions'', ctx=Store())],
      value=List(elts=[Constant(value=''Call code graph''), Constant(value=''Docstring'')],
      ctx=Load())), Assign(targets=[Name(id=''code_qa'', ctx=Store())], value=Call(func=Attribute(value=Constant(value=''\n''),
      attr=''join'', ctx=Load()), args=[ListComp(elt=JoinedStr(values=[Constant(value=''Q:
      ''), FormattedValue(value=Subscript(value=Name(id=''item'', ctx=Load()), slice=Constant(value=''instruction''),
      ctx=Load()), conversion=-1), Constant(value='' \nA: ''), FormattedValue(value=Subscript(value=Name(id=''item'',
      ctx=Load()), slice=Constant(value=''output''), ctx=Load()), conversion=-1)]),
      generators=[comprehension(target=Name(id=''item'', ctx=Store()), iter=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''instruct_list'', ctx=Load()), ifs=[UnaryOp(op=Not(), operand=Call(func=Name(id=''any'',
      ctx=Load()), args=[GeneratorExp(elt=Call(func=Attribute(value=Subscript(value=Name(id=''item'',
      ctx=Load()), slice=Constant(value=''instruction''), ctx=Load()), attr=''startswith'',
      ctx=Load()), args=[Name(id=''prefix'', ctx=Load())], keywords=[]), generators=[comprehension(target=Name(id=''prefix'',
      ctx=Store()), iter=Name(id=''excluded_instructions'', ctx=Load()), ifs=[], is_async=0)])],
      keywords=[]))], is_async=0)])], keywords=[])), Assign(targets=[Name(id=''context_strategies'',
      ctx=Store())], value=List(elts=[Lambda(args=arguments(posonlyargs=[], args=[],
      kwonlyargs=[], kw_defaults=[], defaults=[]), body=BinOp(left=BinOp(left=Constant(value=''```python\n''),
      op=Add(), right=Call(func=Name(id=''str'', ctx=Load()), args=[Name(id=''context'',
      ctx=Load())], keywords=[])), op=Add(), right=Constant(value=''\n```''))), Lambda(args=arguments(posonlyargs=[],
      args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=BinOp(left=BinOp(left=Constant(value=''```python\n''),
      op=Add(), right=Call(func=Name(id=''str'', ctx=Load()), args=[Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
      ctx=Load()), slice=Constant(value=''file_code_simplified''), ctx=Load())], keywords=[])),
      op=Add(), right=Constant(value=''\n```''))), Lambda(args=arguments(posonlyargs=[],
      args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=Call(func=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''get_string_from_info'', ctx=Load()), args=[Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
      ctx=Load()), Constant(value=''file_summary'')], keywords=[])), Lambda(args=arguments(posonlyargs=[],
      args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=Constant(value=''''))],
      ctx=Load())), For(target=Name(id=''strategy'', ctx=Store()), iter=Name(id=''context_strategies'',
      ctx=Load()), body=[Assign(targets=[Name(id=''context'', ctx=Store())], value=Call(func=Name(id=''strategy'',
      ctx=Load()), args=[], keywords=[])), Assign(targets=[Tuple(elts=[Name(id=''full_context'',
      ctx=Store()), Name(id=''prompt'', ctx=Store()), Name(id=''context_size'', ctx=Store())],
      ctx=Store())], value=Call(func=Name(id=''get_context_and_prompt'', ctx=Load()),
      args=[Name(id=''query'', ctx=Load()), Name(id=''context'', ctx=Load()), Name(id=''code_qa'',
      ctx=Load())], keywords=[])), If(test=Compare(left=Name(id=''context_size'',
      ctx=Load()), ops=[LtE()], comparators=[BinOp(left=Constant(value=0.7), op=Mult(),
      right=Name(id=''max_context_length'', ctx=Load()))]), body=[Break()], orelse=[])],
      orelse=[Expr(value=Call(func=Attribute(value=Name(id=''logger'', ctx=Load()),
      attr=''error'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Failed
      to generate model response, adjust context_length > ''), FormattedValue(value=Call(func=Attribute(value=Name(id=''math'',
      ctx=Load()), attr=''ceil'', ctx=Load()), args=[BinOp(left=Name(id=''context_size'',
      ctx=Load()), op=Div(), right=Constant(value=0.7))], keywords=[]), conversion=-1),
      Constant(value='' in py2dataset_model_config.yaml'')])], keywords=[])), Return(value=Constant(value=''''))]),
      Try(body=[Assign(targets=[Name(id=''response'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''re'',
      ctx=Load()), attr=''sub'', ctx=Load()), args=[Constant(value=''\\n\\s*\\n''),
      Constant(value=''\n\n''), Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
      attr=''llm'', ctx=Load()), args=[Name(id=''prompt'', ctx=Load())], keywords=[])],
      keywords=[])), Expr(value=Call(func=Attribute(value=Name(id=''logging'', ctx=Load()),
      attr=''info'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Response:
      ''), FormattedValue(value=Name(id=''response'', ctx=Load()), conversion=-1)])],
      keywords=[]))], handlers=[ExceptHandler(body=[Expr(value=Call(func=Attribute(value=Name(id=''logger'',
      ctx=Load()), attr=''error'', ctx=Load()), args=[Constant(value=''Failed to generate
      model response'')], keywords=[])), Assign(targets=[Name(id=''response'', ctx=Store())],
      value=Constant(value=''''))])], orelse=[], finalbody=[]), Return(value=Name(id=''response'',
      ctx=Load()))], decorator_list=[], returns=Name(id=''str'', ctx=Load())), FunctionDef(name=''process_question'',
      args=arguments(posonlyargs=[], args=[arg(arg=''self''), arg(arg=''question_type'',
      annotation=Name(id=''str'', ctx=Load())), arg(arg=''question_id'', annotation=Name(id=''str'',
      ctx=Load())), arg(arg=''query'', annotation=Name(id=''str'', ctx=Load())), arg(arg=''context'',
      annotation=Name(id=''str'', ctx=Load())), arg(arg=''info'', annotation=Name(id=''Dict'',
      ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Process
      question and add the generated response to the instruct_list.\n        Args:\n            question_type
      (str): The type of question to be processed.\n            question_id (str):
      The ID of the question to be processed.\n            query (str): The query
      to be processed.\n            context (str): The context to be used for generating
      the response.\n            info (Dict): The information of the Python file.\n        Returns:\n            None\n        '')),
      If(test=BoolOp(op=Or(), values=[Call(func=Attribute(value=Name(id=''question_id'',
      ctx=Load()), attr=''endswith'', ctx=Load()), args=[Constant(value=''code_graph'')],
      keywords=[]), Call(func=Attribute(value=Name(id=''question_id'', ctx=Load()),
      attr=''endswith'', ctx=Load()), args=[Constant(value=''docstring'')], keywords=[])]),
      body=[Assign(targets=[Name(id=''response'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''info'',
      ctx=Load()), attr=''get'', ctx=Load()), args=[Name(id=''question_id'', ctx=Load()),
      Dict(keys=[], values=[])], keywords=[]))], orelse=[Assign(targets=[Name(id=''response'',
      ctx=Store())], value=IfExp(test=BoolOp(op=And(), values=[Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''use_llm'', ctx=Load()), Call(func=Attribute(value=Name(id=''question_id'',
      ctx=Load()), attr=''endswith'', ctx=Load()), args=[Constant(value=''purpose'')],
      keywords=[])]), body=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
      attr=''get_response_from_llm'', ctx=Load()), args=[Name(id=''query'', ctx=Load()),
      Name(id=''context'', ctx=Load())], keywords=[]), orelse=Call(func=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''clean_and_get_unique_elements'', ctx=Load()), args=[Call(func=Name(id=''str'',
      ctx=Load()), args=[Call(func=Attribute(value=Name(id=''info'', ctx=Load()),
      attr=''get'', ctx=Load()), args=[Name(id=''question_id'', ctx=Load()), Constant(value='''')],
      keywords=[])], keywords=[])], keywords=[])))]), If(test=Compare(left=Name(id=''question_type'',
      ctx=Load()), ops=[Eq()], comparators=[Constant(value=''file'')]), body=[Assign(targets=[Name(id=''context'',
      ctx=Store())], value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
      ctx=Load()), slice=Constant(value=''file_code''), ctx=Load()))], orelse=[]),
      If(test=BoolOp(op=And(), values=[Name(id=''response'', ctx=Load()), Compare(left=Name(id=''response'',
      ctx=Load()), ops=[NotEq()], comparators=[Constant(value=''None'')])]), body=[Assign(targets=[Name(id=''response_str'',
      ctx=Store())], value=Call(func=Attribute(value=Call(func=Name(id=''str'', ctx=Load()),
      args=[Name(id=''response'', ctx=Load())], keywords=[]), attr=''strip'', ctx=Load()),
      args=[], keywords=[])), If(test=Name(id=''response_str'', ctx=Load()), body=[Expr(value=Call(func=Attribute(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''instruct_list'', ctx=Load()), attr=''append'', ctx=Load()),
      args=[Dict(keys=[Constant(value=''instruction''), Constant(value=''input''),
      Constant(value=''output'')], values=[Name(id=''query'', ctx=Load()), Name(id=''context'',
      ctx=Load()), Name(id=''response_str'', ctx=Load())])], keywords=[]))], orelse=[])],
      orelse=[])], decorator_list=[], returns=Constant(value=None)), FunctionDef(name=''get_string_from_info'',
      args=arguments(posonlyargs=[], args=[arg(arg=''info''), arg(arg=''item_type'')],
      kwonlyargs=[], kw_defaults=[], defaults=[]), body=[If(test=Subscript(value=Name(id=''info'',
      ctx=Load()), slice=Name(id=''item_type'', ctx=Load()), ctx=Load()), body=[Assign(targets=[Name(id=''items'',
      ctx=Store())], value=ListComp(elt=Call(func=Attribute(value=Name(id=''item'',
      ctx=Load()), attr=''strip'', ctx=Load()), args=[], keywords=[]), generators=[comprehension(target=Name(id=''item'',
      ctx=Store()), iter=Call(func=Attribute(value=Call(func=Name(id=''str'', ctx=Load()),
      args=[Subscript(value=Name(id=''info'', ctx=Load()), slice=Name(id=''item_type'',
      ctx=Load()), ctx=Load())], keywords=[]), attr=''split'', ctx=Load()), args=[Constant(value='','')],
      keywords=[]), ifs=[Name(id=''item'', ctx=Load())], is_async=0)])), Return(value=Call(func=Attribute(value=Constant(value='',
      ''), attr=''join'', ctx=Load()), args=[Name(id=''items'', ctx=Load())], keywords=[]))],
      orelse=[]), Return(value=Constant(value=''''))], decorator_list=[Name(id=''staticmethod'',
      ctx=Load())]), FunctionDef(name=''process_question_type'', args=arguments(posonlyargs=[],
      args=[arg(arg=''self''), arg(arg=''question_type'', annotation=Name(id=''str'',
      ctx=Load())), arg(arg=''question_id'', annotation=Name(id=''str'', ctx=Load())),
      arg(arg=''question_text'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
      kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Process
      questions related to a file, function, class, or method.\n        Args:\n            question_type
      (str): The type of question to be processed.\n            question_id (str):
      The ID of the question to be processed.\n            question_text (str): The
      text of the question to be processed.\n        Returns:\n            None\n        '')),
      If(test=Compare(left=Name(id=''question_type'', ctx=Load()), ops=[Eq()], comparators=[Constant(value=''file'')]),
      body=[Assign(targets=[Name(id=''query'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''question_text'',
      ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''filename'',
      value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'', ctx=Load()))])),
      Assign(targets=[Name(id=''info'', ctx=Store())], value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
      ctx=Load())), Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
      ctx=Load()), slice=Constant(value=''file_code''), ctx=Load())), Expr(value=Call(func=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'',
      ctx=Load()), Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()),
      Name(id=''context'', ctx=Load()), Name(id=''info'', ctx=Load())], keywords=[]))],
      orelse=[If(test=Compare(left=Name(id=''question_type'', ctx=Load()), ops=[Eq()],
      comparators=[Constant(value=''method'')]), body=[For(target=Tuple(elts=[Name(id=''class_name'',
      ctx=Store()), Name(id=''class_info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''classes''),
      ctx=Load()), attr=''items'', ctx=Load()), args=[], keywords=[]), body=[For(target=Tuple(elts=[Name(id=''key'',
      ctx=Store()), Name(id=''method_info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Name(id=''class_info'',
      ctx=Load()), attr=''items'', ctx=Load()), args=[], keywords=[]), body=[If(test=Call(func=Attribute(value=Name(id=''key'',
      ctx=Load()), attr=''startswith'', ctx=Load()), args=[Constant(value=''class_method_'')],
      keywords=[]), body=[Assign(targets=[Name(id=''method_name'', ctx=Store())],
      value=Subscript(value=Name(id=''key'', ctx=Load()), slice=Slice(lower=Call(func=Name(id=''len'',
      ctx=Load()), args=[Constant(value=''class_method_'')], keywords=[])), ctx=Load())),
      Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Name(id=''method_info'',
      ctx=Load()), slice=Constant(value=''method_code''), ctx=Load())), Assign(targets=[Name(id=''mapping'',
      ctx=Store())], value=Dict(keys=[Constant(value=''class_name''), Constant(value=''method_name'')],
      values=[Name(id=''class_name'', ctx=Load()), Name(id=''method_name'', ctx=Load())])),
      Assign(targets=[Name(id=''query'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''question_text'',
      ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''filename'',
      value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'', ctx=Load())),
      keyword(value=Name(id=''mapping'', ctx=Load()))])), Expr(value=Call(func=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'',
      ctx=Load()), Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()),
      Name(id=''context'', ctx=Load()), Name(id=''method_info'', ctx=Load())], keywords=[]))],
      orelse=[])], orelse=[])], orelse=[])], orelse=[For(target=Tuple(elts=[Name(id=''name'',
      ctx=Store()), Name(id=''info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_details'', ctx=Load()), slice=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''question_mapping'', ctx=Load()), slice=Name(id=''question_type'',
      ctx=Load()), ctx=Load()), ctx=Load()), attr=''items'', ctx=Load()), args=[],
      keywords=[]), body=[Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Name(id=''info'',
      ctx=Load()), slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
      ctx=Load()), conversion=-1), Constant(value=''_code'')]), ctx=Load())), Assign(targets=[Name(id=''mapping'',
      ctx=Store())], value=Dict(keys=[JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
      ctx=Load()), conversion=-1), Constant(value=''_name'')])], values=[Name(id=''name'',
      ctx=Load())])), If(test=BoolOp(op=And(), values=[Compare(left=Name(id=''question_id'',
      ctx=Load()), ops=[Eq()], comparators=[JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
      ctx=Load()), conversion=-1), Constant(value=''_purpose'')])]), Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''use_llm'', ctx=Load())]), body=[Assign(targets=[Name(id=''variables_string'',
      ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
      attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
      JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
      conversion=-1), Constant(value=''_variables'')])], keywords=[])), Assign(targets=[Name(id=''inputs_string'',
      ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
      attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
      JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
      conversion=-1), Constant(value=''_inputs'')])], keywords=[])), Assign(targets=[Name(id=''combined_string'',
      ctx=Store())], value=Call(func=Attribute(value=Constant(value='', ''), attr=''join'',
      ctx=Load()), args=[ListComp(elt=Name(id=''s'', ctx=Load()), generators=[comprehension(target=Name(id=''s'',
      ctx=Store()), iter=List(elts=[Name(id=''variables_string'', ctx=Load()), Name(id=''inputs_string'',
      ctx=Load())], ctx=Load()), ifs=[Name(id=''s'', ctx=Load())], is_async=0)])],
      keywords=[])), Assign(targets=[Subscript(value=Name(id=''mapping'', ctx=Load()),
      slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
      conversion=-1), Constant(value=''_variables'')]), ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''clean_and_get_unique_elements'', ctx=Load()), args=[Name(id=''combined_string'',
      ctx=Load())], keywords=[])), If(test=Compare(left=Name(id=''question_type'',
      ctx=Load()), ops=[Eq()], comparators=[Constant(value=''class'')]), body=[Assign(targets=[Name(id=''methods_string'',
      ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
      attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
      JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
      conversion=-1), Constant(value=''_methods'')])], keywords=[])), Assign(targets=[Subscript(value=Name(id=''mapping'',
      ctx=Load()), slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
      ctx=Load()), conversion=-1), Constant(value=''_methods'')]), ctx=Store())],
      value=Name(id=''methods_string'', ctx=Load()))], orelse=[])], orelse=[]), Assign(targets=[Name(id=''query'',
      ctx=Store())], value=Call(func=Attribute(value=Name(id=''question_text'', ctx=Load()),
      attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''filename'', value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''base_name'', ctx=Load())), keyword(value=Name(id=''mapping'',
      ctx=Load()))])), Expr(value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
      attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'', ctx=Load()),
      Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()), Name(id=''context'',
      ctx=Load()), Name(id=''info'', ctx=Load())], keywords=[]))], orelse=[])])])],
      decorator_list=[], returns=Constant(value=None)), FunctionDef(name=''generate'',
      args=arguments(posonlyargs=[], args=[arg(arg=''self'')], kwonlyargs=[], kw_defaults=[],
      defaults=[]), body=[Expr(value=Constant(value=''\n        Generate responses
      for all the questions and returns the instruct_list.\n        Args:\n            None\n        Returns:\n            Tuple[List[Dict],
      List[Dict]]: The generated question-answer pairs and instructions.\n        '')),
      For(target=Name(id=''question'', ctx=Store()), iter=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''questions'', ctx=Load()), body=[Expr(value=Call(func=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''process_question_type'', ctx=Load()), args=[Subscript(value=Name(id=''question'',
      ctx=Load()), slice=Constant(value=''type''), ctx=Load()), Subscript(value=Name(id=''question'',
      ctx=Load()), slice=Constant(value=''id''), ctx=Load()), Subscript(value=Name(id=''question'',
      ctx=Load()), slice=Constant(value=''text''), ctx=Load())], keywords=[]))], orelse=[]),
      Return(value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''instruct_list'',
      ctx=Load()))], decorator_list=[], returns=Subscript(value=Name(id=''Tuple'',
      ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id=''List'', ctx=Load()),
      slice=Name(id=''Dict'', ctx=Load()), ctx=Load()), Subscript(value=Name(id=''List'',
      ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load())], ctx=Load()),
      ctx=Load()))], decorator_list=[])'
    class_docstring: "\n    Generate JSON formatted dictionary outputs for a Python\
      \ file.\n    Attributes:\n        file_path (str): The path to the Python file.\n\
      \        file_details (Dict[str, Any]): Details of the Python file.\n      \
      \  base_name (str): The base name of the Python file.\n        questions (List[Dict[str,\
      \ str]]): Questions for generating responses.\n        instruct_list (List[Dict[str,\
      \ str]]): Storage for generated instructions.\n        question_mapping (Dict[str,\
      \ str]): Mapping of question types to keys in file details.\n        use_llm\
      \ (bool): Flag indicating if a language model should be used.\n        llm (object):\
      \ The language model for generating responses.\n        prompt (str): The prompt\
      \ format for querying the language model.\n    Methods:\n        clean_and_get_unique_elements(input_str:\
      \ str) -> str: \n            Clean and return unique elements from an input\
      \ string.\n        add_to_list(list_to_update: List[Dict], query: str, response:\
      \ str, additional_field=None) -> List[Dict]: \n            Add response to the\
      \ instruct list.\n        get_response_from_llm(query: str, context: str) ->\
      \ str:\n            Get language model response to query for given context.\n\
      \        process_question(question_type: str, question_id: str, query: str,\
      \ context: str, info: Dict) -> None:\n            Process question and add generated\
      \ response to the instruct_list.\n        process_question_type(question_type:\
      \ str, question_id: str, question_text: str) -> None:\n            Process question\
      \ related to file, function, class, or method.\n        generate() -> Tuple[List[Dict],\
      \ List[Dict]]:\n            Generate responses for all the questions and return\
      \ the instruct_list.\n    "
    class_inputs: null
    class_defaults: null
    class_returns:
    - ''', ''.join(cleaned_elements)'
    - response
    - ''''''
    - self.instruct_list
    - (full_context, prompt, context_size)
    - ''''''
    - ''', ''.join(items)'
    class_calls:
    - set
    - re.sub
    - element.strip
    - re.sub('\\s+', ' ', input_str).split
    - ''', ''.join'
    - self.model_config['prompt_template'].format
    - len
    - self.llm.tokenize
    - '''\n''.join'
    - any
    - item['instruction'].startswith
    - str
    - self.get_string_from_info
    - strategy
    - get_context_and_prompt
    - logger.error
    - math.ceil
    - self.llm
    - logging.info
    - question_id.endswith
    - info.get
    - self.get_response_from_llm
    - self.clean_and_get_unique_elements
    - str(response).strip
    - self.instruct_list.append
    - item.strip
    - str(info[item_type]).split
    - question_text.format
    - self.process_question
    - self.file_details['classes'].items
    - class_info.items
    - key.startswith
    - self.file_details[self.question_mapping[question_type]].items
    - self.process_question_type
    class_call_inputs:
      set:
      - (re.sub('[^\\w\\-_>\\s:/.]', '', element.strip()) for element in re.sub('\\s+',
        ' ', input_str).split(','))
      re.sub:
      - '''\\n\\s*\\n'''
      - '''\n\n'''
      - self.llm(prompt)
      element.strip: []
      re.sub('\\s+', ' ', input_str).split:
      - ''','''
      ''', ''.join':
      - '[s for s in [variables_string, inputs_string] if s]'
      self.model_config['prompt_template'].format: []
      len:
      - '''class_method_'''
      self.llm.tokenize:
      - prompt
      '''\n''.join':
      - '[f"Q: {item[''instruction'']} \nA: {item[''output'']}" for item in self.instruct_list
        if not any((item[''instruction''].startswith(prefix) for prefix in excluded_instructions))]'
      any:
      - (item['instruction'].startswith(prefix) for prefix in excluded_instructions)
      item['instruction'].startswith:
      - prefix
      str:
      - info[item_type]
      self.get_string_from_info:
      - info
      - f'{question_type}_methods'
      strategy: []
      get_context_and_prompt:
      - query
      - context
      - code_qa
      logger.error:
      - '''Failed to generate model response'''
      math.ceil:
      - context_size / 0.7
      self.llm:
      - prompt
      logging.info:
      - 'f''Response: {response}'''
      question_id.endswith:
      - '''purpose'''
      info.get:
      - question_id
      - ''''''
      self.get_response_from_llm:
      - query
      - context
      self.clean_and_get_unique_elements:
      - combined_string
      str(response).strip: []
      self.instruct_list.append:
      - '{''instruction'': query, ''input'': context, ''output'': response_str}'
      item.strip: []
      str(info[item_type]).split:
      - ''','''
      question_text.format: []
      self.process_question:
      - question_type
      - question_id
      - query
      - context
      - info
      self.file_details['classes'].items: []
      class_info.items: []
      key.startswith:
      - '''class_method_'''
      self.file_details[self.question_mapping[question_type]].items: []
      self.process_question_type:
      - question['type']
      - question['id']
      - question['text']
    class_variables:
    - combined_string
    - mapping
    - response_str
    - variables_string
    - excluded_instructions
    - context_size
    - context
    - items
    - methods_string
    - method_name
    - max_context_length
    - code_qa
    - context_strategies
    - response
    - query
    - inputs_string
    - prompt
    - full_context
    - info
    - cleaned_elements
    class_decorators: []
    class_annotations: []
    class_properties:
    - self.use_llm
    - self.base_name
    - self.file_path
    - self.question_mapping
    - self.llm
    - self.file_details
    - self.instruct_list
    - self.model_config
    - self.questions
    class_attributes:
    - file_path
    - file_details
    - base_name
    - questions
    - model_config
    - llm
    - instruct_list
    - question_mapping
    - use_llm
    - use_llm
    class_methods:
    - clean_and_get_unique_elements
    - get_response_from_llm
    - process_question
    - get_string_from_info
    - process_question_type
    - generate
    class_inheritance: []
    class_static_methods:
    - clean_and_get_unique_elements
    - get_string_from_info
    class_method___init__:
      method_name: __init__
      method_code: "def __init__(self, file_path: str, file_details: Dict, base_name:\
        \ str, questions: List[Dict], model_config: Dict) -> None:\n    self.file_path\
        \ = file_path\n    self.file_details = file_details\n    self.base_name =\
        \ base_name\n    self.questions = questions\n    self.model_config = model_config\n\
        \    self.llm = model_config['model']\n    if self.llm is None:\n        self.use_llm\
        \ = False\n    else:\n        self.use_llm = True\n    self.instruct_list\
        \ = []\n    self.question_mapping = {'file': 'file', 'function': 'functions',\
        \ 'class': 'classes', 'method': 'classes'}"
      method_ast: FunctionDef(name='__init__', args=arguments(posonlyargs=[], args=[arg(arg='self'),
        arg(arg='file_path', annotation=Name(id='str', ctx=Load())), arg(arg='file_details',
        annotation=Name(id='Dict', ctx=Load())), arg(arg='base_name', annotation=Name(id='str',
        ctx=Load())), arg(arg='questions', annotation=Subscript(value=Name(id='List',
        ctx=Load()), slice=Name(id='Dict', ctx=Load()), ctx=Load())), arg(arg='model_config',
        annotation=Name(id='Dict', ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]),
        body=[Assign(targets=[Attribute(value=Name(id='self', ctx=Load()), attr='file_path',
        ctx=Store())], value=Name(id='file_path', ctx=Load())), Assign(targets=[Attribute(value=Name(id='self',
        ctx=Load()), attr='file_details', ctx=Store())], value=Name(id='file_details',
        ctx=Load())), Assign(targets=[Attribute(value=Name(id='self', ctx=Load()),
        attr='base_name', ctx=Store())], value=Name(id='base_name', ctx=Load())),
        Assign(targets=[Attribute(value=Name(id='self', ctx=Load()), attr='questions',
        ctx=Store())], value=Name(id='questions', ctx=Load())), Assign(targets=[Attribute(value=Name(id='self',
        ctx=Load()), attr='model_config', ctx=Store())], value=Name(id='model_config',
        ctx=Load())), Assign(targets=[Attribute(value=Name(id='self', ctx=Load()),
        attr='llm', ctx=Store())], value=Subscript(value=Name(id='model_config', ctx=Load()),
        slice=Constant(value='model'), ctx=Load())), If(test=Compare(left=Attribute(value=Name(id='self',
        ctx=Load()), attr='llm', ctx=Load()), ops=[Is()], comparators=[Constant(value=None)]),
        body=[Assign(targets=[Attribute(value=Name(id='self', ctx=Load()), attr='use_llm',
        ctx=Store())], value=Constant(value=False))], orelse=[Assign(targets=[Attribute(value=Name(id='self',
        ctx=Load()), attr='use_llm', ctx=Store())], value=Constant(value=True))]),
        Assign(targets=[Attribute(value=Name(id='self', ctx=Load()), attr='instruct_list',
        ctx=Store())], value=List(elts=[], ctx=Load())), Assign(targets=[Attribute(value=Name(id='self',
        ctx=Load()), attr='question_mapping', ctx=Store())], value=Dict(keys=[Constant(value='file'),
        Constant(value='function'), Constant(value='class'), Constant(value='method')],
        values=[Constant(value='file'), Constant(value='functions'), Constant(value='classes'),
        Constant(value='classes')]))], decorator_list=[], returns=Constant(value=None))
      method_docstring: null
      method_inputs:
      - self
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      method_defaults: []
      method_returns: []
      method_calls: []
      method_call_inputs: {}
      method_variables: []
      method_decorators: []
      method_annotations: []
      method_properties:
      - self.use_llm
      - self.base_name
      - self.file_path
      - self.question_mapping
      - self.llm
      - self.file_details
      - self.instruct_list
      - self.model_config
      - self.questions
    class_method_clean_and_get_unique_elements:
      method_name: clean_and_get_unique_elements
      method_code: "@staticmethod\ndef clean_and_get_unique_elements(input_str: str)\
        \ -> str:\n    \"\"\"\n        Clean input string and return string of unique\
        \ elements.\n        Args:\n            input_str (str): The input string\
        \ to be cleaned.\n        Returns:\n            str: The cleaned string.\n\
        \        \"\"\"\n    cleaned_elements = set((re.sub('[^\\\\w\\\\-_>\\\\s:/.]',\
        \ '', element.strip()) for element in re.sub('\\\\s+', ' ', input_str).split(',')))\n\
        \    return ', '.join(cleaned_elements)"
      method_ast: 'FunctionDef(name=''clean_and_get_unique_elements'', args=arguments(posonlyargs=[],
        args=[arg(arg=''input_str'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
        kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Clean
        input string and return string of unique elements.\n        Args:\n            input_str
        (str): The input string to be cleaned.\n        Returns:\n            str:
        The cleaned string.\n        '')), Assign(targets=[Name(id=''cleaned_elements'',
        ctx=Store())], value=Call(func=Name(id=''set'', ctx=Load()), args=[GeneratorExp(elt=Call(func=Attribute(value=Name(id=''re'',
        ctx=Load()), attr=''sub'', ctx=Load()), args=[Constant(value=''[^\\w\\-_>\\s:/.]''),
        Constant(value=''''), Call(func=Attribute(value=Name(id=''element'', ctx=Load()),
        attr=''strip'', ctx=Load()), args=[], keywords=[])], keywords=[]), generators=[comprehension(target=Name(id=''element'',
        ctx=Store()), iter=Call(func=Attribute(value=Call(func=Attribute(value=Name(id=''re'',
        ctx=Load()), attr=''sub'', ctx=Load()), args=[Constant(value=''\\s+''), Constant(value=''
        ''), Name(id=''input_str'', ctx=Load())], keywords=[]), attr=''split'', ctx=Load()),
        args=[Constant(value='','')], keywords=[]), ifs=[], is_async=0)])], keywords=[])),
        Return(value=Call(func=Attribute(value=Constant(value='', ''), attr=''join'',
        ctx=Load()), args=[Name(id=''cleaned_elements'', ctx=Load())], keywords=[]))],
        decorator_list=[Name(id=''staticmethod'', ctx=Load())], returns=Name(id=''str'',
        ctx=Load()))'
      method_docstring: "\n        Clean input string and return string of unique\
        \ elements.\n        Args:\n            input_str (str): The input string\
        \ to be cleaned.\n        Returns:\n            str: The cleaned string.\n\
        \        "
      method_inputs:
      - input_str
      method_defaults: []
      method_returns:
      - ''', ''.join(cleaned_elements)'
      method_calls:
      - set
      - re.sub
      - element.strip
      - re.sub('\\s+', ' ', input_str).split
      - ''', ''.join'
      method_call_inputs:
        set:
        - (re.sub('[^\\w\\-_>\\s:/.]', '', element.strip()) for element in re.sub('\\s+',
          ' ', input_str).split(','))
        re.sub:
        - '''\\s+'''
        - ''' '''
        - input_str
        element.strip: []
        re.sub('\\s+', ' ', input_str).split:
        - ''','''
        ''', ''.join':
        - cleaned_elements
      method_variables:
      - cleaned_elements
      method_decorators:
      - staticmethod
      method_annotations: []
      method_properties: []
    class_method_get_response_from_llm:
      method_name: get_response_from_llm
      method_code: "def get_response_from_llm(self, query: str, context: str) -> str:\n\
        \    \"\"\"\n        Get language model response to query for given context.\n\
        \        Args:\n            query (str): The query to be used for generating\
        \ the response.\n            context (str): The context to be used for generating\
        \ the response.\n        Returns:\n            str: The generated response.\n\
        \        \"\"\"\n\n    def get_context_and_prompt(query, context, code_qa):\n\
        \        full_context = f'{context}\\nCODE Q and A:\\n{code_qa}'\n       \
        \ prompt = self.model_config['prompt_template'].format(context=full_context,\
        \ query=query)\n        context_size = len(self.llm.tokenize(prompt))\n  \
        \      return (full_context, prompt, context_size)\n    max_context_length\
        \ = self.model_config['inference_model']['model_params']['context_length']\n\
        \    excluded_instructions = ['Call code graph', 'Docstring']\n    code_qa\
        \ = '\\n'.join([f\"Q: {item['instruction']} \\nA: {item['output']}\" for item\
        \ in self.instruct_list if not any((item['instruction'].startswith(prefix)\
        \ for prefix in excluded_instructions))])\n    context_strategies = [lambda:\
        \ '```python\\n' + str(context) + '\\n```', lambda: '```python\\n' + str(self.file_details['file_info']['file_code_simplified'])\
        \ + '\\n```', lambda: self.get_string_from_info(self.file_details['file_info'],\
        \ 'file_summary'), lambda: '']\n    for strategy in context_strategies:\n\
        \        context = strategy()\n        full_context, prompt, context_size\
        \ = get_context_and_prompt(query, context, code_qa)\n        if context_size\
        \ <= 0.7 * max_context_length:\n            break\n    else:\n        logger.error(f'Failed\
        \ to generate model response, adjust context_length > {math.ceil(context_size\
        \ / 0.7)} in py2dataset_model_config.yaml')\n        return ''\n    try:\n\
        \        response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n\
        \        logging.info(f'Response: {response}')\n    except:\n        logger.error('Failed\
        \ to generate model response')\n        response = ''\n    return response"
      method_ast: 'FunctionDef(name=''get_response_from_llm'', args=arguments(posonlyargs=[],
        args=[arg(arg=''self''), arg(arg=''query'', annotation=Name(id=''str'', ctx=Load())),
        arg(arg=''context'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
        kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Get
        language model response to query for given context.\n        Args:\n            query
        (str): The query to be used for generating the response.\n            context
        (str): The context to be used for generating the response.\n        Returns:\n            str:
        The generated response.\n        '')), FunctionDef(name=''get_context_and_prompt'',
        args=arguments(posonlyargs=[], args=[arg(arg=''query''), arg(arg=''context''),
        arg(arg=''code_qa'')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id=''full_context'',
        ctx=Store())], value=JoinedStr(values=[FormattedValue(value=Name(id=''context'',
        ctx=Load()), conversion=-1), Constant(value=''\nCODE Q and A:\n''), FormattedValue(value=Name(id=''code_qa'',
        ctx=Load()), conversion=-1)])), Assign(targets=[Name(id=''prompt'', ctx=Store())],
        value=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''model_config'', ctx=Load()), slice=Constant(value=''prompt_template''),
        ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''context'',
        value=Name(id=''full_context'', ctx=Load())), keyword(arg=''query'', value=Name(id=''query'',
        ctx=Load()))])), Assign(targets=[Name(id=''context_size'', ctx=Store())],
        value=Call(func=Name(id=''len'', ctx=Load()), args=[Call(func=Attribute(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''llm'', ctx=Load()), attr=''tokenize'', ctx=Load()), args=[Name(id=''prompt'',
        ctx=Load())], keywords=[])], keywords=[])), Return(value=Tuple(elts=[Name(id=''full_context'',
        ctx=Load()), Name(id=''prompt'', ctx=Load()), Name(id=''context_size'', ctx=Load())],
        ctx=Load()))], decorator_list=[]), Assign(targets=[Name(id=''max_context_length'',
        ctx=Store())], value=Subscript(value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''model_config'', ctx=Load()), slice=Constant(value=''inference_model''),
        ctx=Load()), slice=Constant(value=''model_params''), ctx=Load()), slice=Constant(value=''context_length''),
        ctx=Load())), Assign(targets=[Name(id=''excluded_instructions'', ctx=Store())],
        value=List(elts=[Constant(value=''Call code graph''), Constant(value=''Docstring'')],
        ctx=Load())), Assign(targets=[Name(id=''code_qa'', ctx=Store())], value=Call(func=Attribute(value=Constant(value=''\n''),
        attr=''join'', ctx=Load()), args=[ListComp(elt=JoinedStr(values=[Constant(value=''Q:
        ''), FormattedValue(value=Subscript(value=Name(id=''item'', ctx=Load()), slice=Constant(value=''instruction''),
        ctx=Load()), conversion=-1), Constant(value='' \nA: ''), FormattedValue(value=Subscript(value=Name(id=''item'',
        ctx=Load()), slice=Constant(value=''output''), ctx=Load()), conversion=-1)]),
        generators=[comprehension(target=Name(id=''item'', ctx=Store()), iter=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''instruct_list'', ctx=Load()), ifs=[UnaryOp(op=Not(), operand=Call(func=Name(id=''any'',
        ctx=Load()), args=[GeneratorExp(elt=Call(func=Attribute(value=Subscript(value=Name(id=''item'',
        ctx=Load()), slice=Constant(value=''instruction''), ctx=Load()), attr=''startswith'',
        ctx=Load()), args=[Name(id=''prefix'', ctx=Load())], keywords=[]), generators=[comprehension(target=Name(id=''prefix'',
        ctx=Store()), iter=Name(id=''excluded_instructions'', ctx=Load()), ifs=[],
        is_async=0)])], keywords=[]))], is_async=0)])], keywords=[])), Assign(targets=[Name(id=''context_strategies'',
        ctx=Store())], value=List(elts=[Lambda(args=arguments(posonlyargs=[], args=[],
        kwonlyargs=[], kw_defaults=[], defaults=[]), body=BinOp(left=BinOp(left=Constant(value=''```python\n''),
        op=Add(), right=Call(func=Name(id=''str'', ctx=Load()), args=[Name(id=''context'',
        ctx=Load())], keywords=[])), op=Add(), right=Constant(value=''\n```''))),
        Lambda(args=arguments(posonlyargs=[], args=[], kwonlyargs=[], kw_defaults=[],
        defaults=[]), body=BinOp(left=BinOp(left=Constant(value=''```python\n''),
        op=Add(), right=Call(func=Name(id=''str'', ctx=Load()), args=[Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
        ctx=Load()), slice=Constant(value=''file_code_simplified''), ctx=Load())],
        keywords=[])), op=Add(), right=Constant(value=''\n```''))), Lambda(args=arguments(posonlyargs=[],
        args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=Call(func=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''get_string_from_info'', ctx=Load()), args=[Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
        ctx=Load()), Constant(value=''file_summary'')], keywords=[])), Lambda(args=arguments(posonlyargs=[],
        args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=Constant(value=''''))],
        ctx=Load())), For(target=Name(id=''strategy'', ctx=Store()), iter=Name(id=''context_strategies'',
        ctx=Load()), body=[Assign(targets=[Name(id=''context'', ctx=Store())], value=Call(func=Name(id=''strategy'',
        ctx=Load()), args=[], keywords=[])), Assign(targets=[Tuple(elts=[Name(id=''full_context'',
        ctx=Store()), Name(id=''prompt'', ctx=Store()), Name(id=''context_size'',
        ctx=Store())], ctx=Store())], value=Call(func=Name(id=''get_context_and_prompt'',
        ctx=Load()), args=[Name(id=''query'', ctx=Load()), Name(id=''context'', ctx=Load()),
        Name(id=''code_qa'', ctx=Load())], keywords=[])), If(test=Compare(left=Name(id=''context_size'',
        ctx=Load()), ops=[LtE()], comparators=[BinOp(left=Constant(value=0.7), op=Mult(),
        right=Name(id=''max_context_length'', ctx=Load()))]), body=[Break()], orelse=[])],
        orelse=[Expr(value=Call(func=Attribute(value=Name(id=''logger'', ctx=Load()),
        attr=''error'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Failed
        to generate model response, adjust context_length > ''), FormattedValue(value=Call(func=Attribute(value=Name(id=''math'',
        ctx=Load()), attr=''ceil'', ctx=Load()), args=[BinOp(left=Name(id=''context_size'',
        ctx=Load()), op=Div(), right=Constant(value=0.7))], keywords=[]), conversion=-1),
        Constant(value='' in py2dataset_model_config.yaml'')])], keywords=[])), Return(value=Constant(value=''''))]),
        Try(body=[Assign(targets=[Name(id=''response'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''re'',
        ctx=Load()), attr=''sub'', ctx=Load()), args=[Constant(value=''\\n\\s*\\n''),
        Constant(value=''\n\n''), Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
        attr=''llm'', ctx=Load()), args=[Name(id=''prompt'', ctx=Load())], keywords=[])],
        keywords=[])), Expr(value=Call(func=Attribute(value=Name(id=''logging'', ctx=Load()),
        attr=''info'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Response:
        ''), FormattedValue(value=Name(id=''response'', ctx=Load()), conversion=-1)])],
        keywords=[]))], handlers=[ExceptHandler(body=[Expr(value=Call(func=Attribute(value=Name(id=''logger'',
        ctx=Load()), attr=''error'', ctx=Load()), args=[Constant(value=''Failed to
        generate model response'')], keywords=[])), Assign(targets=[Name(id=''response'',
        ctx=Store())], value=Constant(value=''''))])], orelse=[], finalbody=[]), Return(value=Name(id=''response'',
        ctx=Load()))], decorator_list=[], returns=Name(id=''str'', ctx=Load()))'
      method_docstring: "\n        Get language model response to query for given\
        \ context.\n        Args:\n            query (str): The query to be used for\
        \ generating the response.\n            context (str): The context to be used\
        \ for generating the response.\n        Returns:\n            str: The generated\
        \ response.\n        "
      method_inputs:
      - self
      - query
      - context
      method_defaults: []
      method_returns:
      - response
      - (full_context, prompt, context_size)
      - ''''''
      method_calls:
      - self.model_config['prompt_template'].format
      - len
      - self.llm.tokenize
      - '''\n''.join'
      - any
      - item['instruction'].startswith
      - str
      - self.get_string_from_info
      - strategy
      - get_context_and_prompt
      - logger.error
      - math.ceil
      - re.sub
      - self.llm
      - logging.info
      method_call_inputs:
        self.model_config['prompt_template'].format: []
        len:
        - self.llm.tokenize(prompt)
        self.llm.tokenize:
        - prompt
        '''\n''.join':
        - '[f"Q: {item[''instruction'']} \nA: {item[''output'']}" for item in self.instruct_list
          if not any((item[''instruction''].startswith(prefix) for prefix in excluded_instructions))]'
        any:
        - (item['instruction'].startswith(prefix) for prefix in excluded_instructions)
        item['instruction'].startswith:
        - prefix
        str:
        - self.file_details['file_info']['file_code_simplified']
        self.get_string_from_info:
        - self.file_details['file_info']
        - '''file_summary'''
        strategy: []
        get_context_and_prompt:
        - query
        - context
        - code_qa
        logger.error:
        - '''Failed to generate model response'''
        math.ceil:
        - context_size / 0.7
        re.sub:
        - '''\\n\\s*\\n'''
        - '''\n\n'''
        - self.llm(prompt)
        self.llm:
        - prompt
        logging.info:
        - 'f''Response: {response}'''
      method_variables:
      - response
      - context_size
      - excluded_instructions
      - context
      - max_context_length
      - prompt
      - full_context
      - code_qa
      - context_strategies
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_get_context_and_prompt:
      method_name: get_context_and_prompt
      method_code: "def get_context_and_prompt(query, context, code_qa):\n    full_context\
        \ = f'{context}\\nCODE Q and A:\\n{code_qa}'\n    prompt = self.model_config['prompt_template'].format(context=full_context,\
        \ query=query)\n    context_size = len(self.llm.tokenize(prompt))\n    return\
        \ (full_context, prompt, context_size)"
      method_ast: FunctionDef(name='get_context_and_prompt', args=arguments(posonlyargs=[],
        args=[arg(arg='query'), arg(arg='context'), arg(arg='code_qa')], kwonlyargs=[],
        kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id='full_context',
        ctx=Store())], value=JoinedStr(values=[FormattedValue(value=Name(id='context',
        ctx=Load()), conversion=-1), Constant(value='\nCODE Q and A:\n'), FormattedValue(value=Name(id='code_qa',
        ctx=Load()), conversion=-1)])), Assign(targets=[Name(id='prompt', ctx=Store())],
        value=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id='self',
        ctx=Load()), attr='model_config', ctx=Load()), slice=Constant(value='prompt_template'),
        ctx=Load()), attr='format', ctx=Load()), args=[], keywords=[keyword(arg='context',
        value=Name(id='full_context', ctx=Load())), keyword(arg='query', value=Name(id='query',
        ctx=Load()))])), Assign(targets=[Name(id='context_size', ctx=Store())], value=Call(func=Name(id='len',
        ctx=Load()), args=[Call(func=Attribute(value=Attribute(value=Name(id='self',
        ctx=Load()), attr='llm', ctx=Load()), attr='tokenize', ctx=Load()), args=[Name(id='prompt',
        ctx=Load())], keywords=[])], keywords=[])), Return(value=Tuple(elts=[Name(id='full_context',
        ctx=Load()), Name(id='prompt', ctx=Load()), Name(id='context_size', ctx=Load())],
        ctx=Load()))], decorator_list=[])
      method_docstring: null
      method_inputs:
      - query
      - context
      - code_qa
      method_defaults: []
      method_returns:
      - (full_context, prompt, context_size)
      method_calls:
      - self.model_config['prompt_template'].format
      - len
      - self.llm.tokenize
      method_call_inputs:
        self.model_config['prompt_template'].format: []
        len:
        - self.llm.tokenize(prompt)
        self.llm.tokenize:
        - prompt
      method_variables:
      - context_size
      - prompt
      - full_context
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_process_question:
      method_name: process_question
      method_code: "def process_question(self, question_type: str, question_id: str,\
        \ query: str, context: str, info: Dict) -> None:\n    \"\"\"\n        Process\
        \ question and add the generated response to the instruct_list.\n        Args:\n\
        \            question_type (str): The type of question to be processed.\n\
        \            question_id (str): The ID of the question to be processed.\n\
        \            query (str): The query to be processed.\n            context\
        \ (str): The context to be used for generating the response.\n           \
        \ info (Dict): The information of the Python file.\n        Returns:\n   \
        \         None\n        \"\"\"\n    if question_id.endswith('code_graph')\
        \ or question_id.endswith('docstring'):\n        response = info.get(question_id,\
        \ {})\n    else:\n        response = self.get_response_from_llm(query, context)\
        \ if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id,\
        \ '')))\n    if question_type == 'file':\n        context = self.file_details['file_info']['file_code']\n\
        \    if response and response != 'None':\n        response_str = str(response).strip()\n\
        \        if response_str:\n            self.instruct_list.append({'instruction':\
        \ query, 'input': context, 'output': response_str})"
      method_ast: 'FunctionDef(name=''process_question'', args=arguments(posonlyargs=[],
        args=[arg(arg=''self''), arg(arg=''question_type'', annotation=Name(id=''str'',
        ctx=Load())), arg(arg=''question_id'', annotation=Name(id=''str'', ctx=Load())),
        arg(arg=''query'', annotation=Name(id=''str'', ctx=Load())), arg(arg=''context'',
        annotation=Name(id=''str'', ctx=Load())), arg(arg=''info'', annotation=Name(id=''Dict'',
        ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Process
        question and add the generated response to the instruct_list.\n        Args:\n            question_type
        (str): The type of question to be processed.\n            question_id (str):
        The ID of the question to be processed.\n            query (str): The query
        to be processed.\n            context (str): The context to be used for generating
        the response.\n            info (Dict): The information of the Python file.\n        Returns:\n            None\n        '')),
        If(test=BoolOp(op=Or(), values=[Call(func=Attribute(value=Name(id=''question_id'',
        ctx=Load()), attr=''endswith'', ctx=Load()), args=[Constant(value=''code_graph'')],
        keywords=[]), Call(func=Attribute(value=Name(id=''question_id'', ctx=Load()),
        attr=''endswith'', ctx=Load()), args=[Constant(value=''docstring'')], keywords=[])]),
        body=[Assign(targets=[Name(id=''response'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''info'',
        ctx=Load()), attr=''get'', ctx=Load()), args=[Name(id=''question_id'', ctx=Load()),
        Dict(keys=[], values=[])], keywords=[]))], orelse=[Assign(targets=[Name(id=''response'',
        ctx=Store())], value=IfExp(test=BoolOp(op=And(), values=[Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''use_llm'', ctx=Load()), Call(func=Attribute(value=Name(id=''question_id'',
        ctx=Load()), attr=''endswith'', ctx=Load()), args=[Constant(value=''purpose'')],
        keywords=[])]), body=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
        attr=''get_response_from_llm'', ctx=Load()), args=[Name(id=''query'', ctx=Load()),
        Name(id=''context'', ctx=Load())], keywords=[]), orelse=Call(func=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''clean_and_get_unique_elements'', ctx=Load()), args=[Call(func=Name(id=''str'',
        ctx=Load()), args=[Call(func=Attribute(value=Name(id=''info'', ctx=Load()),
        attr=''get'', ctx=Load()), args=[Name(id=''question_id'', ctx=Load()), Constant(value='''')],
        keywords=[])], keywords=[])], keywords=[])))]), If(test=Compare(left=Name(id=''question_type'',
        ctx=Load()), ops=[Eq()], comparators=[Constant(value=''file'')]), body=[Assign(targets=[Name(id=''context'',
        ctx=Store())], value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
        ctx=Load()), slice=Constant(value=''file_code''), ctx=Load()))], orelse=[]),
        If(test=BoolOp(op=And(), values=[Name(id=''response'', ctx=Load()), Compare(left=Name(id=''response'',
        ctx=Load()), ops=[NotEq()], comparators=[Constant(value=''None'')])]), body=[Assign(targets=[Name(id=''response_str'',
        ctx=Store())], value=Call(func=Attribute(value=Call(func=Name(id=''str'',
        ctx=Load()), args=[Name(id=''response'', ctx=Load())], keywords=[]), attr=''strip'',
        ctx=Load()), args=[], keywords=[])), If(test=Name(id=''response_str'', ctx=Load()),
        body=[Expr(value=Call(func=Attribute(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''instruct_list'', ctx=Load()), attr=''append'', ctx=Load()),
        args=[Dict(keys=[Constant(value=''instruction''), Constant(value=''input''),
        Constant(value=''output'')], values=[Name(id=''query'', ctx=Load()), Name(id=''context'',
        ctx=Load()), Name(id=''response_str'', ctx=Load())])], keywords=[]))], orelse=[])],
        orelse=[])], decorator_list=[], returns=Constant(value=None))'
      method_docstring: "\n        Process question and add the generated response\
        \ to the instruct_list.\n        Args:\n            question_type (str): The\
        \ type of question to be processed.\n            question_id (str): The ID\
        \ of the question to be processed.\n            query (str): The query to\
        \ be processed.\n            context (str): The context to be used for generating\
        \ the response.\n            info (Dict): The information of the Python file.\n\
        \        Returns:\n            None\n        "
      method_inputs:
      - self
      - question_type
      - question_id
      - query
      - context
      - info
      method_defaults: []
      method_returns: []
      method_calls:
      - question_id.endswith
      - info.get
      - self.get_response_from_llm
      - self.clean_and_get_unique_elements
      - str
      - str(response).strip
      - self.instruct_list.append
      method_call_inputs:
        question_id.endswith:
        - '''purpose'''
        info.get:
        - question_id
        - ''''''
        self.get_response_from_llm:
        - query
        - context
        self.clean_and_get_unique_elements:
        - str(info.get(question_id, ''))
        str:
        - response
        str(response).strip: []
        self.instruct_list.append:
        - '{''instruction'': query, ''input'': context, ''output'': response_str}'
      method_variables:
      - response
      - context
      - response_str
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_get_string_from_info:
      method_name: get_string_from_info
      method_code: "@staticmethod\ndef get_string_from_info(info, item_type):\n  \
        \  if info[item_type]:\n        items = [item.strip() for item in str(info[item_type]).split(',')\
        \ if item]\n        return ', '.join(items)\n    return ''"
      method_ast: FunctionDef(name='get_string_from_info', args=arguments(posonlyargs=[],
        args=[arg(arg='info'), arg(arg='item_type')], kwonlyargs=[], kw_defaults=[],
        defaults=[]), body=[If(test=Subscript(value=Name(id='info', ctx=Load()), slice=Name(id='item_type',
        ctx=Load()), ctx=Load()), body=[Assign(targets=[Name(id='items', ctx=Store())],
        value=ListComp(elt=Call(func=Attribute(value=Name(id='item', ctx=Load()),
        attr='strip', ctx=Load()), args=[], keywords=[]), generators=[comprehension(target=Name(id='item',
        ctx=Store()), iter=Call(func=Attribute(value=Call(func=Name(id='str', ctx=Load()),
        args=[Subscript(value=Name(id='info', ctx=Load()), slice=Name(id='item_type',
        ctx=Load()), ctx=Load())], keywords=[]), attr='split', ctx=Load()), args=[Constant(value=',')],
        keywords=[]), ifs=[Name(id='item', ctx=Load())], is_async=0)])), Return(value=Call(func=Attribute(value=Constant(value=',
        '), attr='join', ctx=Load()), args=[Name(id='items', ctx=Load())], keywords=[]))],
        orelse=[]), Return(value=Constant(value=''))], decorator_list=[Name(id='staticmethod',
        ctx=Load())])
      method_docstring: null
      method_inputs:
      - info
      - item_type
      method_defaults: []
      method_returns:
      - ''''''
      - ''', ''.join(items)'
      method_calls:
      - item.strip
      - str(info[item_type]).split
      - str
      - ''', ''.join'
      method_call_inputs:
        item.strip: []
        str(info[item_type]).split:
        - ''','''
        str:
        - info[item_type]
        ''', ''.join':
        - items
      method_variables:
      - items
      method_decorators:
      - staticmethod
      method_annotations: []
      method_properties: []
    class_method_process_question_type:
      method_name: process_question_type
      method_code: "def process_question_type(self, question_type: str, question_id:\
        \ str, question_text: str) -> None:\n    \"\"\"\n        Process questions\
        \ related to a file, function, class, or method.\n        Args:\n        \
        \    question_type (str): The type of question to be processed.\n        \
        \    question_id (str): The ID of the question to be processed.\n        \
        \    question_text (str): The text of the question to be processed.\n    \
        \    Returns:\n            None\n        \"\"\"\n    if question_type == 'file':\n\
        \        query = question_text.format(filename=self.base_name)\n        info\
        \ = self.file_details['file_info']\n        context = self.file_details['file_info']['file_code']\n\
        \        self.process_question(question_type, question_id, query, context,\
        \ info)\n    elif question_type == 'method':\n        for class_name, class_info\
        \ in self.file_details['classes'].items():\n            for key, method_info\
        \ in class_info.items():\n                if key.startswith('class_method_'):\n\
        \                    method_name = key[len('class_method_'):]\n          \
        \          context = method_info['method_code']\n                    mapping\
        \ = {'class_name': class_name, 'method_name': method_name}\n             \
        \       query = question_text.format(filename=self.base_name, **mapping)\n\
        \                    self.process_question(question_type, question_id, query,\
        \ context, method_info)\n    else:\n        for name, info in self.file_details[self.question_mapping[question_type]].items():\n\
        \            context = info[f'{question_type}_code']\n            mapping\
        \ = {f'{question_type}_name': name}\n            if question_id == f'{question_type}_purpose'\
        \ and self.use_llm:\n                variables_string = self.get_string_from_info(info,\
        \ f'{question_type}_variables')\n                inputs_string = self.get_string_from_info(info,\
        \ f'{question_type}_inputs')\n                combined_string = ', '.join([s\
        \ for s in [variables_string, inputs_string] if s])\n                mapping[f'{question_type}_variables']\
        \ = self.clean_and_get_unique_elements(combined_string)\n                if\
        \ question_type == 'class':\n                    methods_string = self.get_string_from_info(info,\
        \ f'{question_type}_methods')\n                    mapping[f'{question_type}_methods']\
        \ = methods_string\n            query = question_text.format(filename=self.base_name,\
        \ **mapping)\n            self.process_question(question_type, question_id,\
        \ query, context, info)"
      method_ast: 'FunctionDef(name=''process_question_type'', args=arguments(posonlyargs=[],
        args=[arg(arg=''self''), arg(arg=''question_type'', annotation=Name(id=''str'',
        ctx=Load())), arg(arg=''question_id'', annotation=Name(id=''str'', ctx=Load())),
        arg(arg=''question_text'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
        kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Process
        questions related to a file, function, class, or method.\n        Args:\n            question_type
        (str): The type of question to be processed.\n            question_id (str):
        The ID of the question to be processed.\n            question_text (str):
        The text of the question to be processed.\n        Returns:\n            None\n        '')),
        If(test=Compare(left=Name(id=''question_type'', ctx=Load()), ops=[Eq()], comparators=[Constant(value=''file'')]),
        body=[Assign(targets=[Name(id=''query'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''question_text'',
        ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''filename'',
        value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'', ctx=Load()))])),
        Assign(targets=[Name(id=''info'', ctx=Store())], value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
        ctx=Load())), Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
        ctx=Load()), slice=Constant(value=''file_code''), ctx=Load())), Expr(value=Call(func=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'',
        ctx=Load()), Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()),
        Name(id=''context'', ctx=Load()), Name(id=''info'', ctx=Load())], keywords=[]))],
        orelse=[If(test=Compare(left=Name(id=''question_type'', ctx=Load()), ops=[Eq()],
        comparators=[Constant(value=''method'')]), body=[For(target=Tuple(elts=[Name(id=''class_name'',
        ctx=Store()), Name(id=''class_info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''classes''),
        ctx=Load()), attr=''items'', ctx=Load()), args=[], keywords=[]), body=[For(target=Tuple(elts=[Name(id=''key'',
        ctx=Store()), Name(id=''method_info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Name(id=''class_info'',
        ctx=Load()), attr=''items'', ctx=Load()), args=[], keywords=[]), body=[If(test=Call(func=Attribute(value=Name(id=''key'',
        ctx=Load()), attr=''startswith'', ctx=Load()), args=[Constant(value=''class_method_'')],
        keywords=[]), body=[Assign(targets=[Name(id=''method_name'', ctx=Store())],
        value=Subscript(value=Name(id=''key'', ctx=Load()), slice=Slice(lower=Call(func=Name(id=''len'',
        ctx=Load()), args=[Constant(value=''class_method_'')], keywords=[])), ctx=Load())),
        Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Name(id=''method_info'',
        ctx=Load()), slice=Constant(value=''method_code''), ctx=Load())), Assign(targets=[Name(id=''mapping'',
        ctx=Store())], value=Dict(keys=[Constant(value=''class_name''), Constant(value=''method_name'')],
        values=[Name(id=''class_name'', ctx=Load()), Name(id=''method_name'', ctx=Load())])),
        Assign(targets=[Name(id=''query'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''question_text'',
        ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''filename'',
        value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'', ctx=Load())),
        keyword(value=Name(id=''mapping'', ctx=Load()))])), Expr(value=Call(func=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'',
        ctx=Load()), Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()),
        Name(id=''context'', ctx=Load()), Name(id=''method_info'', ctx=Load())], keywords=[]))],
        orelse=[])], orelse=[])], orelse=[])], orelse=[For(target=Tuple(elts=[Name(id=''name'',
        ctx=Store()), Name(id=''info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Load()), slice=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''question_mapping'', ctx=Load()), slice=Name(id=''question_type'',
        ctx=Load()), ctx=Load()), ctx=Load()), attr=''items'', ctx=Load()), args=[],
        keywords=[]), body=[Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Name(id=''info'',
        ctx=Load()), slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
        ctx=Load()), conversion=-1), Constant(value=''_code'')]), ctx=Load())), Assign(targets=[Name(id=''mapping'',
        ctx=Store())], value=Dict(keys=[JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
        ctx=Load()), conversion=-1), Constant(value=''_name'')])], values=[Name(id=''name'',
        ctx=Load())])), If(test=BoolOp(op=And(), values=[Compare(left=Name(id=''question_id'',
        ctx=Load()), ops=[Eq()], comparators=[JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
        ctx=Load()), conversion=-1), Constant(value=''_purpose'')])]), Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''use_llm'', ctx=Load())]), body=[Assign(targets=[Name(id=''variables_string'',
        ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
        attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
        JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
        conversion=-1), Constant(value=''_variables'')])], keywords=[])), Assign(targets=[Name(id=''inputs_string'',
        ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
        attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
        JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
        conversion=-1), Constant(value=''_inputs'')])], keywords=[])), Assign(targets=[Name(id=''combined_string'',
        ctx=Store())], value=Call(func=Attribute(value=Constant(value='', ''), attr=''join'',
        ctx=Load()), args=[ListComp(elt=Name(id=''s'', ctx=Load()), generators=[comprehension(target=Name(id=''s'',
        ctx=Store()), iter=List(elts=[Name(id=''variables_string'', ctx=Load()), Name(id=''inputs_string'',
        ctx=Load())], ctx=Load()), ifs=[Name(id=''s'', ctx=Load())], is_async=0)])],
        keywords=[])), Assign(targets=[Subscript(value=Name(id=''mapping'', ctx=Load()),
        slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
        conversion=-1), Constant(value=''_variables'')]), ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''clean_and_get_unique_elements'', ctx=Load()), args=[Name(id=''combined_string'',
        ctx=Load())], keywords=[])), If(test=Compare(left=Name(id=''question_type'',
        ctx=Load()), ops=[Eq()], comparators=[Constant(value=''class'')]), body=[Assign(targets=[Name(id=''methods_string'',
        ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
        attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
        JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
        conversion=-1), Constant(value=''_methods'')])], keywords=[])), Assign(targets=[Subscript(value=Name(id=''mapping'',
        ctx=Load()), slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
        ctx=Load()), conversion=-1), Constant(value=''_methods'')]), ctx=Store())],
        value=Name(id=''methods_string'', ctx=Load()))], orelse=[])], orelse=[]),
        Assign(targets=[Name(id=''query'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''question_text'',
        ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''filename'',
        value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'', ctx=Load())),
        keyword(value=Name(id=''mapping'', ctx=Load()))])), Expr(value=Call(func=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'',
        ctx=Load()), Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()),
        Name(id=''context'', ctx=Load()), Name(id=''info'', ctx=Load())], keywords=[]))],
        orelse=[])])])], decorator_list=[], returns=Constant(value=None))'
      method_docstring: "\n        Process questions related to a file, function,\
        \ class, or method.\n        Args:\n            question_type (str): The type\
        \ of question to be processed.\n            question_id (str): The ID of the\
        \ question to be processed.\n            question_text (str): The text of\
        \ the question to be processed.\n        Returns:\n            None\n    \
        \    "
      method_inputs:
      - self
      - question_type
      - question_id
      - question_text
      method_defaults: []
      method_returns: []
      method_calls:
      - question_text.format
      - self.process_question
      - self.file_details['classes'].items
      - class_info.items
      - key.startswith
      - len
      - self.file_details[self.question_mapping[question_type]].items
      - self.get_string_from_info
      - ''', ''.join'
      - self.clean_and_get_unique_elements
      method_call_inputs:
        question_text.format: []
        self.process_question:
        - question_type
        - question_id
        - query
        - context
        - info
        self.file_details['classes'].items: []
        class_info.items: []
        key.startswith:
        - '''class_method_'''
        len:
        - '''class_method_'''
        self.file_details[self.question_mapping[question_type]].items: []
        self.get_string_from_info:
        - info
        - f'{question_type}_methods'
        ''', ''.join':
        - '[s for s in [variables_string, inputs_string] if s]'
        self.clean_and_get_unique_elements:
        - combined_string
      method_variables:
      - variables_string
      - context
      - query
      - inputs_string
      - method_name
      - combined_string
      - methods_string
      - info
      - mapping
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_generate:
      method_name: generate
      method_code: "def generate(self) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\
        \"\n        Generate responses for all the questions and returns the instruct_list.\n\
        \        Args:\n            None\n        Returns:\n            Tuple[List[Dict],\
        \ List[Dict]]: The generated question-answer pairs and instructions.\n   \
        \     \"\"\"\n    for question in self.questions:\n        self.process_question_type(question['type'],\
        \ question['id'], question['text'])\n    return self.instruct_list"
      method_ast: 'FunctionDef(name=''generate'', args=arguments(posonlyargs=[], args=[arg(arg=''self'')],
        kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Generate
        responses for all the questions and returns the instruct_list.\n        Args:\n            None\n        Returns:\n            Tuple[List[Dict],
        List[Dict]]: The generated question-answer pairs and instructions.\n        '')),
        For(target=Name(id=''question'', ctx=Store()), iter=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''questions'', ctx=Load()), body=[Expr(value=Call(func=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''process_question_type'', ctx=Load()), args=[Subscript(value=Name(id=''question'',
        ctx=Load()), slice=Constant(value=''type''), ctx=Load()), Subscript(value=Name(id=''question'',
        ctx=Load()), slice=Constant(value=''id''), ctx=Load()), Subscript(value=Name(id=''question'',
        ctx=Load()), slice=Constant(value=''text''), ctx=Load())], keywords=[]))],
        orelse=[]), Return(value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''instruct_list'',
        ctx=Load()))], decorator_list=[], returns=Subscript(value=Name(id=''Tuple'',
        ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id=''List'', ctx=Load()),
        slice=Name(id=''Dict'', ctx=Load()), ctx=Load()), Subscript(value=Name(id=''List'',
        ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load())], ctx=Load()),
        ctx=Load()))'
      method_docstring: "\n        Generate responses for all the questions and returns\
        \ the instruct_list.\n        Args:\n            None\n        Returns:\n\
        \            Tuple[List[Dict], List[Dict]]: The generated question-answer\
        \ pairs and instructions.\n        "
      method_inputs:
      - self
      method_defaults: []
      method_returns:
      - self.instruct_list
      method_calls:
      - self.process_question_type
      method_call_inputs:
        self.process_question_type:
        - question['type']
        - question['id']
        - question['text']
      method_variables: []
      method_decorators: []
      method_annotations: []
      method_properties: []
