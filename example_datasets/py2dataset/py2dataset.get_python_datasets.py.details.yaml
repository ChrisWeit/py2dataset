file_info:
  file_code: "\"\"\"\nGenerates JSON format question-answer pairs and instructions\
    \ for a Python file\nRequirements:\n[req01] The `DatasetGenerator` class shall:\n\
    \        a. Accept Python file path, file details, base name, list of questions,\n\
    \        and model configuration as input during instantiation.\n        b. Initialize\
    \ and store Python file path, file details, base name,\n        question list,\
    \ llm, and use_llm flag as class attributes.\n        d. Provide `get_response_from_llm`\
    \ method to retrieve llm response.\n        e. Provide `process_question` method\
    \ to process each question, generate\n        corresponding responses, and add\
    \ to the instruct_list.\n        f. Provide `process_question_type` method to\
    \ process questions\n        related to the file, functions, classes, and methods.\n\
    \        g. Provide `generate` method to generate responses for all questions\n\
    \        and return the instruct_list.\n        h. Internally manage question\
    \ mapping to file details.\n[req02] The `get_python_datasets` function shall:\n\
    \        a. Accept a Python file path, file details, base name, questions list,\n\
    \        and model config as input.\n        b. Instantiate `DatasetGenerator`\
    \ class using the provided input.\n        c. Generate instruct_list using `DatasetGenerator`\
    \ `generate` method.\n        d. Return the generated `instruct_list`.\n[req03]\
    \ The `clean_and_get_unique_elements` function shall:\n        a. Clean an input\
    \ string (str) and return a string of unique elements.\n\"\"\"\nimport logging\n\
    import re\nimport math\nfrom typing import Dict, List, Tuple\n\ndef clean_and_get_unique_elements(input_str:\
    \ str) -> str:\n    \"\"\"\n    Clean input string and return string of unique\
    \ elements.\n    Args:\n        input_str (str): The input string to be cleaned.\n\
    \    Returns:\n        str: The cleaned string.\n    \"\"\"\n    cleaned_elements\
    \ = set(re.sub(r'[^\\w\\-_>\\s:/.]', '', element.strip())\n                  \
    \          for element in re.sub(r'\\s+', ' ', input_str).split(','))\n    return\
    \ ', '.join(cleaned_elements)\n\n\nclass DatasetGenerator:\n    \"\"\"\n    Generate\
    \ JSON formatted dictionary outputs for a Python file.\n    Attributes:\n    \
    \    file_path (str): The path to the Python file.\n        file_details (Dict[str,\
    \ Any]): Details of the Python file.\n        base_name (str): The base name of\
    \ the Python file.\n        questions (List[Dict[str, str]]): Questions for generating\
    \ responses.\n        instruct_list (List[Dict[str, str]]): Storage for generated\
    \ instructions.\n        question_mapping (Dict[str, str]): Mapping of question\
    \ types to keys in file details.\n        use_llm (bool): Flag indicating if a\
    \ language model should be used.\n        llm (object): The language model for\
    \ generating responses.\n        prompt (str): The prompt format for querying\
    \ the language model.\n    Methods:\n        add_to_list(list_to_update: List[Dict],\
    \ query: str, response: str, \n        additional_field=None) -> List[Dict]: \n\
    \            Add response to the instruct list.\n        get_response_from_llm(query:\
    \ str, context: str) -> str:\n            Get language model response to query\
    \ for given context.\n        process_question(question_type: str, question_id:\
    \ str, query: str, \n        context: str, info: Dict) -> None:\n            Process\
    \ question and add generated response to the instruct_list.\n        process_question_type(question_type:\
    \ str, question_id: str, \n        question_text: str) -> None:\n            Process\
    \ question related to file, function, class, or method.\n        generate() ->\
    \ Tuple[List[Dict], List[Dict]]:\n            Generate responses for all the questions\
    \ and return the instruct_list.\n    \"\"\"\n\n    def __init__(self, file_path:\
    \ str, file_details: Dict, base_name: str,\n                 questions: List[Dict],\
    \ model_config: Dict) -> None:\n        \"\"\"\n        Initialize the DatasetGenerator\
    \ class.\n        Args:\n            file_path (str): The path to the Python file.\n\
    \            file_details (Dict[str, Any]): Details of the Python file.\n    \
    \        base_name (str): The base name of the Python file.\n            questions\
    \ (List[Dict[str, str]]): Questions for generating responses.\n            model_config\
    \ (Dict): Configuration for the language model.\n        Returns:\n          \
    \  None\n        \"\"\"\n        self.file_path = file_path\n        self.file_details\
    \ = file_details\n        self.base_name = base_name\n        self.questions =\
    \ questions\n        self.model_config = model_config\n        self.llm = model_config['model']\n\
    \        if self.llm is None:\n            self.use_llm = False\n        else:\n\
    \            self.use_llm = True\n        self.instruct_list = []\n        self.question_mapping\
    \ = {\n            'file': 'file',\n            'function': 'functions',\n   \
    \         'class': 'classes',\n            'method': 'classes'\n        }\n\n\
    \    def add_to_list(\n        self,\n        list_to_update: List[Dict],\n  \
    \      query: str,\n        response: str,\n        additional_field=None\n  \
    \      ) -> List[Dict]:\n        \"\"\"\n        Add response to the instruct\
    \ list.\n        Args:\n            list_to_update (List[Dict]): The list to update.\n\
    \            query (str): The query to be added.\n            response (str):\
    \ The response to be added.\n            additional_field (Any): Additional field\
    \ to be added.\n        Returns:\n            List[Dict]: The updated list.\n\
    \        \"\"\"\n        list_to_update.append({'instruction': query, 'input':\
    \ additional_field, 'output': response})\n        return list_to_update\n\n  \
    \  def get_response_from_llm(self, query: str, context: str) -> str:\n       \
    \ \"\"\"\n        Get language model response to query for given context.\n  \
    \      Args:\n            query (str): The query to be used for generating the\
    \ response.\n            context (str): The context to be used for generating\
    \ the response.\n        Returns:\n            str: The generated response.\n\
    \        \"\"\"\n        def get_context_and_prompt(query, context, code_qa):\n\
    \            full_context = f\"{context}\\nCODE Q and A:\\n{code_qa}\"\n     \
    \       prompt = self.model_config['prompt_template'].format(context=full_context,\
    \ query=query)\n            context_size = len(self.llm.tokenize(prompt))\n  \
    \          return prompt, context_size\n\n        max_context_length = self.model_config['inference_model']['model_params']['context_length']\n\
    \        excluded_instructions = [\"Call code graph\", \"Docstring\"]\n      \
    \  code_qa = (\n            \"\\n\".join([f\"Q: {item['instruction']} \\nA: {item['output']}\"\
    \ \n            for item in self.instruct_list if not any(item['instruction'].startswith(prefix)\n\
    \            for prefix in excluded_instructions)])\n            )\n\n       \
    \ # manage context length for LLM starting with the longest and most comprehensive\n\
    \        context_strategies = [\n            lambda: '```python\\n' + str(context)\
    \ + '\\n```',\n            lambda: '```python\\n' + str(self.file_details['file_info']['file_code_simplified'])\
    \ + '\\n```',\n            lambda: self.get_string_from_info(self.file_details['file_info'],\
    \ 'file_summary'),\n            lambda: ''\n        ]\n        for strategy in\
    \ context_strategies:\n            context = strategy()\n            prompt, context_size\
    \ = get_context_and_prompt(query, context, code_qa)\n            if context_size\
    \ <= 0.70 * max_context_length:\n                break\n        else:\n      \
    \      logging.error(f'Model response failed, increase py2dataset_model_config.yaml\
    \ context_length > {math.ceil(context_size/0.70)}')\n            return ''\n\n\
    \        try:\n            response = re.sub(r'\\n\\s*\\n', '\\n\\n', self.llm(prompt))\n\
    \            logging.info(f'Response: {response}')\n        except Exception as\
    \ error:\n            logging.error(f'Failed to generate model response: {error}')\n\
    \            response = ''\n        return response\n\n    def process_question(self,\
    \ question_type: str, question_id: str, query: str,\n                        \
    \ context: str, info: Dict) -> None:\n        \"\"\"\n        Process question\
    \ and add the generated response to the instruct_list.\n        Args:\n      \
    \      question_type (str): The type of question to be processed.\n          \
    \  question_id (str): The ID of the question to be processed.\n            query\
    \ (str): The query to be processed.\n            context (str): The context to\
    \ be used for generating the response.\n            info (Dict): The information\
    \ of the Python file.\n        Returns:\n            None\n        \"\"\"\n  \
    \      if question_id.endswith('code_graph') or question_id.endswith('docstring'):\n\
    \            response = info.get(question_id, {})\n        else:\n           \
    \ response = (\n                self.get_response_from_llm(query, context)\n \
    \               if self.use_llm and question_id.endswith('purpose')\n        \
    \        else clean_and_get_unique_elements(str(info.get(question_id, '')))\n\
    \                )\n        if question_type == 'file':\n            context =\
    \ self.file_details['file_info']['file_code']\n        if response and response\
    \ != 'None':\n            response_str = str(response).strip()\n            if\
    \ response_str:\n                self.instruct_list.append({\n               \
    \     'instruction': query,\n                    'input': context,\n         \
    \           'output': response_str\n                })\n\n    @staticmethod\n\
    \    def get_string_from_info(info, item_type):\n        \"\"\"\n        Get string\
    \ from info dictionary.\n        Args:\n            info (Dict): The information\
    \ of the Python file.\n            item_type (str): The type of item to get the\
    \ string from.\n        Returns:\n            str: The string from the info.\n\
    \        \"\"\"\n        if info[item_type]:\n            items = [item.strip()\
    \ for item in str(info[item_type]).split(',') if item]\n            return ',\
    \ '.join(items)\n        return ''\n\n    def process_question_type(self, question_type:\
    \ str, question_id: str,\n                              question_text: str) ->\
    \ None:\n        \"\"\"\n        Process questions related to a file, function,\
    \ class, or method.\n        Args:\n            question_type (str): The type\
    \ of question to be processed.\n            question_id (str): The ID of the question\
    \ to be processed.\n            question_text (str): The text of the question\
    \ to be processed.\n        Returns:\n            None\n        \"\"\"\n     \
    \   if question_type == 'file':\n            query = question_text.format(filename=self.base_name)\n\
    \            info = self.file_details['file_info']\n            context = self.file_details['file_info']['file_code']\n\
    \            self.process_question(question_type, question_id, query, context,\
    \ info)\n        elif question_type == 'method':\n            for class_name,\
    \ class_info in self.file_details['classes'].items():\n                for key,\
    \ method_info in class_info.items():\n                    if key.startswith('class_method_'):\n\
    \                        method_name = key[len('class_method_'):]\n          \
    \              context = method_info['method_code']\n                        mapping\
    \ = {'class_name': class_name, 'method_name': method_name}\n                 \
    \       query = question_text.format(filename=self.base_name, **mapping)\n   \
    \                     self.process_question(question_type, question_id, query,\
    \ context, method_info)\n        else:  # if question_type == 'function' or question_type\
    \ == 'class'\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n\
    \                context = info[f'{question_type}_code']\n                mapping\
    \ = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_purpose'\
    \ and self.use_llm:\n                    variables_string = self.get_string_from_info(info,\
    \ f'{question_type}_variables')\n                    inputs_string = self.get_string_from_info(info,\
    \ f'{question_type}_inputs')\n                    combined_string = ', '.join([s\
    \ for s in [variables_string, inputs_string] if s])\n                    mapping[f'{question_type}_variables']\
    \ = clean_and_get_unique_elements(combined_string)\n                    # get\
    \ methods to include in mapping for query\n                    if question_type\
    \ == 'class':\n                        methods_string = self.get_string_from_info(info,\
    \ f'{question_type}_methods')\n                        mapping[f'{question_type}_methods']\
    \ = methods_string\n\n                query = question_text.format(filename=self.base_name,\
    \ **mapping)\n                self.process_question(question_type, question_id,\
    \ query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n\
    \n        \"\"\"\n        Generate responses for all the questions and returns\
    \ the instruct_list.\n        Args:\n            None\n        Returns:\n    \
    \        Tuple[List[Dict], List[Dict]]: The generated question-answer pairs and\
    \ instructions.\n        \"\"\"\n        for question in self.questions:\n   \
    \         self.process_question_type(question['type'], question['id'], question['text'])\n\
    \        return self.instruct_list\n\n\ndef get_python_datasets(file_path: str,\
    \ file_details: Dict, base_name: str, questions: List[Dict],\n               \
    \         model_config: Dict) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\"\n\
    \    Extract information from a Python file and return it in JSON format.\n  \
    \  Args:\n        file_path (str): The path to the Python file.\n        file_details\
    \ (Dict): The details of the file.\n        base_name (str): The base Python code\
    \ filename.\n        questions (List[Dict]): The list of questions.\n        llm\
    \ (object): The language model to be used for generating responses.\n        prompt\
    \ (str): The prompt to be used for generating responses.\n    Returns:\n     \
    \   Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n   \
    \ \"\"\"\n    generator = DatasetGenerator(file_path, file_details, base_name,\
    \ questions, model_config)\n    return generator.generate()\n"
  file_ast: 'Module(body=[Expr(value=Constant(value=''\nGenerates JSON format question-answer
    pairs and instructions for a Python file\nRequirements:\n[req01] The `DatasetGenerator`
    class shall:\n        a. Accept Python file path, file details, base name, list
    of questions,\n        and model configuration as input during instantiation.\n        b.
    Initialize and store Python file path, file details, base name,\n        question
    list, llm, and use_llm flag as class attributes.\n        d. Provide `get_response_from_llm`
    method to retrieve llm response.\n        e. Provide `process_question` method
    to process each question, generate\n        corresponding responses, and add to
    the instruct_list.\n        f. Provide `process_question_type` method to process
    questions\n        related to the file, functions, classes, and methods.\n        g.
    Provide `generate` method to generate responses for all questions\n        and
    return the instruct_list.\n        h. Internally manage question mapping to file
    details.\n[req02] The `get_python_datasets` function shall:\n        a. Accept
    a Python file path, file details, base name, questions list,\n        and model
    config as input.\n        b. Instantiate `DatasetGenerator` class using the provided
    input.\n        c. Generate instruct_list using `DatasetGenerator` `generate`
    method.\n        d. Return the generated `instruct_list`.\n[req03] The `clean_and_get_unique_elements`
    function shall:\n        a. Clean an input string (str) and return a string of
    unique elements.\n'')), Import(names=[alias(name=''logging'')]), Import(names=[alias(name=''re'')]),
    Import(names=[alias(name=''math'')]), ImportFrom(module=''typing'', names=[alias(name=''Dict''),
    alias(name=''List''), alias(name=''Tuple'')], level=0), FunctionDef(name=''clean_and_get_unique_elements'',
    args=arguments(posonlyargs=[], args=[arg(arg=''input_str'', annotation=Name(id=''str'',
    ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n    Clean
    input string and return string of unique elements.\n    Args:\n        input_str
    (str): The input string to be cleaned.\n    Returns:\n        str: The cleaned
    string.\n    '')), Assign(targets=[Name(id=''cleaned_elements'', ctx=Store())],
    value=Call(func=Name(id=''set'', ctx=Load()), args=[GeneratorExp(elt=Call(func=Attribute(value=Name(id=''re'',
    ctx=Load()), attr=''sub'', ctx=Load()), args=[Constant(value=''[^\\w\\-_>\\s:/.]''),
    Constant(value=''''), Call(func=Attribute(value=Name(id=''element'', ctx=Load()),
    attr=''strip'', ctx=Load()), args=[], keywords=[])], keywords=[]), generators=[comprehension(target=Name(id=''element'',
    ctx=Store()), iter=Call(func=Attribute(value=Call(func=Attribute(value=Name(id=''re'',
    ctx=Load()), attr=''sub'', ctx=Load()), args=[Constant(value=''\\s+''), Constant(value=''
    ''), Name(id=''input_str'', ctx=Load())], keywords=[]), attr=''split'', ctx=Load()),
    args=[Constant(value='','')], keywords=[]), ifs=[], is_async=0)])], keywords=[])),
    Return(value=Call(func=Attribute(value=Constant(value='', ''), attr=''join'',
    ctx=Load()), args=[Name(id=''cleaned_elements'', ctx=Load())], keywords=[]))],
    decorator_list=[], returns=Name(id=''str'', ctx=Load())), ClassDef(name=''DatasetGenerator'',
    bases=[], keywords=[], body=[Expr(value=Constant(value=''\n    Generate JSON formatted
    dictionary outputs for a Python file.\n    Attributes:\n        file_path (str):
    The path to the Python file.\n        file_details (Dict[str, Any]): Details of
    the Python file.\n        base_name (str): The base name of the Python file.\n        questions
    (List[Dict[str, str]]): Questions for generating responses.\n        instruct_list
    (List[Dict[str, str]]): Storage for generated instructions.\n        question_mapping
    (Dict[str, str]): Mapping of question types to keys in file details.\n        use_llm
    (bool): Flag indicating if a language model should be used.\n        llm (object):
    The language model for generating responses.\n        prompt (str): The prompt
    format for querying the language model.\n    Methods:\n        add_to_list(list_to_update:
    List[Dict], query: str, response: str, \n        additional_field=None) -> List[Dict]:
    \n            Add response to the instruct list.\n        get_response_from_llm(query:
    str, context: str) -> str:\n            Get language model response to query for
    given context.\n        process_question(question_type: str, question_id: str,
    query: str, \n        context: str, info: Dict) -> None:\n            Process
    question and add generated response to the instruct_list.\n        process_question_type(question_type:
    str, question_id: str, \n        question_text: str) -> None:\n            Process
    question related to file, function, class, or method.\n        generate() -> Tuple[List[Dict],
    List[Dict]]:\n            Generate responses for all the questions and return
    the instruct_list.\n    '')), FunctionDef(name=''__init__'', args=arguments(posonlyargs=[],
    args=[arg(arg=''self''), arg(arg=''file_path'', annotation=Name(id=''str'', ctx=Load())),
    arg(arg=''file_details'', annotation=Name(id=''Dict'', ctx=Load())), arg(arg=''base_name'',
    annotation=Name(id=''str'', ctx=Load())), arg(arg=''questions'', annotation=Subscript(value=Name(id=''List'',
    ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load())), arg(arg=''model_config'',
    annotation=Name(id=''Dict'', ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]),
    body=[Expr(value=Constant(value=''\n        Initialize the DatasetGenerator class.\n        Args:\n            file_path
    (str): The path to the Python file.\n            file_details (Dict[str, Any]):
    Details of the Python file.\n            base_name (str): The base name of the
    Python file.\n            questions (List[Dict[str, str]]): Questions for generating
    responses.\n            model_config (Dict): Configuration for the language model.\n        Returns:\n            None\n        '')),
    Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''file_path'',
    ctx=Store())], value=Name(id=''file_path'', ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Store())], value=Name(id=''file_details'',
    ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'',
    ctx=Store())], value=Name(id=''base_name'', ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''questions'', ctx=Store())], value=Name(id=''questions'', ctx=Load())),
    Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''model_config'',
    ctx=Store())], value=Name(id=''model_config'', ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''llm'', ctx=Store())], value=Subscript(value=Name(id=''model_config'',
    ctx=Load()), slice=Constant(value=''model''), ctx=Load())), If(test=Compare(left=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''llm'', ctx=Load()), ops=[Is()], comparators=[Constant(value=None)]),
    body=[Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''use_llm'',
    ctx=Store())], value=Constant(value=False))], orelse=[Assign(targets=[Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''use_llm'', ctx=Store())], value=Constant(value=True))]), Assign(targets=[Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''instruct_list'', ctx=Store())], value=List(elts=[], ctx=Load())),
    Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''question_mapping'',
    ctx=Store())], value=Dict(keys=[Constant(value=''file''), Constant(value=''function''),
    Constant(value=''class''), Constant(value=''method'')], values=[Constant(value=''file''),
    Constant(value=''functions''), Constant(value=''classes''), Constant(value=''classes'')]))],
    decorator_list=[], returns=Constant(value=None)), FunctionDef(name=''add_to_list'',
    args=arguments(posonlyargs=[], args=[arg(arg=''self''), arg(arg=''list_to_update'',
    annotation=Subscript(value=Name(id=''List'', ctx=Load()), slice=Name(id=''Dict'',
    ctx=Load()), ctx=Load())), arg(arg=''query'', annotation=Name(id=''str'', ctx=Load())),
    arg(arg=''response'', annotation=Name(id=''str'', ctx=Load())), arg(arg=''additional_field'')],
    kwonlyargs=[], kw_defaults=[], defaults=[Constant(value=None)]), body=[Expr(value=Constant(value=''\n        Add
    response to the instruct list.\n        Args:\n            list_to_update (List[Dict]):
    The list to update.\n            query (str): The query to be added.\n            response
    (str): The response to be added.\n            additional_field (Any): Additional
    field to be added.\n        Returns:\n            List[Dict]: The updated list.\n        '')),
    Expr(value=Call(func=Attribute(value=Name(id=''list_to_update'', ctx=Load()),
    attr=''append'', ctx=Load()), args=[Dict(keys=[Constant(value=''instruction''),
    Constant(value=''input''), Constant(value=''output'')], values=[Name(id=''query'',
    ctx=Load()), Name(id=''additional_field'', ctx=Load()), Name(id=''response'',
    ctx=Load())])], keywords=[])), Return(value=Name(id=''list_to_update'', ctx=Load()))],
    decorator_list=[], returns=Subscript(value=Name(id=''List'', ctx=Load()), slice=Name(id=''Dict'',
    ctx=Load()), ctx=Load())), FunctionDef(name=''get_response_from_llm'', args=arguments(posonlyargs=[],
    args=[arg(arg=''self''), arg(arg=''query'', annotation=Name(id=''str'', ctx=Load())),
    arg(arg=''context'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
    kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Get
    language model response to query for given context.\n        Args:\n            query
    (str): The query to be used for generating the response.\n            context
    (str): The context to be used for generating the response.\n        Returns:\n            str:
    The generated response.\n        '')), FunctionDef(name=''get_context_and_prompt'',
    args=arguments(posonlyargs=[], args=[arg(arg=''query''), arg(arg=''context''),
    arg(arg=''code_qa'')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id=''full_context'',
    ctx=Store())], value=JoinedStr(values=[FormattedValue(value=Name(id=''context'',
    ctx=Load()), conversion=-1), Constant(value=''\nCODE Q and A:\n''), FormattedValue(value=Name(id=''code_qa'',
    ctx=Load()), conversion=-1)])), Assign(targets=[Name(id=''prompt'', ctx=Store())],
    value=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''model_config'', ctx=Load()), slice=Constant(value=''prompt_template''),
    ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''context'',
    value=Name(id=''full_context'', ctx=Load())), keyword(arg=''query'', value=Name(id=''query'',
    ctx=Load()))])), Assign(targets=[Name(id=''context_size'', ctx=Store())], value=Call(func=Name(id=''len'',
    ctx=Load()), args=[Call(func=Attribute(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''llm'', ctx=Load()), attr=''tokenize'', ctx=Load()), args=[Name(id=''prompt'',
    ctx=Load())], keywords=[])], keywords=[])), Return(value=Tuple(elts=[Name(id=''prompt'',
    ctx=Load()), Name(id=''context_size'', ctx=Load())], ctx=Load()))], decorator_list=[]),
    Assign(targets=[Name(id=''max_context_length'', ctx=Store())], value=Subscript(value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''model_config'', ctx=Load()), slice=Constant(value=''inference_model''),
    ctx=Load()), slice=Constant(value=''model_params''), ctx=Load()), slice=Constant(value=''context_length''),
    ctx=Load())), Assign(targets=[Name(id=''excluded_instructions'', ctx=Store())],
    value=List(elts=[Constant(value=''Call code graph''), Constant(value=''Docstring'')],
    ctx=Load())), Assign(targets=[Name(id=''code_qa'', ctx=Store())], value=Call(func=Attribute(value=Constant(value=''\n''),
    attr=''join'', ctx=Load()), args=[ListComp(elt=JoinedStr(values=[Constant(value=''Q:
    ''), FormattedValue(value=Subscript(value=Name(id=''item'', ctx=Load()), slice=Constant(value=''instruction''),
    ctx=Load()), conversion=-1), Constant(value='' \nA: ''), FormattedValue(value=Subscript(value=Name(id=''item'',
    ctx=Load()), slice=Constant(value=''output''), ctx=Load()), conversion=-1)]),
    generators=[comprehension(target=Name(id=''item'', ctx=Store()), iter=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''instruct_list'', ctx=Load()), ifs=[UnaryOp(op=Not(), operand=Call(func=Name(id=''any'',
    ctx=Load()), args=[GeneratorExp(elt=Call(func=Attribute(value=Subscript(value=Name(id=''item'',
    ctx=Load()), slice=Constant(value=''instruction''), ctx=Load()), attr=''startswith'',
    ctx=Load()), args=[Name(id=''prefix'', ctx=Load())], keywords=[]), generators=[comprehension(target=Name(id=''prefix'',
    ctx=Store()), iter=Name(id=''excluded_instructions'', ctx=Load()), ifs=[], is_async=0)])],
    keywords=[]))], is_async=0)])], keywords=[])), Assign(targets=[Name(id=''context_strategies'',
    ctx=Store())], value=List(elts=[Lambda(args=arguments(posonlyargs=[], args=[],
    kwonlyargs=[], kw_defaults=[], defaults=[]), body=BinOp(left=BinOp(left=Constant(value=''```python\n''),
    op=Add(), right=Call(func=Name(id=''str'', ctx=Load()), args=[Name(id=''context'',
    ctx=Load())], keywords=[])), op=Add(), right=Constant(value=''\n```''))), Lambda(args=arguments(posonlyargs=[],
    args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=BinOp(left=BinOp(left=Constant(value=''```python\n''),
    op=Add(), right=Call(func=Name(id=''str'', ctx=Load()), args=[Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
    ctx=Load()), slice=Constant(value=''file_code_simplified''), ctx=Load())], keywords=[])),
    op=Add(), right=Constant(value=''\n```''))), Lambda(args=arguments(posonlyargs=[],
    args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=Call(func=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''get_string_from_info'', ctx=Load()), args=[Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
    ctx=Load()), Constant(value=''file_summary'')], keywords=[])), Lambda(args=arguments(posonlyargs=[],
    args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=Constant(value=''''))],
    ctx=Load())), For(target=Name(id=''strategy'', ctx=Store()), iter=Name(id=''context_strategies'',
    ctx=Load()), body=[Assign(targets=[Name(id=''context'', ctx=Store())], value=Call(func=Name(id=''strategy'',
    ctx=Load()), args=[], keywords=[])), Assign(targets=[Tuple(elts=[Name(id=''prompt'',
    ctx=Store()), Name(id=''context_size'', ctx=Store())], ctx=Store())], value=Call(func=Name(id=''get_context_and_prompt'',
    ctx=Load()), args=[Name(id=''query'', ctx=Load()), Name(id=''context'', ctx=Load()),
    Name(id=''code_qa'', ctx=Load())], keywords=[])), If(test=Compare(left=Name(id=''context_size'',
    ctx=Load()), ops=[LtE()], comparators=[BinOp(left=Constant(value=0.7), op=Mult(),
    right=Name(id=''max_context_length'', ctx=Load()))]), body=[Break()], orelse=[])],
    orelse=[Expr(value=Call(func=Attribute(value=Name(id=''logging'', ctx=Load()),
    attr=''error'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Model response
    failed, increase py2dataset_model_config.yaml context_length > ''), FormattedValue(value=Call(func=Attribute(value=Name(id=''math'',
    ctx=Load()), attr=''ceil'', ctx=Load()), args=[BinOp(left=Name(id=''context_size'',
    ctx=Load()), op=Div(), right=Constant(value=0.7))], keywords=[]), conversion=-1)])],
    keywords=[])), Return(value=Constant(value=''''))]), Try(body=[Assign(targets=[Name(id=''response'',
    ctx=Store())], value=Call(func=Attribute(value=Name(id=''re'', ctx=Load()), attr=''sub'',
    ctx=Load()), args=[Constant(value=''\\n\\s*\\n''), Constant(value=''\n\n''), Call(func=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''llm'', ctx=Load()), args=[Name(id=''prompt'', ctx=Load())],
    keywords=[])], keywords=[])), Expr(value=Call(func=Attribute(value=Name(id=''logging'',
    ctx=Load()), attr=''info'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Response:
    ''), FormattedValue(value=Name(id=''response'', ctx=Load()), conversion=-1)])],
    keywords=[]))], handlers=[ExceptHandler(type=Name(id=''Exception'', ctx=Load()),
    name=''error'', body=[Expr(value=Call(func=Attribute(value=Name(id=''logging'',
    ctx=Load()), attr=''error'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Failed
    to generate model response: ''), FormattedValue(value=Name(id=''error'', ctx=Load()),
    conversion=-1)])], keywords=[])), Assign(targets=[Name(id=''response'', ctx=Store())],
    value=Constant(value=''''))])], orelse=[], finalbody=[]), Return(value=Name(id=''response'',
    ctx=Load()))], decorator_list=[], returns=Name(id=''str'', ctx=Load())), FunctionDef(name=''process_question'',
    args=arguments(posonlyargs=[], args=[arg(arg=''self''), arg(arg=''question_type'',
    annotation=Name(id=''str'', ctx=Load())), arg(arg=''question_id'', annotation=Name(id=''str'',
    ctx=Load())), arg(arg=''query'', annotation=Name(id=''str'', ctx=Load())), arg(arg=''context'',
    annotation=Name(id=''str'', ctx=Load())), arg(arg=''info'', annotation=Name(id=''Dict'',
    ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Process
    question and add the generated response to the instruct_list.\n        Args:\n            question_type
    (str): The type of question to be processed.\n            question_id (str): The
    ID of the question to be processed.\n            query (str): The query to be
    processed.\n            context (str): The context to be used for generating the
    response.\n            info (Dict): The information of the Python file.\n        Returns:\n            None\n        '')),
    If(test=BoolOp(op=Or(), values=[Call(func=Attribute(value=Name(id=''question_id'',
    ctx=Load()), attr=''endswith'', ctx=Load()), args=[Constant(value=''code_graph'')],
    keywords=[]), Call(func=Attribute(value=Name(id=''question_id'', ctx=Load()),
    attr=''endswith'', ctx=Load()), args=[Constant(value=''docstring'')], keywords=[])]),
    body=[Assign(targets=[Name(id=''response'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''info'',
    ctx=Load()), attr=''get'', ctx=Load()), args=[Name(id=''question_id'', ctx=Load()),
    Dict(keys=[], values=[])], keywords=[]))], orelse=[Assign(targets=[Name(id=''response'',
    ctx=Store())], value=IfExp(test=BoolOp(op=And(), values=[Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''use_llm'', ctx=Load()), Call(func=Attribute(value=Name(id=''question_id'',
    ctx=Load()), attr=''endswith'', ctx=Load()), args=[Constant(value=''purpose'')],
    keywords=[])]), body=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
    attr=''get_response_from_llm'', ctx=Load()), args=[Name(id=''query'', ctx=Load()),
    Name(id=''context'', ctx=Load())], keywords=[]), orelse=Call(func=Name(id=''clean_and_get_unique_elements'',
    ctx=Load()), args=[Call(func=Name(id=''str'', ctx=Load()), args=[Call(func=Attribute(value=Name(id=''info'',
    ctx=Load()), attr=''get'', ctx=Load()), args=[Name(id=''question_id'', ctx=Load()),
    Constant(value='''')], keywords=[])], keywords=[])], keywords=[])))]), If(test=Compare(left=Name(id=''question_type'',
    ctx=Load()), ops=[Eq()], comparators=[Constant(value=''file'')]), body=[Assign(targets=[Name(id=''context'',
    ctx=Store())], value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
    ctx=Load()), slice=Constant(value=''file_code''), ctx=Load()))], orelse=[]), If(test=BoolOp(op=And(),
    values=[Name(id=''response'', ctx=Load()), Compare(left=Name(id=''response'',
    ctx=Load()), ops=[NotEq()], comparators=[Constant(value=''None'')])]), body=[Assign(targets=[Name(id=''response_str'',
    ctx=Store())], value=Call(func=Attribute(value=Call(func=Name(id=''str'', ctx=Load()),
    args=[Name(id=''response'', ctx=Load())], keywords=[]), attr=''strip'', ctx=Load()),
    args=[], keywords=[])), If(test=Name(id=''response_str'', ctx=Load()), body=[Expr(value=Call(func=Attribute(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''instruct_list'', ctx=Load()), attr=''append'', ctx=Load()),
    args=[Dict(keys=[Constant(value=''instruction''), Constant(value=''input''), Constant(value=''output'')],
    values=[Name(id=''query'', ctx=Load()), Name(id=''context'', ctx=Load()), Name(id=''response_str'',
    ctx=Load())])], keywords=[]))], orelse=[])], orelse=[])], decorator_list=[], returns=Constant(value=None)),
    FunctionDef(name=''get_string_from_info'', args=arguments(posonlyargs=[], args=[arg(arg=''info''),
    arg(arg=''item_type'')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Get
    string from info dictionary.\n        Args:\n            info (Dict): The information
    of the Python file.\n            item_type (str): The type of item to get the
    string from.\n        Returns:\n            str: The string from the info.\n        '')),
    If(test=Subscript(value=Name(id=''info'', ctx=Load()), slice=Name(id=''item_type'',
    ctx=Load()), ctx=Load()), body=[Assign(targets=[Name(id=''items'', ctx=Store())],
    value=ListComp(elt=Call(func=Attribute(value=Name(id=''item'', ctx=Load()), attr=''strip'',
    ctx=Load()), args=[], keywords=[]), generators=[comprehension(target=Name(id=''item'',
    ctx=Store()), iter=Call(func=Attribute(value=Call(func=Name(id=''str'', ctx=Load()),
    args=[Subscript(value=Name(id=''info'', ctx=Load()), slice=Name(id=''item_type'',
    ctx=Load()), ctx=Load())], keywords=[]), attr=''split'', ctx=Load()), args=[Constant(value='','')],
    keywords=[]), ifs=[Name(id=''item'', ctx=Load())], is_async=0)])), Return(value=Call(func=Attribute(value=Constant(value='',
    ''), attr=''join'', ctx=Load()), args=[Name(id=''items'', ctx=Load())], keywords=[]))],
    orelse=[]), Return(value=Constant(value=''''))], decorator_list=[Name(id=''staticmethod'',
    ctx=Load())]), FunctionDef(name=''process_question_type'', args=arguments(posonlyargs=[],
    args=[arg(arg=''self''), arg(arg=''question_type'', annotation=Name(id=''str'',
    ctx=Load())), arg(arg=''question_id'', annotation=Name(id=''str'', ctx=Load())),
    arg(arg=''question_text'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
    kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Process
    questions related to a file, function, class, or method.\n        Args:\n            question_type
    (str): The type of question to be processed.\n            question_id (str): The
    ID of the question to be processed.\n            question_text (str): The text
    of the question to be processed.\n        Returns:\n            None\n        '')),
    If(test=Compare(left=Name(id=''question_type'', ctx=Load()), ops=[Eq()], comparators=[Constant(value=''file'')]),
    body=[Assign(targets=[Name(id=''query'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''question_text'',
    ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''filename'',
    value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'', ctx=Load()))])),
    Assign(targets=[Name(id=''info'', ctx=Store())], value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
    ctx=Load())), Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
    ctx=Load()), slice=Constant(value=''file_code''), ctx=Load())), Expr(value=Call(func=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'',
    ctx=Load()), Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()),
    Name(id=''context'', ctx=Load()), Name(id=''info'', ctx=Load())], keywords=[]))],
    orelse=[If(test=Compare(left=Name(id=''question_type'', ctx=Load()), ops=[Eq()],
    comparators=[Constant(value=''method'')]), body=[For(target=Tuple(elts=[Name(id=''class_name'',
    ctx=Store()), Name(id=''class_info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''classes''),
    ctx=Load()), attr=''items'', ctx=Load()), args=[], keywords=[]), body=[For(target=Tuple(elts=[Name(id=''key'',
    ctx=Store()), Name(id=''method_info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Name(id=''class_info'',
    ctx=Load()), attr=''items'', ctx=Load()), args=[], keywords=[]), body=[If(test=Call(func=Attribute(value=Name(id=''key'',
    ctx=Load()), attr=''startswith'', ctx=Load()), args=[Constant(value=''class_method_'')],
    keywords=[]), body=[Assign(targets=[Name(id=''method_name'', ctx=Store())], value=Subscript(value=Name(id=''key'',
    ctx=Load()), slice=Slice(lower=Call(func=Name(id=''len'', ctx=Load()), args=[Constant(value=''class_method_'')],
    keywords=[])), ctx=Load())), Assign(targets=[Name(id=''context'', ctx=Store())],
    value=Subscript(value=Name(id=''method_info'', ctx=Load()), slice=Constant(value=''method_code''),
    ctx=Load())), Assign(targets=[Name(id=''mapping'', ctx=Store())], value=Dict(keys=[Constant(value=''class_name''),
    Constant(value=''method_name'')], values=[Name(id=''class_name'', ctx=Load()),
    Name(id=''method_name'', ctx=Load())])), Assign(targets=[Name(id=''query'', ctx=Store())],
    value=Call(func=Attribute(value=Name(id=''question_text'', ctx=Load()), attr=''format'',
    ctx=Load()), args=[], keywords=[keyword(arg=''filename'', value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''base_name'', ctx=Load())), keyword(value=Name(id=''mapping'',
    ctx=Load()))])), Expr(value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
    attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'', ctx=Load()),
    Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()), Name(id=''context'',
    ctx=Load()), Name(id=''method_info'', ctx=Load())], keywords=[]))], orelse=[])],
    orelse=[])], orelse=[])], orelse=[For(target=Tuple(elts=[Name(id=''name'', ctx=Store()),
    Name(id=''info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''file_details'', ctx=Load()), slice=Subscript(value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''question_mapping'', ctx=Load()), slice=Name(id=''question_type'',
    ctx=Load()), ctx=Load()), ctx=Load()), attr=''items'', ctx=Load()), args=[], keywords=[]),
    body=[Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Name(id=''info'',
    ctx=Load()), slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
    ctx=Load()), conversion=-1), Constant(value=''_code'')]), ctx=Load())), Assign(targets=[Name(id=''mapping'',
    ctx=Store())], value=Dict(keys=[JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
    ctx=Load()), conversion=-1), Constant(value=''_name'')])], values=[Name(id=''name'',
    ctx=Load())])), If(test=BoolOp(op=And(), values=[Compare(left=Name(id=''question_id'',
    ctx=Load()), ops=[Eq()], comparators=[JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
    ctx=Load()), conversion=-1), Constant(value=''_purpose'')])]), Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''use_llm'', ctx=Load())]), body=[Assign(targets=[Name(id=''variables_string'',
    ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
    attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
    JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
    conversion=-1), Constant(value=''_variables'')])], keywords=[])), Assign(targets=[Name(id=''inputs_string'',
    ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
    attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
    JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
    conversion=-1), Constant(value=''_inputs'')])], keywords=[])), Assign(targets=[Name(id=''combined_string'',
    ctx=Store())], value=Call(func=Attribute(value=Constant(value='', ''), attr=''join'',
    ctx=Load()), args=[ListComp(elt=Name(id=''s'', ctx=Load()), generators=[comprehension(target=Name(id=''s'',
    ctx=Store()), iter=List(elts=[Name(id=''variables_string'', ctx=Load()), Name(id=''inputs_string'',
    ctx=Load())], ctx=Load()), ifs=[Name(id=''s'', ctx=Load())], is_async=0)])], keywords=[])),
    Assign(targets=[Subscript(value=Name(id=''mapping'', ctx=Load()), slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
    ctx=Load()), conversion=-1), Constant(value=''_variables'')]), ctx=Store())],
    value=Call(func=Name(id=''clean_and_get_unique_elements'', ctx=Load()), args=[Name(id=''combined_string'',
    ctx=Load())], keywords=[])), If(test=Compare(left=Name(id=''question_type'', ctx=Load()),
    ops=[Eq()], comparators=[Constant(value=''class'')]), body=[Assign(targets=[Name(id=''methods_string'',
    ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
    attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
    JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
    conversion=-1), Constant(value=''_methods'')])], keywords=[])), Assign(targets=[Subscript(value=Name(id=''mapping'',
    ctx=Load()), slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
    ctx=Load()), conversion=-1), Constant(value=''_methods'')]), ctx=Store())], value=Name(id=''methods_string'',
    ctx=Load()))], orelse=[])], orelse=[]), Assign(targets=[Name(id=''query'', ctx=Store())],
    value=Call(func=Attribute(value=Name(id=''question_text'', ctx=Load()), attr=''format'',
    ctx=Load()), args=[], keywords=[keyword(arg=''filename'', value=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''base_name'', ctx=Load())), keyword(value=Name(id=''mapping'',
    ctx=Load()))])), Expr(value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
    attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'', ctx=Load()),
    Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()), Name(id=''context'',
    ctx=Load()), Name(id=''info'', ctx=Load())], keywords=[]))], orelse=[])])])],
    decorator_list=[], returns=Constant(value=None)), FunctionDef(name=''generate'',
    args=arguments(posonlyargs=[], args=[arg(arg=''self'')], kwonlyargs=[], kw_defaults=[],
    defaults=[]), body=[Expr(value=Constant(value=''\n        Generate responses for
    all the questions and returns the instruct_list.\n        Args:\n            None\n        Returns:\n            Tuple[List[Dict],
    List[Dict]]: The generated question-answer pairs and instructions.\n        '')),
    For(target=Name(id=''question'', ctx=Store()), iter=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''questions'', ctx=Load()), body=[Expr(value=Call(func=Attribute(value=Name(id=''self'',
    ctx=Load()), attr=''process_question_type'', ctx=Load()), args=[Subscript(value=Name(id=''question'',
    ctx=Load()), slice=Constant(value=''type''), ctx=Load()), Subscript(value=Name(id=''question'',
    ctx=Load()), slice=Constant(value=''id''), ctx=Load()), Subscript(value=Name(id=''question'',
    ctx=Load()), slice=Constant(value=''text''), ctx=Load())], keywords=[]))], orelse=[]),
    Return(value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''instruct_list'',
    ctx=Load()))], decorator_list=[], returns=Subscript(value=Name(id=''Tuple'', ctx=Load()),
    slice=Tuple(elts=[Subscript(value=Name(id=''List'', ctx=Load()), slice=Name(id=''Dict'',
    ctx=Load()), ctx=Load()), Subscript(value=Name(id=''List'', ctx=Load()), slice=Name(id=''Dict'',
    ctx=Load()), ctx=Load())], ctx=Load()), ctx=Load()))], decorator_list=[]), FunctionDef(name=''get_python_datasets'',
    args=arguments(posonlyargs=[], args=[arg(arg=''file_path'', annotation=Name(id=''str'',
    ctx=Load())), arg(arg=''file_details'', annotation=Name(id=''Dict'', ctx=Load())),
    arg(arg=''base_name'', annotation=Name(id=''str'', ctx=Load())), arg(arg=''questions'',
    annotation=Subscript(value=Name(id=''List'', ctx=Load()), slice=Name(id=''Dict'',
    ctx=Load()), ctx=Load())), arg(arg=''model_config'', annotation=Name(id=''Dict'',
    ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n    Extract
    information from a Python file and return it in JSON format.\n    Args:\n        file_path
    (str): The path to the Python file.\n        file_details (Dict): The details
    of the file.\n        base_name (str): The base Python code filename.\n        questions
    (List[Dict]): The list of questions.\n        llm (object): The language model
    to be used for generating responses.\n        prompt (str): The prompt to be used
    for generating responses.\n    Returns:\n        Tuple[List[Dict], List[Dict]]:
    Extracted information in JSON format.\n    '')), Assign(targets=[Name(id=''generator'',
    ctx=Store())], value=Call(func=Name(id=''DatasetGenerator'', ctx=Load()), args=[Name(id=''file_path'',
    ctx=Load()), Name(id=''file_details'', ctx=Load()), Name(id=''base_name'', ctx=Load()),
    Name(id=''questions'', ctx=Load()), Name(id=''model_config'', ctx=Load())], keywords=[])),
    Return(value=Call(func=Attribute(value=Name(id=''generator'', ctx=Load()), attr=''generate'',
    ctx=Load()), args=[], keywords=[]))], decorator_list=[], returns=Subscript(value=Name(id=''Tuple'',
    ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id=''List'', ctx=Load()),
    slice=Name(id=''Dict'', ctx=Load()), ctx=Load()), Subscript(value=Name(id=''List'',
    ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load())], ctx=Load()), ctx=Load()))],
    type_ignores=[])'
  file_dependencies:
  - math
  - logging
  - re
  - typing
  file_functions:
  - clean_and_get_unique_elements
  - get_python_datasets
  file_classes:
  - DatasetGenerator
  file_summary: '{dependencies: [math, logging, re, typing], function_defs: [{clean_and_get_unique_elements:
    {inputs: [input_str], calls: [set, re.sub, element.strip, re.sub(''\\\\s+'', ''
    '', input_str).split, '', ''.join], call_inputs: {set: [(re.sub(''[^\\\\w\\\\-_>\\\\s:/.]'',
    '''', element.strip()) for element in re.sub(''\\\\s+'', '' '', input_str).split('',''))],
    re.sub: [''\\\\s+'', '' '', input_str], element.strip: [], re.sub(''\\\\s+'',
    '' '', input_str).split: ['',''], '', ''.join: [cleaned_elements]}, returns: ['',
    ''.join(cleaned_elements)]}}, {get_python_datasets: {inputs: [file_path, file_details,
    base_name, questions, model_config], calls: [DatasetGenerator, generator.generate],
    call_inputs: {DatasetGenerator: [file_path, file_details, base_name, questions,
    model_config], generator.generate: []}, returns: [generator.generate()]}}], class_defs:
    [{DatasetGenerator: {method_defs: {__init__: {inputs: [self, file_path, file_details,
    base_name, questions, model_config], calls: [], call_inputs: {}, returns: []},
    add_to_list: {inputs: [self, list_to_update, query, response, additional_field],
    calls: [list_to_update.append], call_inputs: {list_to_update.append: [{''instruction'':
    query, ''input'': additional_field, ''output'': response}]}, returns: [list_to_update]},
    get_response_from_llm: {inputs: [self, query, context], calls: [self.model_config[''prompt_template''].format,
    len, self.llm.tokenize, ''\\n''.join, any, item[''instruction''].startswith, str,
    self.get_string_from_info, strategy, get_context_and_prompt, logging.error, math.ceil,
    re.sub, self.llm, logging.info], call_inputs: {self.model_config[''prompt_template''].format:
    [], len: [self.llm.tokenize(prompt)], self.llm.tokenize: [prompt], ''\\n''.join:
    [[f\Q: {item[''instruction'']} \\nA: {item[''output'']}\ for item in self.instruct_list
    if not any((item[''instruction''].startswith(prefix) for prefix in excluded_instructions))]],
    any: [(item[''instruction''].startswith(prefix) for prefix in excluded_instructions)],
    item[''instruction''].startswith: [prefix], str: [self.file_details[''file_info''][''file_code_simplified'']],
    self.get_string_from_info: [self.file_details[''file_info''], ''file_summary''],
    strategy: [], get_context_and_prompt: [query, context, code_qa], logging.error:
    [f''Failed to generate model response: {error}''], math.ceil: [context_size /
    0.7], re.sub: [''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(prompt)], self.llm:
    [prompt], logging.info: [f''Response: {response}'']}, returns: [response, (prompt,
    context_size), '''']}, get_context_and_prompt: {inputs: [query, context, code_qa],
    calls: [self.model_config[''prompt_template''].format, len, self.llm.tokenize],
    call_inputs: {self.model_config[''prompt_template''].format: [], len: [self.llm.tokenize(prompt)],
    self.llm.tokenize: [prompt]}, returns: [(prompt, context_size)]}, process_question:
    {inputs: [self, question_type, question_id, query, context, info], calls: [question_id.endswith,
    info.get, self.get_response_from_llm, clean_and_get_unique_elements, str, str(response).strip,
    self.instruct_list.append], call_inputs: {question_id.endswith: [''purpose''],
    info.get: [question_id, ''''], self.get_response_from_llm: [query, context], clean_and_get_unique_elements:
    [str(info.get(question_id, ''''))], str: [response], str(response).strip: [],
    self.instruct_list.append: [{''instruction'': query, ''input'': context, ''output'':
    response_str}]}, returns: []}, get_string_from_info: {inputs: [info, item_type],
    calls: [item.strip, str(info[item_type]).split, str, '', ''.join], call_inputs:
    {item.strip: [], str(info[item_type]).split: ['',''], str: [info[item_type]],
    '', ''.join: [items]}, returns: ['''', '', ''.join(items)]}, process_question_type:
    {inputs: [self, question_type, question_id, question_text], calls: [question_text.format,
    self.process_question, self.file_details[''classes''].items, class_info.items,
    key.startswith, len, self.file_details[self.question_mapping[question_type]].items,
    self.get_string_from_info, '', ''.join, clean_and_get_unique_elements], call_inputs:
    {question_text.format: [], self.process_question: [question_type, question_id,
    query, context, info], self.file_details[''classes''].items: [], class_info.items:
    [], key.startswith: [''class_method_''], len: [''class_method_''], self.file_details[self.question_mapping[question_type]].items:
    [], self.get_string_from_info: [info, f''{question_type}_methods''], '', ''.join:
    [[s for s in [variables_string, inputs_string] if s]], clean_and_get_unique_elements:
    [combined_string]}, returns: []}, generate: {inputs: [self], calls: [self.process_question_type],
    call_inputs: {self.process_question_type: [question[''type''], question[''id''],
    question[''text'']]}, returns: [self.instruct_list]}}}}]}'
  file_code_simplified: "\"\"\"\"\"\"\nimport logging\nimport re\nimport math\nfrom\
    \ typing import Dict, List, Tuple\n\n\ndef clean_and_get_unique_elements(input_str:\
    \ str) ->str:\n    \"\"\"\"\"\"\n    cleaned_elements = set(re.sub('[^\\\\w\\\\\
    -_>\\\\s:/.]', '', element.strip()) for\n        element in re.sub('\\\\s+', '\
    \ ', input_str).split(','))\n    return ', '.join(cleaned_elements)\n\n\nclass\
    \ DatasetGenerator:\n    \"\"\"\"\"\"\n\n    def __init__(self, file_path: str,\
    \ file_details: Dict, base_name: str,\n        questions: List[Dict], model_config:\
    \ Dict) ->None:\n        \"\"\"\"\"\"\n        self.file_path = file_path\n  \
    \      self.file_details = file_details\n        self.base_name = base_name\n\
    \        self.questions = questions\n        self.model_config = model_config\n\
    \        self.llm = model_config['model']\n        if self.llm is None:\n    \
    \        self.use_llm = False\n        else:\n            self.use_llm = True\n\
    \        self.instruct_list = []\n        self.question_mapping = {'file': 'file',\
    \ 'function': 'functions',\n            'class': 'classes', 'method': 'classes'}\n\
    \n    def add_to_list(self, list_to_update: List[Dict], query: str, response:\n\
    \        str, additional_field=None) ->List[Dict]:\n        \"\"\"\"\"\"\n   \
    \     list_to_update.append({'instruction': query, 'input':\n            additional_field,\
    \ 'output': response})\n        return list_to_update\n\n    def get_response_from_llm(self,\
    \ query: str, context: str) ->str:\n        \"\"\"\"\"\"\n\n        def get_context_and_prompt(query,\
    \ context, code_qa):\n            full_context = f'{context}\\nCODE Q and A:\\\
    n{code_qa}'\n            prompt = self.model_config['prompt_template'].format(context=\n\
    \                full_context, query=query)\n            context_size = len(self.llm.tokenize(prompt))\n\
    \            return prompt, context_size\n        max_context_length = self.model_config['inference_model'][\n\
    \            'model_params']['context_length']\n        excluded_instructions\
    \ = ['Call code graph', 'Docstring']\n        code_qa = '\\n'.join([\n       \
    \     f\"Q: {item['instruction']} \\nA: {item['output']}\" for item in\n     \
    \       self.instruct_list if not any(item['instruction'].startswith(\n      \
    \      prefix) for prefix in excluded_instructions)])\n        context_strategies\
    \ = [lambda : '```python\\n' + str(context) +\n            '\\n```', lambda :\
    \ '```python\\n' + str(self.file_details[\n            'file_info']['file_code_simplified'])\
    \ + '\\n```', lambda : self.\n            get_string_from_info(self.file_details['file_info'],\n\
    \            'file_summary'), lambda : '']\n        for strategy in context_strategies:\n\
    \            context = strategy()\n            prompt, context_size = get_context_and_prompt(query,\
    \ context,\n                code_qa)\n            if context_size <= 0.7 * max_context_length:\n\
    \                break\n        else:\n            logging.error(\n          \
    \      f'Model response failed, increase py2dataset_model_config.yaml context_length\
    \ > {math.ceil(context_size / 0.7)}'\n                )\n            return ''\n\
    \        try:\n            response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n\
    \            logging.info(f'Response: {response}')\n        except Exception as\
    \ error:\n            logging.error(f'Failed to generate model response: {error}')\n\
    \            response = ''\n        return response\n\n    def process_question(self,\
    \ question_type: str, question_id: str, query:\n        str, context: str, info:\
    \ Dict) ->None:\n        \"\"\"\"\"\"\n        if question_id.endswith('code_graph')\
    \ or question_id.endswith(\n            'docstring'):\n            response =\
    \ info.get(question_id, {})\n        else:\n            response = self.get_response_from_llm(query,\
    \ context\n                ) if self.use_llm and question_id.endswith('purpose'\n\
    \                ) else clean_and_get_unique_elements(str(info.get(\n        \
    \        question_id, '')))\n        if question_type == 'file':\n           \
    \ context = self.file_details['file_info']['file_code']\n        if response and\
    \ response != 'None':\n            response_str = str(response).strip()\n    \
    \        if response_str:\n                self.instruct_list.append({'instruction':\
    \ query, 'input':\n                    context, 'output': response_str})\n\n \
    \   @staticmethod\n    def get_string_from_info(info, item_type):\n        \"\"\
    \"\"\"\"\n        if info[item_type]:\n            items = [item.strip() for item\
    \ in str(info[item_type]).split(\n                ',') if item]\n            return\
    \ ', '.join(items)\n        return ''\n\n    def process_question_type(self, question_type:\
    \ str, question_id: str,\n        question_text: str) ->None:\n        \"\"\"\"\
    \"\"\n        if question_type == 'file':\n            query = question_text.format(filename=self.base_name)\n\
    \            info = self.file_details['file_info']\n            context = self.file_details['file_info']['file_code']\n\
    \            self.process_question(question_type, question_id, query,\n      \
    \          context, info)\n        elif question_type == 'method':\n         \
    \   for class_name, class_info in self.file_details['classes'].items():\n    \
    \            for key, method_info in class_info.items():\n                   \
    \ if key.startswith('class_method_'):\n                        method_name = key[len('class_method_'):]\n\
    \                        context = method_info['method_code']\n              \
    \          mapping = {'class_name': class_name, 'method_name':\n             \
    \               method_name}\n                        query = question_text.format(filename=self.\n\
    \                            base_name, **mapping)\n                        self.process_question(question_type,\
    \ question_id,\n                            query, context, method_info)\n   \
    \     else:\n            for name, info in self.file_details[self.question_mapping[\n\
    \                question_type]].items():\n                context = info[f'{question_type}_code']\n\
    \                mapping = {f'{question_type}_name': name}\n                if\
    \ question_id == f'{question_type}_purpose' and self.use_llm:\n              \
    \      variables_string = self.get_string_from_info(info,\n                  \
    \      f'{question_type}_variables')\n                    inputs_string = self.get_string_from_info(info,\n\
    \                        f'{question_type}_inputs')\n                    combined_string\
    \ = ', '.join([s for s in [\n                        variables_string, inputs_string]\
    \ if s])\n                    mapping[f'{question_type}_variables'\n         \
    \               ] = clean_and_get_unique_elements(combined_string)\n         \
    \           if question_type == 'class':\n                        methods_string\
    \ = self.get_string_from_info(info,\n                            f'{question_type}_methods')\n\
    \                        mapping[f'{question_type}_methods'] = methods_string\n\
    \                query = question_text.format(filename=self.base_name, **mapping\n\
    \                    )\n                self.process_question(question_type, question_id,\
    \ query,\n                    context, info)\n\n    def generate(self) ->Tuple[List[Dict],\
    \ List[Dict]]:\n        \"\"\"\"\"\"\n        for question in self.questions:\n\
    \            self.process_question_type(question['type'], question['id'],\n  \
    \              question['text'])\n        return self.instruct_list\n\n\ndef get_python_datasets(file_path:\
    \ str, file_details: Dict, base_name: str,\n    questions: List[Dict], model_config:\
    \ Dict) ->Tuple[List[Dict], List[Dict]]:\n    \"\"\"\"\"\"\n    generator = DatasetGenerator(file_path,\
    \ file_details, base_name,\n        questions, model_config)\n    return generator.generate()"
  entire_code_graph:
    nodes:
    - DatasetGenerator
    - DatasetGenerator.__init__
    - DatasetGenerator.add_to_list
    - DatasetGenerator.get_response_from_llm
    - DatasetGenerator.get_context_and_prompt
    - DatasetGenerator.process_question
    - DatasetGenerator.get_string_from_info
    - DatasetGenerator.process_question_type
    - DatasetGenerator.generate
    - clean_and_get_unique_elements
    - get_python_datasets
    - set
    - re.sub
    - element.strip
    - re.sub('\\s+', ' ', input_str).split
    - ''', ''.join'
    - generator.generate
    - list_to_update.append
    - self.model_config['prompt_template'].format
    - len
    - self.llm.tokenize
    - '''\n''.join'
    - any
    - item['instruction'].startswith
    - str
    - strategy
    - get_context_and_prompt
    - logging.error
    - math.ceil
    - self.llm
    - logging.info
    - question_id.endswith
    - info.get
    - str(response).strip
    - self.instruct_list.append
    - item.strip
    - str(info[item_type]).split
    - question_text.format
    - self.file_details['classes'].items
    - class_info.items
    - key.startswith
    - self.file_details[self.question_mapping[question_type]].items
    edges:
    - source: DatasetGenerator
      target: DatasetGenerator.__init__
      target_inputs:
      - self
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      target_returns: []
    - source: DatasetGenerator
      target: DatasetGenerator.add_to_list
      target_inputs:
      - self
      - list_to_update
      - query
      - response
      - additional_field
      target_returns:
      - list_to_update
    - source: DatasetGenerator
      target: DatasetGenerator.get_response_from_llm
      target_inputs:
      - self
      - query
      - context
      target_returns:
      - response
      - ''''''
      - (prompt, context_size)
    - source: DatasetGenerator
      target: DatasetGenerator.get_context_and_prompt
      target_inputs:
      - query
      - context
      - code_qa
      target_returns:
      - (prompt, context_size)
    - source: DatasetGenerator
      target: DatasetGenerator.process_question
      target_inputs:
      - self
      - question_type
      - question_id
      - query
      - context
      - info
      target_returns: []
    - source: DatasetGenerator
      target: DatasetGenerator.get_string_from_info
      target_inputs:
      - info
      - item_type
      target_returns:
      - ''', ''.join(items)'
      - ''''''
    - source: DatasetGenerator
      target: DatasetGenerator.process_question_type
      target_inputs:
      - self
      - question_type
      - question_id
      - question_text
      target_returns: []
    - source: DatasetGenerator
      target: DatasetGenerator.generate
      target_inputs:
      - self
      target_returns:
      - self.instruct_list
    - source: DatasetGenerator.add_to_list
      target: list_to_update.append
      target_inputs:
      - '{''instruction'': query, ''input'': additional_field, ''output'': response}'
    - source: DatasetGenerator.get_response_from_llm
      target: self.model_config['prompt_template'].format
      target_inputs: []
    - source: DatasetGenerator.get_response_from_llm
      target: len
      target_inputs:
      - self.llm.tokenize(prompt)
    - source: DatasetGenerator.get_response_from_llm
      target: self.llm.tokenize
      target_inputs:
      - prompt
    - source: DatasetGenerator.get_response_from_llm
      target: '''\n''.join'
      target_inputs:
      - '[f"Q: {item[''instruction'']} \nA: {item[''output'']}" for item in self.instruct_list
        if not any((item[''instruction''].startswith(prefix) for prefix in excluded_instructions))]'
    - source: DatasetGenerator.get_response_from_llm
      target: any
      target_inputs:
      - (item['instruction'].startswith(prefix) for prefix in excluded_instructions)
    - source: DatasetGenerator.get_response_from_llm
      target: item['instruction'].startswith
      target_inputs:
      - prefix
    - source: DatasetGenerator.get_response_from_llm
      target: str
      target_inputs:
      - self.file_details['file_info']['file_code_simplified']
    - source: DatasetGenerator.get_response_from_llm
      target: DatasetGenerator.get_string_from_info
      target_inputs:
      - info
      - item_type
      target_returns:
      - ''', ''.join(items)'
      - ''''''
    - source: DatasetGenerator.get_response_from_llm
      target: strategy
      target_inputs: []
    - source: DatasetGenerator.get_response_from_llm
      target: get_context_and_prompt
      target_inputs:
      - query
      - context
      - code_qa
    - source: DatasetGenerator.get_response_from_llm
      target: logging.error
      target_inputs:
      - 'f''Failed to generate model response: {error}'''
    - source: DatasetGenerator.get_response_from_llm
      target: math.ceil
      target_inputs:
      - context_size / 0.7
    - source: DatasetGenerator.get_response_from_llm
      target: re.sub
      target_inputs:
      - '''\\n\\s*\\n'''
      - '''\n\n'''
      - self.llm(prompt)
    - source: DatasetGenerator.get_response_from_llm
      target: self.llm
      target_inputs:
      - prompt
    - source: DatasetGenerator.get_response_from_llm
      target: logging.info
      target_inputs:
      - 'f''Response: {response}'''
    - source: DatasetGenerator.get_context_and_prompt
      target: self.model_config['prompt_template'].format
      target_inputs: []
    - source: DatasetGenerator.get_context_and_prompt
      target: len
      target_inputs:
      - self.llm.tokenize(prompt)
    - source: DatasetGenerator.get_context_and_prompt
      target: self.llm.tokenize
      target_inputs:
      - prompt
    - source: DatasetGenerator.process_question
      target: question_id.endswith
      target_inputs:
      - '''purpose'''
    - source: DatasetGenerator.process_question
      target: info.get
      target_inputs:
      - question_id
      - ''''''
    - source: DatasetGenerator.process_question
      target: DatasetGenerator.get_response_from_llm
      target_inputs:
      - self
      - query
      - context
      target_returns:
      - response
      - ''''''
      - (prompt, context_size)
    - source: DatasetGenerator.process_question
      target: clean_and_get_unique_elements
      target_inputs:
      - str(info.get(question_id, ''))
      target_returns:
      - ''', ''.join(cleaned_elements)'
    - source: DatasetGenerator.process_question
      target: str
      target_inputs:
      - response
    - source: DatasetGenerator.process_question
      target: str(response).strip
      target_inputs: []
    - source: DatasetGenerator.process_question
      target: self.instruct_list.append
      target_inputs:
      - '{''instruction'': query, ''input'': context, ''output'': response_str}'
    - source: DatasetGenerator.get_string_from_info
      target: item.strip
      target_inputs: []
    - source: DatasetGenerator.get_string_from_info
      target: str(info[item_type]).split
      target_inputs:
      - ''','''
    - source: DatasetGenerator.get_string_from_info
      target: str
      target_inputs:
      - info[item_type]
    - source: DatasetGenerator.get_string_from_info
      target: ''', ''.join'
      target_inputs:
      - items
    - source: DatasetGenerator.process_question_type
      target: question_text.format
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: DatasetGenerator.process_question
      target_inputs:
      - self
      - question_type
      - question_id
      - query
      - context
      - info
      target_returns: []
    - source: DatasetGenerator.process_question_type
      target: self.file_details['classes'].items
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: class_info.items
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: key.startswith
      target_inputs:
      - '''class_method_'''
    - source: DatasetGenerator.process_question_type
      target: len
      target_inputs:
      - '''class_method_'''
    - source: DatasetGenerator.process_question_type
      target: self.file_details[self.question_mapping[question_type]].items
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: DatasetGenerator.get_string_from_info
      target_inputs:
      - info
      - item_type
      target_returns:
      - ''', ''.join(items)'
      - ''''''
    - source: DatasetGenerator.process_question_type
      target: ''', ''.join'
      target_inputs:
      - '[s for s in [variables_string, inputs_string] if s]'
    - source: DatasetGenerator.process_question_type
      target: clean_and_get_unique_elements
      target_inputs:
      - combined_string
      target_returns:
      - ''', ''.join(cleaned_elements)'
    - source: DatasetGenerator.generate
      target: DatasetGenerator.process_question_type
      target_inputs:
      - self
      - question_type
      - question_id
      - question_text
      target_returns: []
    - source: clean_and_get_unique_elements
      target: set
      target_inputs:
      - (re.sub('[^\\w\\-_>\\s:/.]', '', element.strip()) for element in re.sub('\\s+',
        ' ', input_str).split(','))
    - source: clean_and_get_unique_elements
      target: re.sub
      target_inputs:
      - '''\\s+'''
      - ''' '''
      - input_str
    - source: clean_and_get_unique_elements
      target: element.strip
      target_inputs: []
    - source: clean_and_get_unique_elements
      target: re.sub('\\s+', ' ', input_str).split
      target_inputs:
      - ''','''
    - source: clean_and_get_unique_elements
      target: ''', ''.join'
      target_inputs:
      - cleaned_elements
    - source: get_python_datasets
      target: DatasetGenerator
      target_inputs:
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      target_returns: []
    - source: get_python_datasets
      target: generator.generate
      target_inputs: []
functions:
  clean_and_get_unique_elements:
    function_name: clean_and_get_unique_elements
    function_code: "def clean_and_get_unique_elements(input_str: str) -> str:\n  \
      \  \"\"\"\n    Clean input string and return string of unique elements.\n  \
      \  Args:\n        input_str (str): The input string to be cleaned.\n    Returns:\n\
      \        str: The cleaned string.\n    \"\"\"\n    cleaned_elements = set((re.sub('[^\\\
      \\w\\\\-_>\\\\s:/.]', '', element.strip()) for element in re.sub('\\\\s+', '\
      \ ', input_str).split(',')))\n    return ', '.join(cleaned_elements)"
    function_ast: 'FunctionDef(name=''clean_and_get_unique_elements'', args=arguments(posonlyargs=[],
      args=[arg(arg=''input_str'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
      kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n    Clean
      input string and return string of unique elements.\n    Args:\n        input_str
      (str): The input string to be cleaned.\n    Returns:\n        str: The cleaned
      string.\n    '')), Assign(targets=[Name(id=''cleaned_elements'', ctx=Store())],
      value=Call(func=Name(id=''set'', ctx=Load()), args=[GeneratorExp(elt=Call(func=Attribute(value=Name(id=''re'',
      ctx=Load()), attr=''sub'', ctx=Load()), args=[Constant(value=''[^\\w\\-_>\\s:/.]''),
      Constant(value=''''), Call(func=Attribute(value=Name(id=''element'', ctx=Load()),
      attr=''strip'', ctx=Load()), args=[], keywords=[])], keywords=[]), generators=[comprehension(target=Name(id=''element'',
      ctx=Store()), iter=Call(func=Attribute(value=Call(func=Attribute(value=Name(id=''re'',
      ctx=Load()), attr=''sub'', ctx=Load()), args=[Constant(value=''\\s+''), Constant(value=''
      ''), Name(id=''input_str'', ctx=Load())], keywords=[]), attr=''split'', ctx=Load()),
      args=[Constant(value='','')], keywords=[]), ifs=[], is_async=0)])], keywords=[])),
      Return(value=Call(func=Attribute(value=Constant(value='', ''), attr=''join'',
      ctx=Load()), args=[Name(id=''cleaned_elements'', ctx=Load())], keywords=[]))],
      decorator_list=[], returns=Name(id=''str'', ctx=Load()))'
    function_docstring: "\n    Clean input string and return string of unique elements.\n\
      \    Args:\n        input_str (str): The input string to be cleaned.\n    Returns:\n\
      \        str: The cleaned string.\n    "
    function_inputs:
    - input_str
    function_defaults: []
    function_returns:
    - ''', ''.join(cleaned_elements)'
    function_calls:
    - set
    - re.sub
    - element.strip
    - re.sub('\\s+', ' ', input_str).split
    - ''', ''.join'
    function_call_inputs:
      set:
      - (re.sub('[^\\w\\-_>\\s:/.]', '', element.strip()) for element in re.sub('\\s+',
        ' ', input_str).split(','))
      re.sub:
      - '''\\s+'''
      - ''' '''
      - input_str
      element.strip: []
      re.sub('\\s+', ' ', input_str).split:
      - ''','''
      ''', ''.join':
      - cleaned_elements
    function_variables:
    - cleaned_elements
    function_decorators: []
    function_annotations: []
    function_properties: []
  get_python_datasets:
    function_name: get_python_datasets
    function_code: "def get_python_datasets(file_path: str, file_details: Dict, base_name:\
      \ str, questions: List[Dict], model_config: Dict) -> Tuple[List[Dict], List[Dict]]:\n\
      \    \"\"\"\n    Extract information from a Python file and return it in JSON\
      \ format.\n    Args:\n        file_path (str): The path to the Python file.\n\
      \        file_details (Dict): The details of the file.\n        base_name (str):\
      \ The base Python code filename.\n        questions (List[Dict]): The list of\
      \ questions.\n        llm (object): The language model to be used for generating\
      \ responses.\n        prompt (str): The prompt to be used for generating responses.\n\
      \    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information\
      \ in JSON format.\n    \"\"\"\n    generator = DatasetGenerator(file_path, file_details,\
      \ base_name, questions, model_config)\n    return generator.generate()"
    function_ast: 'FunctionDef(name=''get_python_datasets'', args=arguments(posonlyargs=[],
      args=[arg(arg=''file_path'', annotation=Name(id=''str'', ctx=Load())), arg(arg=''file_details'',
      annotation=Name(id=''Dict'', ctx=Load())), arg(arg=''base_name'', annotation=Name(id=''str'',
      ctx=Load())), arg(arg=''questions'', annotation=Subscript(value=Name(id=''List'',
      ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load())), arg(arg=''model_config'',
      annotation=Name(id=''Dict'', ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]),
      body=[Expr(value=Constant(value=''\n    Extract information from a Python file
      and return it in JSON format.\n    Args:\n        file_path (str): The path
      to the Python file.\n        file_details (Dict): The details of the file.\n        base_name
      (str): The base Python code filename.\n        questions (List[Dict]): The list
      of questions.\n        llm (object): The language model to be used for generating
      responses.\n        prompt (str): The prompt to be used for generating responses.\n    Returns:\n        Tuple[List[Dict],
      List[Dict]]: Extracted information in JSON format.\n    '')), Assign(targets=[Name(id=''generator'',
      ctx=Store())], value=Call(func=Name(id=''DatasetGenerator'', ctx=Load()), args=[Name(id=''file_path'',
      ctx=Load()), Name(id=''file_details'', ctx=Load()), Name(id=''base_name'', ctx=Load()),
      Name(id=''questions'', ctx=Load()), Name(id=''model_config'', ctx=Load())],
      keywords=[])), Return(value=Call(func=Attribute(value=Name(id=''generator'',
      ctx=Load()), attr=''generate'', ctx=Load()), args=[], keywords=[]))], decorator_list=[],
      returns=Subscript(value=Name(id=''Tuple'', ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id=''List'',
      ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load()), Subscript(value=Name(id=''List'',
      ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load())], ctx=Load()),
      ctx=Load()))'
    function_docstring: "\n    Extract information from a Python file and return it\
      \ in JSON format.\n    Args:\n        file_path (str): The path to the Python\
      \ file.\n        file_details (Dict): The details of the file.\n        base_name\
      \ (str): The base Python code filename.\n        questions (List[Dict]): The\
      \ list of questions.\n        llm (object): The language model to be used for\
      \ generating responses.\n        prompt (str): The prompt to be used for generating\
      \ responses.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted\
      \ information in JSON format.\n    "
    function_inputs:
    - file_path
    - file_details
    - base_name
    - questions
    - model_config
    function_defaults: []
    function_returns:
    - generator.generate()
    function_calls:
    - DatasetGenerator
    - generator.generate
    function_call_inputs:
      DatasetGenerator:
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      generator.generate: []
    function_variables:
    - generator
    function_decorators: []
    function_annotations: []
    function_properties: []
classes:
  DatasetGenerator:
    class_name: DatasetGenerator
    class_code: "class DatasetGenerator:\n    \"\"\"\n    Generate JSON formatted\
      \ dictionary outputs for a Python file.\n    Attributes:\n        file_path\
      \ (str): The path to the Python file.\n        file_details (Dict[str, Any]):\
      \ Details of the Python file.\n        base_name (str): The base name of the\
      \ Python file.\n        questions (List[Dict[str, str]]): Questions for generating\
      \ responses.\n        instruct_list (List[Dict[str, str]]): Storage for generated\
      \ instructions.\n        question_mapping (Dict[str, str]): Mapping of question\
      \ types to keys in file details.\n        use_llm (bool): Flag indicating if\
      \ a language model should be used.\n        llm (object): The language model\
      \ for generating responses.\n        prompt (str): The prompt format for querying\
      \ the language model.\n    Methods:\n        add_to_list(list_to_update: List[Dict],\
      \ query: str, response: str, \n        additional_field=None) -> List[Dict]:\
      \ \n            Add response to the instruct list.\n        get_response_from_llm(query:\
      \ str, context: str) -> str:\n            Get language model response to query\
      \ for given context.\n        process_question(question_type: str, question_id:\
      \ str, query: str, \n        context: str, info: Dict) -> None:\n          \
      \  Process question and add generated response to the instruct_list.\n     \
      \   process_question_type(question_type: str, question_id: str, \n        question_text:\
      \ str) -> None:\n            Process question related to file, function, class,\
      \ or method.\n        generate() -> Tuple[List[Dict], List[Dict]]:\n       \
      \     Generate responses for all the questions and return the instruct_list.\n\
      \    \"\"\"\n\n    def __init__(self, file_path: str, file_details: Dict, base_name:\
      \ str, questions: List[Dict], model_config: Dict) -> None:\n        \"\"\"\n\
      \        Initialize the DatasetGenerator class.\n        Args:\n           \
      \ file_path (str): The path to the Python file.\n            file_details (Dict[str,\
      \ Any]): Details of the Python file.\n            base_name (str): The base\
      \ name of the Python file.\n            questions (List[Dict[str, str]]): Questions\
      \ for generating responses.\n            model_config (Dict): Configuration\
      \ for the language model.\n        Returns:\n            None\n        \"\"\"\
      \n        self.file_path = file_path\n        self.file_details = file_details\n\
      \        self.base_name = base_name\n        self.questions = questions\n  \
      \      self.model_config = model_config\n        self.llm = model_config['model']\n\
      \        if self.llm is None:\n            self.use_llm = False\n        else:\n\
      \            self.use_llm = True\n        self.instruct_list = []\n        self.question_mapping\
      \ = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method':\
      \ 'classes'}\n\n    def add_to_list(self, list_to_update: List[Dict], query:\
      \ str, response: str, additional_field=None) -> List[Dict]:\n        \"\"\"\n\
      \        Add response to the instruct list.\n        Args:\n            list_to_update\
      \ (List[Dict]): The list to update.\n            query (str): The query to be\
      \ added.\n            response (str): The response to be added.\n          \
      \  additional_field (Any): Additional field to be added.\n        Returns:\n\
      \            List[Dict]: The updated list.\n        \"\"\"\n        list_to_update.append({'instruction':\
      \ query, 'input': additional_field, 'output': response})\n        return list_to_update\n\
      \n    def get_response_from_llm(self, query: str, context: str) -> str:\n  \
      \      \"\"\"\n        Get language model response to query for given context.\n\
      \        Args:\n            query (str): The query to be used for generating\
      \ the response.\n            context (str): The context to be used for generating\
      \ the response.\n        Returns:\n            str: The generated response.\n\
      \        \"\"\"\n\n        def get_context_and_prompt(query, context, code_qa):\n\
      \            full_context = f'{context}\\nCODE Q and A:\\n{code_qa}'\n     \
      \       prompt = self.model_config['prompt_template'].format(context=full_context,\
      \ query=query)\n            context_size = len(self.llm.tokenize(prompt))\n\
      \            return (prompt, context_size)\n        max_context_length = self.model_config['inference_model']['model_params']['context_length']\n\
      \        excluded_instructions = ['Call code graph', 'Docstring']\n        code_qa\
      \ = '\\n'.join([f\"Q: {item['instruction']} \\nA: {item['output']}\" for item\
      \ in self.instruct_list if not any((item['instruction'].startswith(prefix) for\
      \ prefix in excluded_instructions))])\n        context_strategies = [lambda:\
      \ '```python\\n' + str(context) + '\\n```', lambda: '```python\\n' + str(self.file_details['file_info']['file_code_simplified'])\
      \ + '\\n```', lambda: self.get_string_from_info(self.file_details['file_info'],\
      \ 'file_summary'), lambda: '']\n        for strategy in context_strategies:\n\
      \            context = strategy()\n            prompt, context_size = get_context_and_prompt(query,\
      \ context, code_qa)\n            if context_size <= 0.7 * max_context_length:\n\
      \                break\n        else:\n            logging.error(f'Model response\
      \ failed, increase py2dataset_model_config.yaml context_length > {math.ceil(context_size\
      \ / 0.7)}')\n            return ''\n        try:\n            response = re.sub('\\\
      \\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n            logging.info(f'Response:\
      \ {response}')\n        except Exception as error:\n            logging.error(f'Failed\
      \ to generate model response: {error}')\n            response = ''\n       \
      \ return response\n\n    def process_question(self, question_type: str, question_id:\
      \ str, query: str, context: str, info: Dict) -> None:\n        \"\"\"\n    \
      \    Process question and add the generated response to the instruct_list.\n\
      \        Args:\n            question_type (str): The type of question to be\
      \ processed.\n            question_id (str): The ID of the question to be processed.\n\
      \            query (str): The query to be processed.\n            context (str):\
      \ The context to be used for generating the response.\n            info (Dict):\
      \ The information of the Python file.\n        Returns:\n            None\n\
      \        \"\"\"\n        if question_id.endswith('code_graph') or question_id.endswith('docstring'):\n\
      \            response = info.get(question_id, {})\n        else:\n         \
      \   response = self.get_response_from_llm(query, context) if self.use_llm and\
      \ question_id.endswith('purpose') else clean_and_get_unique_elements(str(info.get(question_id,\
      \ '')))\n        if question_type == 'file':\n            context = self.file_details['file_info']['file_code']\n\
      \        if response and response != 'None':\n            response_str = str(response).strip()\n\
      \            if response_str:\n                self.instruct_list.append({'instruction':\
      \ query, 'input': context, 'output': response_str})\n\n    @staticmethod\n \
      \   def get_string_from_info(info, item_type):\n        \"\"\"\n        Get\
      \ string from info dictionary.\n        Args:\n            info (Dict): The\
      \ information of the Python file.\n            item_type (str): The type of\
      \ item to get the string from.\n        Returns:\n            str: The string\
      \ from the info.\n        \"\"\"\n        if info[item_type]:\n            items\
      \ = [item.strip() for item in str(info[item_type]).split(',') if item]\n   \
      \         return ', '.join(items)\n        return ''\n\n    def process_question_type(self,\
      \ question_type: str, question_id: str, question_text: str) -> None:\n     \
      \   \"\"\"\n        Process questions related to a file, function, class, or\
      \ method.\n        Args:\n            question_type (str): The type of question\
      \ to be processed.\n            question_id (str): The ID of the question to\
      \ be processed.\n            question_text (str): The text of the question to\
      \ be processed.\n        Returns:\n            None\n        \"\"\"\n      \
      \  if question_type == 'file':\n            query = question_text.format(filename=self.base_name)\n\
      \            info = self.file_details['file_info']\n            context = self.file_details['file_info']['file_code']\n\
      \            self.process_question(question_type, question_id, query, context,\
      \ info)\n        elif question_type == 'method':\n            for class_name,\
      \ class_info in self.file_details['classes'].items():\n                for key,\
      \ method_info in class_info.items():\n                    if key.startswith('class_method_'):\n\
      \                        method_name = key[len('class_method_'):]\n        \
      \                context = method_info['method_code']\n                    \
      \    mapping = {'class_name': class_name, 'method_name': method_name}\n    \
      \                    query = question_text.format(filename=self.base_name, **mapping)\n\
      \                        self.process_question(question_type, question_id, query,\
      \ context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n\
      \                context = info[f'{question_type}_code']\n                mapping\
      \ = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_purpose'\
      \ and self.use_llm:\n                    variables_string = self.get_string_from_info(info,\
      \ f'{question_type}_variables')\n                    inputs_string = self.get_string_from_info(info,\
      \ f'{question_type}_inputs')\n                    combined_string = ', '.join([s\
      \ for s in [variables_string, inputs_string] if s])\n                    mapping[f'{question_type}_variables']\
      \ = clean_and_get_unique_elements(combined_string)\n                    if question_type\
      \ == 'class':\n                        methods_string = self.get_string_from_info(info,\
      \ f'{question_type}_methods')\n                        mapping[f'{question_type}_methods']\
      \ = methods_string\n                query = question_text.format(filename=self.base_name,\
      \ **mapping)\n                self.process_question(question_type, question_id,\
      \ query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n\
      \        \"\"\"\n        Generate responses for all the questions and returns\
      \ the instruct_list.\n        Args:\n            None\n        Returns:\n  \
      \          Tuple[List[Dict], List[Dict]]: The generated question-answer pairs\
      \ and instructions.\n        \"\"\"\n        for question in self.questions:\n\
      \            self.process_question_type(question['type'], question['id'], question['text'])\n\
      \        return self.instruct_list"
    class_ast: 'ClassDef(name=''DatasetGenerator'', bases=[], keywords=[], body=[Expr(value=Constant(value=''\n    Generate
      JSON formatted dictionary outputs for a Python file.\n    Attributes:\n        file_path
      (str): The path to the Python file.\n        file_details (Dict[str, Any]):
      Details of the Python file.\n        base_name (str): The base name of the Python
      file.\n        questions (List[Dict[str, str]]): Questions for generating responses.\n        instruct_list
      (List[Dict[str, str]]): Storage for generated instructions.\n        question_mapping
      (Dict[str, str]): Mapping of question types to keys in file details.\n        use_llm
      (bool): Flag indicating if a language model should be used.\n        llm (object):
      The language model for generating responses.\n        prompt (str): The prompt
      format for querying the language model.\n    Methods:\n        add_to_list(list_to_update:
      List[Dict], query: str, response: str, \n        additional_field=None) -> List[Dict]:
      \n            Add response to the instruct list.\n        get_response_from_llm(query:
      str, context: str) -> str:\n            Get language model response to query
      for given context.\n        process_question(question_type: str, question_id:
      str, query: str, \n        context: str, info: Dict) -> None:\n            Process
      question and add generated response to the instruct_list.\n        process_question_type(question_type:
      str, question_id: str, \n        question_text: str) -> None:\n            Process
      question related to file, function, class, or method.\n        generate() ->
      Tuple[List[Dict], List[Dict]]:\n            Generate responses for all the questions
      and return the instruct_list.\n    '')), FunctionDef(name=''__init__'', args=arguments(posonlyargs=[],
      args=[arg(arg=''self''), arg(arg=''file_path'', annotation=Name(id=''str'',
      ctx=Load())), arg(arg=''file_details'', annotation=Name(id=''Dict'', ctx=Load())),
      arg(arg=''base_name'', annotation=Name(id=''str'', ctx=Load())), arg(arg=''questions'',
      annotation=Subscript(value=Name(id=''List'', ctx=Load()), slice=Name(id=''Dict'',
      ctx=Load()), ctx=Load())), arg(arg=''model_config'', annotation=Name(id=''Dict'',
      ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Initialize
      the DatasetGenerator class.\n        Args:\n            file_path (str): The
      path to the Python file.\n            file_details (Dict[str, Any]): Details
      of the Python file.\n            base_name (str): The base name of the Python
      file.\n            questions (List[Dict[str, str]]): Questions for generating
      responses.\n            model_config (Dict): Configuration for the language
      model.\n        Returns:\n            None\n        '')), Assign(targets=[Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_path'', ctx=Store())], value=Name(id=''file_path'',
      ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()),
      attr=''file_details'', ctx=Store())], value=Name(id=''file_details'', ctx=Load())),
      Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'',
      ctx=Store())], value=Name(id=''base_name'', ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''questions'', ctx=Store())], value=Name(id=''questions'',
      ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()),
      attr=''model_config'', ctx=Store())], value=Name(id=''model_config'', ctx=Load())),
      Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''llm'',
      ctx=Store())], value=Subscript(value=Name(id=''model_config'', ctx=Load()),
      slice=Constant(value=''model''), ctx=Load())), If(test=Compare(left=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''llm'', ctx=Load()), ops=[Is()], comparators=[Constant(value=None)]),
      body=[Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''use_llm'',
      ctx=Store())], value=Constant(value=False))], orelse=[Assign(targets=[Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''use_llm'', ctx=Store())], value=Constant(value=True))]),
      Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''instruct_list'',
      ctx=Store())], value=List(elts=[], ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''question_mapping'', ctx=Store())], value=Dict(keys=[Constant(value=''file''),
      Constant(value=''function''), Constant(value=''class''), Constant(value=''method'')],
      values=[Constant(value=''file''), Constant(value=''functions''), Constant(value=''classes''),
      Constant(value=''classes'')]))], decorator_list=[], returns=Constant(value=None)),
      FunctionDef(name=''add_to_list'', args=arguments(posonlyargs=[], args=[arg(arg=''self''),
      arg(arg=''list_to_update'', annotation=Subscript(value=Name(id=''List'', ctx=Load()),
      slice=Name(id=''Dict'', ctx=Load()), ctx=Load())), arg(arg=''query'', annotation=Name(id=''str'',
      ctx=Load())), arg(arg=''response'', annotation=Name(id=''str'', ctx=Load())),
      arg(arg=''additional_field'')], kwonlyargs=[], kw_defaults=[], defaults=[Constant(value=None)]),
      body=[Expr(value=Constant(value=''\n        Add response to the instruct list.\n        Args:\n            list_to_update
      (List[Dict]): The list to update.\n            query (str): The query to be
      added.\n            response (str): The response to be added.\n            additional_field
      (Any): Additional field to be added.\n        Returns:\n            List[Dict]:
      The updated list.\n        '')), Expr(value=Call(func=Attribute(value=Name(id=''list_to_update'',
      ctx=Load()), attr=''append'', ctx=Load()), args=[Dict(keys=[Constant(value=''instruction''),
      Constant(value=''input''), Constant(value=''output'')], values=[Name(id=''query'',
      ctx=Load()), Name(id=''additional_field'', ctx=Load()), Name(id=''response'',
      ctx=Load())])], keywords=[])), Return(value=Name(id=''list_to_update'', ctx=Load()))],
      decorator_list=[], returns=Subscript(value=Name(id=''List'', ctx=Load()), slice=Name(id=''Dict'',
      ctx=Load()), ctx=Load())), FunctionDef(name=''get_response_from_llm'', args=arguments(posonlyargs=[],
      args=[arg(arg=''self''), arg(arg=''query'', annotation=Name(id=''str'', ctx=Load())),
      arg(arg=''context'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
      kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Get
      language model response to query for given context.\n        Args:\n            query
      (str): The query to be used for generating the response.\n            context
      (str): The context to be used for generating the response.\n        Returns:\n            str:
      The generated response.\n        '')), FunctionDef(name=''get_context_and_prompt'',
      args=arguments(posonlyargs=[], args=[arg(arg=''query''), arg(arg=''context''),
      arg(arg=''code_qa'')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id=''full_context'',
      ctx=Store())], value=JoinedStr(values=[FormattedValue(value=Name(id=''context'',
      ctx=Load()), conversion=-1), Constant(value=''\nCODE Q and A:\n''), FormattedValue(value=Name(id=''code_qa'',
      ctx=Load()), conversion=-1)])), Assign(targets=[Name(id=''prompt'', ctx=Store())],
      value=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''model_config'', ctx=Load()), slice=Constant(value=''prompt_template''),
      ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''context'',
      value=Name(id=''full_context'', ctx=Load())), keyword(arg=''query'', value=Name(id=''query'',
      ctx=Load()))])), Assign(targets=[Name(id=''context_size'', ctx=Store())], value=Call(func=Name(id=''len'',
      ctx=Load()), args=[Call(func=Attribute(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''llm'', ctx=Load()), attr=''tokenize'', ctx=Load()), args=[Name(id=''prompt'',
      ctx=Load())], keywords=[])], keywords=[])), Return(value=Tuple(elts=[Name(id=''prompt'',
      ctx=Load()), Name(id=''context_size'', ctx=Load())], ctx=Load()))], decorator_list=[]),
      Assign(targets=[Name(id=''max_context_length'', ctx=Store())], value=Subscript(value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''model_config'', ctx=Load()), slice=Constant(value=''inference_model''),
      ctx=Load()), slice=Constant(value=''model_params''), ctx=Load()), slice=Constant(value=''context_length''),
      ctx=Load())), Assign(targets=[Name(id=''excluded_instructions'', ctx=Store())],
      value=List(elts=[Constant(value=''Call code graph''), Constant(value=''Docstring'')],
      ctx=Load())), Assign(targets=[Name(id=''code_qa'', ctx=Store())], value=Call(func=Attribute(value=Constant(value=''\n''),
      attr=''join'', ctx=Load()), args=[ListComp(elt=JoinedStr(values=[Constant(value=''Q:
      ''), FormattedValue(value=Subscript(value=Name(id=''item'', ctx=Load()), slice=Constant(value=''instruction''),
      ctx=Load()), conversion=-1), Constant(value='' \nA: ''), FormattedValue(value=Subscript(value=Name(id=''item'',
      ctx=Load()), slice=Constant(value=''output''), ctx=Load()), conversion=-1)]),
      generators=[comprehension(target=Name(id=''item'', ctx=Store()), iter=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''instruct_list'', ctx=Load()), ifs=[UnaryOp(op=Not(), operand=Call(func=Name(id=''any'',
      ctx=Load()), args=[GeneratorExp(elt=Call(func=Attribute(value=Subscript(value=Name(id=''item'',
      ctx=Load()), slice=Constant(value=''instruction''), ctx=Load()), attr=''startswith'',
      ctx=Load()), args=[Name(id=''prefix'', ctx=Load())], keywords=[]), generators=[comprehension(target=Name(id=''prefix'',
      ctx=Store()), iter=Name(id=''excluded_instructions'', ctx=Load()), ifs=[], is_async=0)])],
      keywords=[]))], is_async=0)])], keywords=[])), Assign(targets=[Name(id=''context_strategies'',
      ctx=Store())], value=List(elts=[Lambda(args=arguments(posonlyargs=[], args=[],
      kwonlyargs=[], kw_defaults=[], defaults=[]), body=BinOp(left=BinOp(left=Constant(value=''```python\n''),
      op=Add(), right=Call(func=Name(id=''str'', ctx=Load()), args=[Name(id=''context'',
      ctx=Load())], keywords=[])), op=Add(), right=Constant(value=''\n```''))), Lambda(args=arguments(posonlyargs=[],
      args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=BinOp(left=BinOp(left=Constant(value=''```python\n''),
      op=Add(), right=Call(func=Name(id=''str'', ctx=Load()), args=[Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
      ctx=Load()), slice=Constant(value=''file_code_simplified''), ctx=Load())], keywords=[])),
      op=Add(), right=Constant(value=''\n```''))), Lambda(args=arguments(posonlyargs=[],
      args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=Call(func=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''get_string_from_info'', ctx=Load()), args=[Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
      ctx=Load()), Constant(value=''file_summary'')], keywords=[])), Lambda(args=arguments(posonlyargs=[],
      args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=Constant(value=''''))],
      ctx=Load())), For(target=Name(id=''strategy'', ctx=Store()), iter=Name(id=''context_strategies'',
      ctx=Load()), body=[Assign(targets=[Name(id=''context'', ctx=Store())], value=Call(func=Name(id=''strategy'',
      ctx=Load()), args=[], keywords=[])), Assign(targets=[Tuple(elts=[Name(id=''prompt'',
      ctx=Store()), Name(id=''context_size'', ctx=Store())], ctx=Store())], value=Call(func=Name(id=''get_context_and_prompt'',
      ctx=Load()), args=[Name(id=''query'', ctx=Load()), Name(id=''context'', ctx=Load()),
      Name(id=''code_qa'', ctx=Load())], keywords=[])), If(test=Compare(left=Name(id=''context_size'',
      ctx=Load()), ops=[LtE()], comparators=[BinOp(left=Constant(value=0.7), op=Mult(),
      right=Name(id=''max_context_length'', ctx=Load()))]), body=[Break()], orelse=[])],
      orelse=[Expr(value=Call(func=Attribute(value=Name(id=''logging'', ctx=Load()),
      attr=''error'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Model
      response failed, increase py2dataset_model_config.yaml context_length > ''),
      FormattedValue(value=Call(func=Attribute(value=Name(id=''math'', ctx=Load()),
      attr=''ceil'', ctx=Load()), args=[BinOp(left=Name(id=''context_size'', ctx=Load()),
      op=Div(), right=Constant(value=0.7))], keywords=[]), conversion=-1)])], keywords=[])),
      Return(value=Constant(value=''''))]), Try(body=[Assign(targets=[Name(id=''response'',
      ctx=Store())], value=Call(func=Attribute(value=Name(id=''re'', ctx=Load()),
      attr=''sub'', ctx=Load()), args=[Constant(value=''\\n\\s*\\n''), Constant(value=''\n\n''),
      Call(func=Attribute(value=Name(id=''self'', ctx=Load()), attr=''llm'', ctx=Load()),
      args=[Name(id=''prompt'', ctx=Load())], keywords=[])], keywords=[])), Expr(value=Call(func=Attribute(value=Name(id=''logging'',
      ctx=Load()), attr=''info'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Response:
      ''), FormattedValue(value=Name(id=''response'', ctx=Load()), conversion=-1)])],
      keywords=[]))], handlers=[ExceptHandler(type=Name(id=''Exception'', ctx=Load()),
      name=''error'', body=[Expr(value=Call(func=Attribute(value=Name(id=''logging'',
      ctx=Load()), attr=''error'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Failed
      to generate model response: ''), FormattedValue(value=Name(id=''error'', ctx=Load()),
      conversion=-1)])], keywords=[])), Assign(targets=[Name(id=''response'', ctx=Store())],
      value=Constant(value=''''))])], orelse=[], finalbody=[]), Return(value=Name(id=''response'',
      ctx=Load()))], decorator_list=[], returns=Name(id=''str'', ctx=Load())), FunctionDef(name=''process_question'',
      args=arguments(posonlyargs=[], args=[arg(arg=''self''), arg(arg=''question_type'',
      annotation=Name(id=''str'', ctx=Load())), arg(arg=''question_id'', annotation=Name(id=''str'',
      ctx=Load())), arg(arg=''query'', annotation=Name(id=''str'', ctx=Load())), arg(arg=''context'',
      annotation=Name(id=''str'', ctx=Load())), arg(arg=''info'', annotation=Name(id=''Dict'',
      ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Process
      question and add the generated response to the instruct_list.\n        Args:\n            question_type
      (str): The type of question to be processed.\n            question_id (str):
      The ID of the question to be processed.\n            query (str): The query
      to be processed.\n            context (str): The context to be used for generating
      the response.\n            info (Dict): The information of the Python file.\n        Returns:\n            None\n        '')),
      If(test=BoolOp(op=Or(), values=[Call(func=Attribute(value=Name(id=''question_id'',
      ctx=Load()), attr=''endswith'', ctx=Load()), args=[Constant(value=''code_graph'')],
      keywords=[]), Call(func=Attribute(value=Name(id=''question_id'', ctx=Load()),
      attr=''endswith'', ctx=Load()), args=[Constant(value=''docstring'')], keywords=[])]),
      body=[Assign(targets=[Name(id=''response'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''info'',
      ctx=Load()), attr=''get'', ctx=Load()), args=[Name(id=''question_id'', ctx=Load()),
      Dict(keys=[], values=[])], keywords=[]))], orelse=[Assign(targets=[Name(id=''response'',
      ctx=Store())], value=IfExp(test=BoolOp(op=And(), values=[Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''use_llm'', ctx=Load()), Call(func=Attribute(value=Name(id=''question_id'',
      ctx=Load()), attr=''endswith'', ctx=Load()), args=[Constant(value=''purpose'')],
      keywords=[])]), body=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
      attr=''get_response_from_llm'', ctx=Load()), args=[Name(id=''query'', ctx=Load()),
      Name(id=''context'', ctx=Load())], keywords=[]), orelse=Call(func=Name(id=''clean_and_get_unique_elements'',
      ctx=Load()), args=[Call(func=Name(id=''str'', ctx=Load()), args=[Call(func=Attribute(value=Name(id=''info'',
      ctx=Load()), attr=''get'', ctx=Load()), args=[Name(id=''question_id'', ctx=Load()),
      Constant(value='''')], keywords=[])], keywords=[])], keywords=[])))]), If(test=Compare(left=Name(id=''question_type'',
      ctx=Load()), ops=[Eq()], comparators=[Constant(value=''file'')]), body=[Assign(targets=[Name(id=''context'',
      ctx=Store())], value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
      ctx=Load()), slice=Constant(value=''file_code''), ctx=Load()))], orelse=[]),
      If(test=BoolOp(op=And(), values=[Name(id=''response'', ctx=Load()), Compare(left=Name(id=''response'',
      ctx=Load()), ops=[NotEq()], comparators=[Constant(value=''None'')])]), body=[Assign(targets=[Name(id=''response_str'',
      ctx=Store())], value=Call(func=Attribute(value=Call(func=Name(id=''str'', ctx=Load()),
      args=[Name(id=''response'', ctx=Load())], keywords=[]), attr=''strip'', ctx=Load()),
      args=[], keywords=[])), If(test=Name(id=''response_str'', ctx=Load()), body=[Expr(value=Call(func=Attribute(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''instruct_list'', ctx=Load()), attr=''append'', ctx=Load()),
      args=[Dict(keys=[Constant(value=''instruction''), Constant(value=''input''),
      Constant(value=''output'')], values=[Name(id=''query'', ctx=Load()), Name(id=''context'',
      ctx=Load()), Name(id=''response_str'', ctx=Load())])], keywords=[]))], orelse=[])],
      orelse=[])], decorator_list=[], returns=Constant(value=None)), FunctionDef(name=''get_string_from_info'',
      args=arguments(posonlyargs=[], args=[arg(arg=''info''), arg(arg=''item_type'')],
      kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Get
      string from info dictionary.\n        Args:\n            info (Dict): The information
      of the Python file.\n            item_type (str): The type of item to get the
      string from.\n        Returns:\n            str: The string from the info.\n        '')),
      If(test=Subscript(value=Name(id=''info'', ctx=Load()), slice=Name(id=''item_type'',
      ctx=Load()), ctx=Load()), body=[Assign(targets=[Name(id=''items'', ctx=Store())],
      value=ListComp(elt=Call(func=Attribute(value=Name(id=''item'', ctx=Load()),
      attr=''strip'', ctx=Load()), args=[], keywords=[]), generators=[comprehension(target=Name(id=''item'',
      ctx=Store()), iter=Call(func=Attribute(value=Call(func=Name(id=''str'', ctx=Load()),
      args=[Subscript(value=Name(id=''info'', ctx=Load()), slice=Name(id=''item_type'',
      ctx=Load()), ctx=Load())], keywords=[]), attr=''split'', ctx=Load()), args=[Constant(value='','')],
      keywords=[]), ifs=[Name(id=''item'', ctx=Load())], is_async=0)])), Return(value=Call(func=Attribute(value=Constant(value='',
      ''), attr=''join'', ctx=Load()), args=[Name(id=''items'', ctx=Load())], keywords=[]))],
      orelse=[]), Return(value=Constant(value=''''))], decorator_list=[Name(id=''staticmethod'',
      ctx=Load())]), FunctionDef(name=''process_question_type'', args=arguments(posonlyargs=[],
      args=[arg(arg=''self''), arg(arg=''question_type'', annotation=Name(id=''str'',
      ctx=Load())), arg(arg=''question_id'', annotation=Name(id=''str'', ctx=Load())),
      arg(arg=''question_text'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
      kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Process
      questions related to a file, function, class, or method.\n        Args:\n            question_type
      (str): The type of question to be processed.\n            question_id (str):
      The ID of the question to be processed.\n            question_text (str): The
      text of the question to be processed.\n        Returns:\n            None\n        '')),
      If(test=Compare(left=Name(id=''question_type'', ctx=Load()), ops=[Eq()], comparators=[Constant(value=''file'')]),
      body=[Assign(targets=[Name(id=''query'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''question_text'',
      ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''filename'',
      value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'', ctx=Load()))])),
      Assign(targets=[Name(id=''info'', ctx=Store())], value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
      ctx=Load())), Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
      ctx=Load()), slice=Constant(value=''file_code''), ctx=Load())), Expr(value=Call(func=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'',
      ctx=Load()), Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()),
      Name(id=''context'', ctx=Load()), Name(id=''info'', ctx=Load())], keywords=[]))],
      orelse=[If(test=Compare(left=Name(id=''question_type'', ctx=Load()), ops=[Eq()],
      comparators=[Constant(value=''method'')]), body=[For(target=Tuple(elts=[Name(id=''class_name'',
      ctx=Store()), Name(id=''class_info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''classes''),
      ctx=Load()), attr=''items'', ctx=Load()), args=[], keywords=[]), body=[For(target=Tuple(elts=[Name(id=''key'',
      ctx=Store()), Name(id=''method_info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Name(id=''class_info'',
      ctx=Load()), attr=''items'', ctx=Load()), args=[], keywords=[]), body=[If(test=Call(func=Attribute(value=Name(id=''key'',
      ctx=Load()), attr=''startswith'', ctx=Load()), args=[Constant(value=''class_method_'')],
      keywords=[]), body=[Assign(targets=[Name(id=''method_name'', ctx=Store())],
      value=Subscript(value=Name(id=''key'', ctx=Load()), slice=Slice(lower=Call(func=Name(id=''len'',
      ctx=Load()), args=[Constant(value=''class_method_'')], keywords=[])), ctx=Load())),
      Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Name(id=''method_info'',
      ctx=Load()), slice=Constant(value=''method_code''), ctx=Load())), Assign(targets=[Name(id=''mapping'',
      ctx=Store())], value=Dict(keys=[Constant(value=''class_name''), Constant(value=''method_name'')],
      values=[Name(id=''class_name'', ctx=Load()), Name(id=''method_name'', ctx=Load())])),
      Assign(targets=[Name(id=''query'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''question_text'',
      ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''filename'',
      value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'', ctx=Load())),
      keyword(value=Name(id=''mapping'', ctx=Load()))])), Expr(value=Call(func=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'',
      ctx=Load()), Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()),
      Name(id=''context'', ctx=Load()), Name(id=''method_info'', ctx=Load())], keywords=[]))],
      orelse=[])], orelse=[])], orelse=[])], orelse=[For(target=Tuple(elts=[Name(id=''name'',
      ctx=Store()), Name(id=''info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''file_details'', ctx=Load()), slice=Subscript(value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''question_mapping'', ctx=Load()), slice=Name(id=''question_type'',
      ctx=Load()), ctx=Load()), ctx=Load()), attr=''items'', ctx=Load()), args=[],
      keywords=[]), body=[Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Name(id=''info'',
      ctx=Load()), slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
      ctx=Load()), conversion=-1), Constant(value=''_code'')]), ctx=Load())), Assign(targets=[Name(id=''mapping'',
      ctx=Store())], value=Dict(keys=[JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
      ctx=Load()), conversion=-1), Constant(value=''_name'')])], values=[Name(id=''name'',
      ctx=Load())])), If(test=BoolOp(op=And(), values=[Compare(left=Name(id=''question_id'',
      ctx=Load()), ops=[Eq()], comparators=[JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
      ctx=Load()), conversion=-1), Constant(value=''_purpose'')])]), Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''use_llm'', ctx=Load())]), body=[Assign(targets=[Name(id=''variables_string'',
      ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
      attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
      JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
      conversion=-1), Constant(value=''_variables'')])], keywords=[])), Assign(targets=[Name(id=''inputs_string'',
      ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
      attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
      JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
      conversion=-1), Constant(value=''_inputs'')])], keywords=[])), Assign(targets=[Name(id=''combined_string'',
      ctx=Store())], value=Call(func=Attribute(value=Constant(value='', ''), attr=''join'',
      ctx=Load()), args=[ListComp(elt=Name(id=''s'', ctx=Load()), generators=[comprehension(target=Name(id=''s'',
      ctx=Store()), iter=List(elts=[Name(id=''variables_string'', ctx=Load()), Name(id=''inputs_string'',
      ctx=Load())], ctx=Load()), ifs=[Name(id=''s'', ctx=Load())], is_async=0)])],
      keywords=[])), Assign(targets=[Subscript(value=Name(id=''mapping'', ctx=Load()),
      slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
      conversion=-1), Constant(value=''_variables'')]), ctx=Store())], value=Call(func=Name(id=''clean_and_get_unique_elements'',
      ctx=Load()), args=[Name(id=''combined_string'', ctx=Load())], keywords=[])),
      If(test=Compare(left=Name(id=''question_type'', ctx=Load()), ops=[Eq()], comparators=[Constant(value=''class'')]),
      body=[Assign(targets=[Name(id=''methods_string'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'',
      ctx=Load()), JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
      ctx=Load()), conversion=-1), Constant(value=''_methods'')])], keywords=[])),
      Assign(targets=[Subscript(value=Name(id=''mapping'', ctx=Load()), slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
      ctx=Load()), conversion=-1), Constant(value=''_methods'')]), ctx=Store())],
      value=Name(id=''methods_string'', ctx=Load()))], orelse=[])], orelse=[]), Assign(targets=[Name(id=''query'',
      ctx=Store())], value=Call(func=Attribute(value=Name(id=''question_text'', ctx=Load()),
      attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''filename'', value=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''base_name'', ctx=Load())), keyword(value=Name(id=''mapping'',
      ctx=Load()))])), Expr(value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
      attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'', ctx=Load()),
      Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()), Name(id=''context'',
      ctx=Load()), Name(id=''info'', ctx=Load())], keywords=[]))], orelse=[])])])],
      decorator_list=[], returns=Constant(value=None)), FunctionDef(name=''generate'',
      args=arguments(posonlyargs=[], args=[arg(arg=''self'')], kwonlyargs=[], kw_defaults=[],
      defaults=[]), body=[Expr(value=Constant(value=''\n        Generate responses
      for all the questions and returns the instruct_list.\n        Args:\n            None\n        Returns:\n            Tuple[List[Dict],
      List[Dict]]: The generated question-answer pairs and instructions.\n        '')),
      For(target=Name(id=''question'', ctx=Store()), iter=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''questions'', ctx=Load()), body=[Expr(value=Call(func=Attribute(value=Name(id=''self'',
      ctx=Load()), attr=''process_question_type'', ctx=Load()), args=[Subscript(value=Name(id=''question'',
      ctx=Load()), slice=Constant(value=''type''), ctx=Load()), Subscript(value=Name(id=''question'',
      ctx=Load()), slice=Constant(value=''id''), ctx=Load()), Subscript(value=Name(id=''question'',
      ctx=Load()), slice=Constant(value=''text''), ctx=Load())], keywords=[]))], orelse=[]),
      Return(value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''instruct_list'',
      ctx=Load()))], decorator_list=[], returns=Subscript(value=Name(id=''Tuple'',
      ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id=''List'', ctx=Load()),
      slice=Name(id=''Dict'', ctx=Load()), ctx=Load()), Subscript(value=Name(id=''List'',
      ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load())], ctx=Load()),
      ctx=Load()))], decorator_list=[])'
    class_docstring: "\n    Generate JSON formatted dictionary outputs for a Python\
      \ file.\n    Attributes:\n        file_path (str): The path to the Python file.\n\
      \        file_details (Dict[str, Any]): Details of the Python file.\n      \
      \  base_name (str): The base name of the Python file.\n        questions (List[Dict[str,\
      \ str]]): Questions for generating responses.\n        instruct_list (List[Dict[str,\
      \ str]]): Storage for generated instructions.\n        question_mapping (Dict[str,\
      \ str]): Mapping of question types to keys in file details.\n        use_llm\
      \ (bool): Flag indicating if a language model should be used.\n        llm (object):\
      \ The language model for generating responses.\n        prompt (str): The prompt\
      \ format for querying the language model.\n    Methods:\n        add_to_list(list_to_update:\
      \ List[Dict], query: str, response: str, \n        additional_field=None) ->\
      \ List[Dict]: \n            Add response to the instruct list.\n        get_response_from_llm(query:\
      \ str, context: str) -> str:\n            Get language model response to query\
      \ for given context.\n        process_question(question_type: str, question_id:\
      \ str, query: str, \n        context: str, info: Dict) -> None:\n          \
      \  Process question and add generated response to the instruct_list.\n     \
      \   process_question_type(question_type: str, question_id: str, \n        question_text:\
      \ str) -> None:\n            Process question related to file, function, class,\
      \ or method.\n        generate() -> Tuple[List[Dict], List[Dict]]:\n       \
      \     Generate responses for all the questions and return the instruct_list.\n\
      \    "
    class_inputs: null
    class_defaults: null
    class_returns:
    - list_to_update
    - response
    - ''''''
    - self.instruct_list
    - (prompt, context_size)
    - ''''''
    - ''', ''.join(items)'
    class_calls:
    - list_to_update.append
    - self.model_config['prompt_template'].format
    - len
    - self.llm.tokenize
    - '''\n''.join'
    - any
    - item['instruction'].startswith
    - str
    - self.get_string_from_info
    - strategy
    - get_context_and_prompt
    - logging.error
    - math.ceil
    - re.sub
    - self.llm
    - logging.info
    - question_id.endswith
    - info.get
    - self.get_response_from_llm
    - clean_and_get_unique_elements
    - str(response).strip
    - self.instruct_list.append
    - item.strip
    - str(info[item_type]).split
    - ''', ''.join'
    - question_text.format
    - self.process_question
    - self.file_details['classes'].items
    - class_info.items
    - key.startswith
    - self.file_details[self.question_mapping[question_type]].items
    - self.process_question_type
    class_call_inputs:
      list_to_update.append:
      - '{''instruction'': query, ''input'': additional_field, ''output'': response}'
      self.model_config['prompt_template'].format: []
      len:
      - '''class_method_'''
      self.llm.tokenize:
      - prompt
      '''\n''.join':
      - '[f"Q: {item[''instruction'']} \nA: {item[''output'']}" for item in self.instruct_list
        if not any((item[''instruction''].startswith(prefix) for prefix in excluded_instructions))]'
      any:
      - (item['instruction'].startswith(prefix) for prefix in excluded_instructions)
      item['instruction'].startswith:
      - prefix
      str:
      - info[item_type]
      self.get_string_from_info:
      - info
      - f'{question_type}_methods'
      strategy: []
      get_context_and_prompt:
      - query
      - context
      - code_qa
      logging.error:
      - 'f''Failed to generate model response: {error}'''
      math.ceil:
      - context_size / 0.7
      re.sub:
      - '''\\n\\s*\\n'''
      - '''\n\n'''
      - self.llm(prompt)
      self.llm:
      - prompt
      logging.info:
      - 'f''Response: {response}'''
      question_id.endswith:
      - '''purpose'''
      info.get:
      - question_id
      - ''''''
      self.get_response_from_llm:
      - query
      - context
      clean_and_get_unique_elements:
      - combined_string
      str(response).strip: []
      self.instruct_list.append:
      - '{''instruction'': query, ''input'': context, ''output'': response_str}'
      item.strip: []
      str(info[item_type]).split:
      - ''','''
      ''', ''.join':
      - '[s for s in [variables_string, inputs_string] if s]'
      question_text.format: []
      self.process_question:
      - question_type
      - question_id
      - query
      - context
      - info
      self.file_details['classes'].items: []
      class_info.items: []
      key.startswith:
      - '''class_method_'''
      self.file_details[self.question_mapping[question_type]].items: []
      self.process_question_type:
      - question['type']
      - question['id']
      - question['text']
    class_variables:
    - items
    - full_context
    - methods_string
    - mapping
    - query
    - variables_string
    - context_strategies
    - response
    - info
    - context
    - method_name
    - context_size
    - combined_string
    - excluded_instructions
    - prompt
    - code_qa
    - inputs_string
    - max_context_length
    - response_str
    class_decorators: []
    class_annotations: []
    class_properties:
    - self.questions
    - self.use_llm
    - self.llm
    - self.file_details
    - self.instruct_list
    - self.file_path
    - self.model_config
    - self.base_name
    - self.question_mapping
    class_attributes:
    - file_path
    - file_details
    - base_name
    - questions
    - model_config
    - llm
    - instruct_list
    - question_mapping
    - use_llm
    - use_llm
    class_methods:
    - add_to_list
    - get_response_from_llm
    - process_question
    - get_string_from_info
    - process_question_type
    - generate
    class_inheritance: []
    class_static_methods:
    - get_string_from_info
    class_method___init__:
      method_name: __init__
      method_code: "def __init__(self, file_path: str, file_details: Dict, base_name:\
        \ str, questions: List[Dict], model_config: Dict) -> None:\n    \"\"\"\n \
        \       Initialize the DatasetGenerator class.\n        Args:\n          \
        \  file_path (str): The path to the Python file.\n            file_details\
        \ (Dict[str, Any]): Details of the Python file.\n            base_name (str):\
        \ The base name of the Python file.\n            questions (List[Dict[str,\
        \ str]]): Questions for generating responses.\n            model_config (Dict):\
        \ Configuration for the language model.\n        Returns:\n            None\n\
        \        \"\"\"\n    self.file_path = file_path\n    self.file_details = file_details\n\
        \    self.base_name = base_name\n    self.questions = questions\n    self.model_config\
        \ = model_config\n    self.llm = model_config['model']\n    if self.llm is\
        \ None:\n        self.use_llm = False\n    else:\n        self.use_llm = True\n\
        \    self.instruct_list = []\n    self.question_mapping = {'file': 'file',\
        \ 'function': 'functions', 'class': 'classes', 'method': 'classes'}"
      method_ast: 'FunctionDef(name=''__init__'', args=arguments(posonlyargs=[], args=[arg(arg=''self''),
        arg(arg=''file_path'', annotation=Name(id=''str'', ctx=Load())), arg(arg=''file_details'',
        annotation=Name(id=''Dict'', ctx=Load())), arg(arg=''base_name'', annotation=Name(id=''str'',
        ctx=Load())), arg(arg=''questions'', annotation=Subscript(value=Name(id=''List'',
        ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load())), arg(arg=''model_config'',
        annotation=Name(id=''Dict'', ctx=Load()))], kwonlyargs=[], kw_defaults=[],
        defaults=[]), body=[Expr(value=Constant(value=''\n        Initialize the DatasetGenerator
        class.\n        Args:\n            file_path (str): The path to the Python
        file.\n            file_details (Dict[str, Any]): Details of the Python file.\n            base_name
        (str): The base name of the Python file.\n            questions (List[Dict[str,
        str]]): Questions for generating responses.\n            model_config (Dict):
        Configuration for the language model.\n        Returns:\n            None\n        '')),
        Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''file_path'',
        ctx=Store())], value=Name(id=''file_path'', ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Store())], value=Name(id=''file_details'',
        ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()),
        attr=''base_name'', ctx=Store())], value=Name(id=''base_name'', ctx=Load())),
        Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''questions'',
        ctx=Store())], value=Name(id=''questions'', ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''model_config'', ctx=Store())], value=Name(id=''model_config'',
        ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()),
        attr=''llm'', ctx=Store())], value=Subscript(value=Name(id=''model_config'',
        ctx=Load()), slice=Constant(value=''model''), ctx=Load())), If(test=Compare(left=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''llm'', ctx=Load()), ops=[Is()], comparators=[Constant(value=None)]),
        body=[Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''use_llm'',
        ctx=Store())], value=Constant(value=False))], orelse=[Assign(targets=[Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''use_llm'', ctx=Store())], value=Constant(value=True))]),
        Assign(targets=[Attribute(value=Name(id=''self'', ctx=Load()), attr=''instruct_list'',
        ctx=Store())], value=List(elts=[], ctx=Load())), Assign(targets=[Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''question_mapping'', ctx=Store())], value=Dict(keys=[Constant(value=''file''),
        Constant(value=''function''), Constant(value=''class''), Constant(value=''method'')],
        values=[Constant(value=''file''), Constant(value=''functions''), Constant(value=''classes''),
        Constant(value=''classes'')]))], decorator_list=[], returns=Constant(value=None))'
      method_docstring: "\n        Initialize the DatasetGenerator class.\n      \
        \  Args:\n            file_path (str): The path to the Python file.\n    \
        \        file_details (Dict[str, Any]): Details of the Python file.\n    \
        \        base_name (str): The base name of the Python file.\n            questions\
        \ (List[Dict[str, str]]): Questions for generating responses.\n          \
        \  model_config (Dict): Configuration for the language model.\n        Returns:\n\
        \            None\n        "
      method_inputs:
      - self
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      method_defaults: []
      method_returns: []
      method_calls: []
      method_call_inputs: {}
      method_variables: []
      method_decorators: []
      method_annotations: []
      method_properties:
      - self.questions
      - self.use_llm
      - self.llm
      - self.file_details
      - self.instruct_list
      - self.file_path
      - self.model_config
      - self.base_name
      - self.question_mapping
    class_method_add_to_list:
      method_name: add_to_list
      method_code: "def add_to_list(self, list_to_update: List[Dict], query: str,\
        \ response: str, additional_field=None) -> List[Dict]:\n    \"\"\"\n     \
        \   Add response to the instruct list.\n        Args:\n            list_to_update\
        \ (List[Dict]): The list to update.\n            query (str): The query to\
        \ be added.\n            response (str): The response to be added.\n     \
        \       additional_field (Any): Additional field to be added.\n        Returns:\n\
        \            List[Dict]: The updated list.\n        \"\"\"\n    list_to_update.append({'instruction':\
        \ query, 'input': additional_field, 'output': response})\n    return list_to_update"
      method_ast: 'FunctionDef(name=''add_to_list'', args=arguments(posonlyargs=[],
        args=[arg(arg=''self''), arg(arg=''list_to_update'', annotation=Subscript(value=Name(id=''List'',
        ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load())), arg(arg=''query'',
        annotation=Name(id=''str'', ctx=Load())), arg(arg=''response'', annotation=Name(id=''str'',
        ctx=Load())), arg(arg=''additional_field'')], kwonlyargs=[], kw_defaults=[],
        defaults=[Constant(value=None)]), body=[Expr(value=Constant(value=''\n        Add
        response to the instruct list.\n        Args:\n            list_to_update
        (List[Dict]): The list to update.\n            query (str): The query to be
        added.\n            response (str): The response to be added.\n            additional_field
        (Any): Additional field to be added.\n        Returns:\n            List[Dict]:
        The updated list.\n        '')), Expr(value=Call(func=Attribute(value=Name(id=''list_to_update'',
        ctx=Load()), attr=''append'', ctx=Load()), args=[Dict(keys=[Constant(value=''instruction''),
        Constant(value=''input''), Constant(value=''output'')], values=[Name(id=''query'',
        ctx=Load()), Name(id=''additional_field'', ctx=Load()), Name(id=''response'',
        ctx=Load())])], keywords=[])), Return(value=Name(id=''list_to_update'', ctx=Load()))],
        decorator_list=[], returns=Subscript(value=Name(id=''List'', ctx=Load()),
        slice=Name(id=''Dict'', ctx=Load()), ctx=Load()))'
      method_docstring: "\n        Add response to the instruct list.\n        Args:\n\
        \            list_to_update (List[Dict]): The list to update.\n          \
        \  query (str): The query to be added.\n            response (str): The response\
        \ to be added.\n            additional_field (Any): Additional field to be\
        \ added.\n        Returns:\n            List[Dict]: The updated list.\n  \
        \      "
      method_inputs:
      - self
      - list_to_update
      - query
      - response
      - additional_field
      method_defaults:
      - None
      method_returns:
      - list_to_update
      method_calls:
      - list_to_update.append
      method_call_inputs:
        list_to_update.append:
        - '{''instruction'': query, ''input'': additional_field, ''output'': response}'
      method_variables: []
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_get_response_from_llm:
      method_name: get_response_from_llm
      method_code: "def get_response_from_llm(self, query: str, context: str) -> str:\n\
        \    \"\"\"\n        Get language model response to query for given context.\n\
        \        Args:\n            query (str): The query to be used for generating\
        \ the response.\n            context (str): The context to be used for generating\
        \ the response.\n        Returns:\n            str: The generated response.\n\
        \        \"\"\"\n\n    def get_context_and_prompt(query, context, code_qa):\n\
        \        full_context = f'{context}\\nCODE Q and A:\\n{code_qa}'\n       \
        \ prompt = self.model_config['prompt_template'].format(context=full_context,\
        \ query=query)\n        context_size = len(self.llm.tokenize(prompt))\n  \
        \      return (prompt, context_size)\n    max_context_length = self.model_config['inference_model']['model_params']['context_length']\n\
        \    excluded_instructions = ['Call code graph', 'Docstring']\n    code_qa\
        \ = '\\n'.join([f\"Q: {item['instruction']} \\nA: {item['output']}\" for item\
        \ in self.instruct_list if not any((item['instruction'].startswith(prefix)\
        \ for prefix in excluded_instructions))])\n    context_strategies = [lambda:\
        \ '```python\\n' + str(context) + '\\n```', lambda: '```python\\n' + str(self.file_details['file_info']['file_code_simplified'])\
        \ + '\\n```', lambda: self.get_string_from_info(self.file_details['file_info'],\
        \ 'file_summary'), lambda: '']\n    for strategy in context_strategies:\n\
        \        context = strategy()\n        prompt, context_size = get_context_and_prompt(query,\
        \ context, code_qa)\n        if context_size <= 0.7 * max_context_length:\n\
        \            break\n    else:\n        logging.error(f'Model response failed,\
        \ increase py2dataset_model_config.yaml context_length > {math.ceil(context_size\
        \ / 0.7)}')\n        return ''\n    try:\n        response = re.sub('\\\\\
        n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n        logging.info(f'Response:\
        \ {response}')\n    except Exception as error:\n        logging.error(f'Failed\
        \ to generate model response: {error}')\n        response = ''\n    return\
        \ response"
      method_ast: 'FunctionDef(name=''get_response_from_llm'', args=arguments(posonlyargs=[],
        args=[arg(arg=''self''), arg(arg=''query'', annotation=Name(id=''str'', ctx=Load())),
        arg(arg=''context'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
        kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Get
        language model response to query for given context.\n        Args:\n            query
        (str): The query to be used for generating the response.\n            context
        (str): The context to be used for generating the response.\n        Returns:\n            str:
        The generated response.\n        '')), FunctionDef(name=''get_context_and_prompt'',
        args=arguments(posonlyargs=[], args=[arg(arg=''query''), arg(arg=''context''),
        arg(arg=''code_qa'')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id=''full_context'',
        ctx=Store())], value=JoinedStr(values=[FormattedValue(value=Name(id=''context'',
        ctx=Load()), conversion=-1), Constant(value=''\nCODE Q and A:\n''), FormattedValue(value=Name(id=''code_qa'',
        ctx=Load()), conversion=-1)])), Assign(targets=[Name(id=''prompt'', ctx=Store())],
        value=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''model_config'', ctx=Load()), slice=Constant(value=''prompt_template''),
        ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''context'',
        value=Name(id=''full_context'', ctx=Load())), keyword(arg=''query'', value=Name(id=''query'',
        ctx=Load()))])), Assign(targets=[Name(id=''context_size'', ctx=Store())],
        value=Call(func=Name(id=''len'', ctx=Load()), args=[Call(func=Attribute(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''llm'', ctx=Load()), attr=''tokenize'', ctx=Load()), args=[Name(id=''prompt'',
        ctx=Load())], keywords=[])], keywords=[])), Return(value=Tuple(elts=[Name(id=''prompt'',
        ctx=Load()), Name(id=''context_size'', ctx=Load())], ctx=Load()))], decorator_list=[]),
        Assign(targets=[Name(id=''max_context_length'', ctx=Store())], value=Subscript(value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''model_config'', ctx=Load()), slice=Constant(value=''inference_model''),
        ctx=Load()), slice=Constant(value=''model_params''), ctx=Load()), slice=Constant(value=''context_length''),
        ctx=Load())), Assign(targets=[Name(id=''excluded_instructions'', ctx=Store())],
        value=List(elts=[Constant(value=''Call code graph''), Constant(value=''Docstring'')],
        ctx=Load())), Assign(targets=[Name(id=''code_qa'', ctx=Store())], value=Call(func=Attribute(value=Constant(value=''\n''),
        attr=''join'', ctx=Load()), args=[ListComp(elt=JoinedStr(values=[Constant(value=''Q:
        ''), FormattedValue(value=Subscript(value=Name(id=''item'', ctx=Load()), slice=Constant(value=''instruction''),
        ctx=Load()), conversion=-1), Constant(value='' \nA: ''), FormattedValue(value=Subscript(value=Name(id=''item'',
        ctx=Load()), slice=Constant(value=''output''), ctx=Load()), conversion=-1)]),
        generators=[comprehension(target=Name(id=''item'', ctx=Store()), iter=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''instruct_list'', ctx=Load()), ifs=[UnaryOp(op=Not(), operand=Call(func=Name(id=''any'',
        ctx=Load()), args=[GeneratorExp(elt=Call(func=Attribute(value=Subscript(value=Name(id=''item'',
        ctx=Load()), slice=Constant(value=''instruction''), ctx=Load()), attr=''startswith'',
        ctx=Load()), args=[Name(id=''prefix'', ctx=Load())], keywords=[]), generators=[comprehension(target=Name(id=''prefix'',
        ctx=Store()), iter=Name(id=''excluded_instructions'', ctx=Load()), ifs=[],
        is_async=0)])], keywords=[]))], is_async=0)])], keywords=[])), Assign(targets=[Name(id=''context_strategies'',
        ctx=Store())], value=List(elts=[Lambda(args=arguments(posonlyargs=[], args=[],
        kwonlyargs=[], kw_defaults=[], defaults=[]), body=BinOp(left=BinOp(left=Constant(value=''```python\n''),
        op=Add(), right=Call(func=Name(id=''str'', ctx=Load()), args=[Name(id=''context'',
        ctx=Load())], keywords=[])), op=Add(), right=Constant(value=''\n```''))),
        Lambda(args=arguments(posonlyargs=[], args=[], kwonlyargs=[], kw_defaults=[],
        defaults=[]), body=BinOp(left=BinOp(left=Constant(value=''```python\n''),
        op=Add(), right=Call(func=Name(id=''str'', ctx=Load()), args=[Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
        ctx=Load()), slice=Constant(value=''file_code_simplified''), ctx=Load())],
        keywords=[])), op=Add(), right=Constant(value=''\n```''))), Lambda(args=arguments(posonlyargs=[],
        args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=Call(func=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''get_string_from_info'', ctx=Load()), args=[Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
        ctx=Load()), Constant(value=''file_summary'')], keywords=[])), Lambda(args=arguments(posonlyargs=[],
        args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=Constant(value=''''))],
        ctx=Load())), For(target=Name(id=''strategy'', ctx=Store()), iter=Name(id=''context_strategies'',
        ctx=Load()), body=[Assign(targets=[Name(id=''context'', ctx=Store())], value=Call(func=Name(id=''strategy'',
        ctx=Load()), args=[], keywords=[])), Assign(targets=[Tuple(elts=[Name(id=''prompt'',
        ctx=Store()), Name(id=''context_size'', ctx=Store())], ctx=Store())], value=Call(func=Name(id=''get_context_and_prompt'',
        ctx=Load()), args=[Name(id=''query'', ctx=Load()), Name(id=''context'', ctx=Load()),
        Name(id=''code_qa'', ctx=Load())], keywords=[])), If(test=Compare(left=Name(id=''context_size'',
        ctx=Load()), ops=[LtE()], comparators=[BinOp(left=Constant(value=0.7), op=Mult(),
        right=Name(id=''max_context_length'', ctx=Load()))]), body=[Break()], orelse=[])],
        orelse=[Expr(value=Call(func=Attribute(value=Name(id=''logging'', ctx=Load()),
        attr=''error'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Model
        response failed, increase py2dataset_model_config.yaml context_length > ''),
        FormattedValue(value=Call(func=Attribute(value=Name(id=''math'', ctx=Load()),
        attr=''ceil'', ctx=Load()), args=[BinOp(left=Name(id=''context_size'', ctx=Load()),
        op=Div(), right=Constant(value=0.7))], keywords=[]), conversion=-1)])], keywords=[])),
        Return(value=Constant(value=''''))]), Try(body=[Assign(targets=[Name(id=''response'',
        ctx=Store())], value=Call(func=Attribute(value=Name(id=''re'', ctx=Load()),
        attr=''sub'', ctx=Load()), args=[Constant(value=''\\n\\s*\\n''), Constant(value=''\n\n''),
        Call(func=Attribute(value=Name(id=''self'', ctx=Load()), attr=''llm'', ctx=Load()),
        args=[Name(id=''prompt'', ctx=Load())], keywords=[])], keywords=[])), Expr(value=Call(func=Attribute(value=Name(id=''logging'',
        ctx=Load()), attr=''info'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Response:
        ''), FormattedValue(value=Name(id=''response'', ctx=Load()), conversion=-1)])],
        keywords=[]))], handlers=[ExceptHandler(type=Name(id=''Exception'', ctx=Load()),
        name=''error'', body=[Expr(value=Call(func=Attribute(value=Name(id=''logging'',
        ctx=Load()), attr=''error'', ctx=Load()), args=[JoinedStr(values=[Constant(value=''Failed
        to generate model response: ''), FormattedValue(value=Name(id=''error'', ctx=Load()),
        conversion=-1)])], keywords=[])), Assign(targets=[Name(id=''response'', ctx=Store())],
        value=Constant(value=''''))])], orelse=[], finalbody=[]), Return(value=Name(id=''response'',
        ctx=Load()))], decorator_list=[], returns=Name(id=''str'', ctx=Load()))'
      method_docstring: "\n        Get language model response to query for given\
        \ context.\n        Args:\n            query (str): The query to be used for\
        \ generating the response.\n            context (str): The context to be used\
        \ for generating the response.\n        Returns:\n            str: The generated\
        \ response.\n        "
      method_inputs:
      - self
      - query
      - context
      method_defaults: []
      method_returns:
      - response
      - (prompt, context_size)
      - ''''''
      method_calls:
      - self.model_config['prompt_template'].format
      - len
      - self.llm.tokenize
      - '''\n''.join'
      - any
      - item['instruction'].startswith
      - str
      - self.get_string_from_info
      - strategy
      - get_context_and_prompt
      - logging.error
      - math.ceil
      - re.sub
      - self.llm
      - logging.info
      method_call_inputs:
        self.model_config['prompt_template'].format: []
        len:
        - self.llm.tokenize(prompt)
        self.llm.tokenize:
        - prompt
        '''\n''.join':
        - '[f"Q: {item[''instruction'']} \nA: {item[''output'']}" for item in self.instruct_list
          if not any((item[''instruction''].startswith(prefix) for prefix in excluded_instructions))]'
        any:
        - (item['instruction'].startswith(prefix) for prefix in excluded_instructions)
        item['instruction'].startswith:
        - prefix
        str:
        - self.file_details['file_info']['file_code_simplified']
        self.get_string_from_info:
        - self.file_details['file_info']
        - '''file_summary'''
        strategy: []
        get_context_and_prompt:
        - query
        - context
        - code_qa
        logging.error:
        - 'f''Failed to generate model response: {error}'''
        math.ceil:
        - context_size / 0.7
        re.sub:
        - '''\\n\\s*\\n'''
        - '''\n\n'''
        - self.llm(prompt)
        self.llm:
        - prompt
        logging.info:
        - 'f''Response: {response}'''
      method_variables:
      - context_size
      - response
      - excluded_instructions
      - full_context
      - context
      - max_context_length
      - prompt
      - code_qa
      - context_strategies
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_get_context_and_prompt:
      method_name: get_context_and_prompt
      method_code: "def get_context_and_prompt(query, context, code_qa):\n    full_context\
        \ = f'{context}\\nCODE Q and A:\\n{code_qa}'\n    prompt = self.model_config['prompt_template'].format(context=full_context,\
        \ query=query)\n    context_size = len(self.llm.tokenize(prompt))\n    return\
        \ (prompt, context_size)"
      method_ast: FunctionDef(name='get_context_and_prompt', args=arguments(posonlyargs=[],
        args=[arg(arg='query'), arg(arg='context'), arg(arg='code_qa')], kwonlyargs=[],
        kw_defaults=[], defaults=[]), body=[Assign(targets=[Name(id='full_context',
        ctx=Store())], value=JoinedStr(values=[FormattedValue(value=Name(id='context',
        ctx=Load()), conversion=-1), Constant(value='\nCODE Q and A:\n'), FormattedValue(value=Name(id='code_qa',
        ctx=Load()), conversion=-1)])), Assign(targets=[Name(id='prompt', ctx=Store())],
        value=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id='self',
        ctx=Load()), attr='model_config', ctx=Load()), slice=Constant(value='prompt_template'),
        ctx=Load()), attr='format', ctx=Load()), args=[], keywords=[keyword(arg='context',
        value=Name(id='full_context', ctx=Load())), keyword(arg='query', value=Name(id='query',
        ctx=Load()))])), Assign(targets=[Name(id='context_size', ctx=Store())], value=Call(func=Name(id='len',
        ctx=Load()), args=[Call(func=Attribute(value=Attribute(value=Name(id='self',
        ctx=Load()), attr='llm', ctx=Load()), attr='tokenize', ctx=Load()), args=[Name(id='prompt',
        ctx=Load())], keywords=[])], keywords=[])), Return(value=Tuple(elts=[Name(id='prompt',
        ctx=Load()), Name(id='context_size', ctx=Load())], ctx=Load()))], decorator_list=[])
      method_docstring: null
      method_inputs:
      - query
      - context
      - code_qa
      method_defaults: []
      method_returns:
      - (prompt, context_size)
      method_calls:
      - self.model_config['prompt_template'].format
      - len
      - self.llm.tokenize
      method_call_inputs:
        self.model_config['prompt_template'].format: []
        len:
        - self.llm.tokenize(prompt)
        self.llm.tokenize:
        - prompt
      method_variables:
      - context_size
      - prompt
      - full_context
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_process_question:
      method_name: process_question
      method_code: "def process_question(self, question_type: str, question_id: str,\
        \ query: str, context: str, info: Dict) -> None:\n    \"\"\"\n        Process\
        \ question and add the generated response to the instruct_list.\n        Args:\n\
        \            question_type (str): The type of question to be processed.\n\
        \            question_id (str): The ID of the question to be processed.\n\
        \            query (str): The query to be processed.\n            context\
        \ (str): The context to be used for generating the response.\n           \
        \ info (Dict): The information of the Python file.\n        Returns:\n   \
        \         None\n        \"\"\"\n    if question_id.endswith('code_graph')\
        \ or question_id.endswith('docstring'):\n        response = info.get(question_id,\
        \ {})\n    else:\n        response = self.get_response_from_llm(query, context)\
        \ if self.use_llm and question_id.endswith('purpose') else clean_and_get_unique_elements(str(info.get(question_id,\
        \ '')))\n    if question_type == 'file':\n        context = self.file_details['file_info']['file_code']\n\
        \    if response and response != 'None':\n        response_str = str(response).strip()\n\
        \        if response_str:\n            self.instruct_list.append({'instruction':\
        \ query, 'input': context, 'output': response_str})"
      method_ast: 'FunctionDef(name=''process_question'', args=arguments(posonlyargs=[],
        args=[arg(arg=''self''), arg(arg=''question_type'', annotation=Name(id=''str'',
        ctx=Load())), arg(arg=''question_id'', annotation=Name(id=''str'', ctx=Load())),
        arg(arg=''query'', annotation=Name(id=''str'', ctx=Load())), arg(arg=''context'',
        annotation=Name(id=''str'', ctx=Load())), arg(arg=''info'', annotation=Name(id=''Dict'',
        ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Process
        question and add the generated response to the instruct_list.\n        Args:\n            question_type
        (str): The type of question to be processed.\n            question_id (str):
        The ID of the question to be processed.\n            query (str): The query
        to be processed.\n            context (str): The context to be used for generating
        the response.\n            info (Dict): The information of the Python file.\n        Returns:\n            None\n        '')),
        If(test=BoolOp(op=Or(), values=[Call(func=Attribute(value=Name(id=''question_id'',
        ctx=Load()), attr=''endswith'', ctx=Load()), args=[Constant(value=''code_graph'')],
        keywords=[]), Call(func=Attribute(value=Name(id=''question_id'', ctx=Load()),
        attr=''endswith'', ctx=Load()), args=[Constant(value=''docstring'')], keywords=[])]),
        body=[Assign(targets=[Name(id=''response'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''info'',
        ctx=Load()), attr=''get'', ctx=Load()), args=[Name(id=''question_id'', ctx=Load()),
        Dict(keys=[], values=[])], keywords=[]))], orelse=[Assign(targets=[Name(id=''response'',
        ctx=Store())], value=IfExp(test=BoolOp(op=And(), values=[Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''use_llm'', ctx=Load()), Call(func=Attribute(value=Name(id=''question_id'',
        ctx=Load()), attr=''endswith'', ctx=Load()), args=[Constant(value=''purpose'')],
        keywords=[])]), body=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
        attr=''get_response_from_llm'', ctx=Load()), args=[Name(id=''query'', ctx=Load()),
        Name(id=''context'', ctx=Load())], keywords=[]), orelse=Call(func=Name(id=''clean_and_get_unique_elements'',
        ctx=Load()), args=[Call(func=Name(id=''str'', ctx=Load()), args=[Call(func=Attribute(value=Name(id=''info'',
        ctx=Load()), attr=''get'', ctx=Load()), args=[Name(id=''question_id'', ctx=Load()),
        Constant(value='''')], keywords=[])], keywords=[])], keywords=[])))]), If(test=Compare(left=Name(id=''question_type'',
        ctx=Load()), ops=[Eq()], comparators=[Constant(value=''file'')]), body=[Assign(targets=[Name(id=''context'',
        ctx=Store())], value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
        ctx=Load()), slice=Constant(value=''file_code''), ctx=Load()))], orelse=[]),
        If(test=BoolOp(op=And(), values=[Name(id=''response'', ctx=Load()), Compare(left=Name(id=''response'',
        ctx=Load()), ops=[NotEq()], comparators=[Constant(value=''None'')])]), body=[Assign(targets=[Name(id=''response_str'',
        ctx=Store())], value=Call(func=Attribute(value=Call(func=Name(id=''str'',
        ctx=Load()), args=[Name(id=''response'', ctx=Load())], keywords=[]), attr=''strip'',
        ctx=Load()), args=[], keywords=[])), If(test=Name(id=''response_str'', ctx=Load()),
        body=[Expr(value=Call(func=Attribute(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''instruct_list'', ctx=Load()), attr=''append'', ctx=Load()),
        args=[Dict(keys=[Constant(value=''instruction''), Constant(value=''input''),
        Constant(value=''output'')], values=[Name(id=''query'', ctx=Load()), Name(id=''context'',
        ctx=Load()), Name(id=''response_str'', ctx=Load())])], keywords=[]))], orelse=[])],
        orelse=[])], decorator_list=[], returns=Constant(value=None))'
      method_docstring: "\n        Process question and add the generated response\
        \ to the instruct_list.\n        Args:\n            question_type (str): The\
        \ type of question to be processed.\n            question_id (str): The ID\
        \ of the question to be processed.\n            query (str): The query to\
        \ be processed.\n            context (str): The context to be used for generating\
        \ the response.\n            info (Dict): The information of the Python file.\n\
        \        Returns:\n            None\n        "
      method_inputs:
      - self
      - question_type
      - question_id
      - query
      - context
      - info
      method_defaults: []
      method_returns: []
      method_calls:
      - question_id.endswith
      - info.get
      - self.get_response_from_llm
      - clean_and_get_unique_elements
      - str
      - str(response).strip
      - self.instruct_list.append
      method_call_inputs:
        question_id.endswith:
        - '''purpose'''
        info.get:
        - question_id
        - ''''''
        self.get_response_from_llm:
        - query
        - context
        clean_and_get_unique_elements:
        - str(info.get(question_id, ''))
        str:
        - response
        str(response).strip: []
        self.instruct_list.append:
        - '{''instruction'': query, ''input'': context, ''output'': response_str}'
      method_variables:
      - response_str
      - response
      - context
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_get_string_from_info:
      method_name: get_string_from_info
      method_code: "@staticmethod\ndef get_string_from_info(info, item_type):\n  \
        \  \"\"\"\n        Get string from info dictionary.\n        Args:\n     \
        \       info (Dict): The information of the Python file.\n            item_type\
        \ (str): The type of item to get the string from.\n        Returns:\n    \
        \        str: The string from the info.\n        \"\"\"\n    if info[item_type]:\n\
        \        items = [item.strip() for item in str(info[item_type]).split(',')\
        \ if item]\n        return ', '.join(items)\n    return ''"
      method_ast: 'FunctionDef(name=''get_string_from_info'', args=arguments(posonlyargs=[],
        args=[arg(arg=''info''), arg(arg=''item_type'')], kwonlyargs=[], kw_defaults=[],
        defaults=[]), body=[Expr(value=Constant(value=''\n        Get string from
        info dictionary.\n        Args:\n            info (Dict): The information
        of the Python file.\n            item_type (str): The type of item to get
        the string from.\n        Returns:\n            str: The string from the info.\n        '')),
        If(test=Subscript(value=Name(id=''info'', ctx=Load()), slice=Name(id=''item_type'',
        ctx=Load()), ctx=Load()), body=[Assign(targets=[Name(id=''items'', ctx=Store())],
        value=ListComp(elt=Call(func=Attribute(value=Name(id=''item'', ctx=Load()),
        attr=''strip'', ctx=Load()), args=[], keywords=[]), generators=[comprehension(target=Name(id=''item'',
        ctx=Store()), iter=Call(func=Attribute(value=Call(func=Name(id=''str'', ctx=Load()),
        args=[Subscript(value=Name(id=''info'', ctx=Load()), slice=Name(id=''item_type'',
        ctx=Load()), ctx=Load())], keywords=[]), attr=''split'', ctx=Load()), args=[Constant(value='','')],
        keywords=[]), ifs=[Name(id=''item'', ctx=Load())], is_async=0)])), Return(value=Call(func=Attribute(value=Constant(value='',
        ''), attr=''join'', ctx=Load()), args=[Name(id=''items'', ctx=Load())], keywords=[]))],
        orelse=[]), Return(value=Constant(value=''''))], decorator_list=[Name(id=''staticmethod'',
        ctx=Load())])'
      method_docstring: "\n        Get string from info dictionary.\n        Args:\n\
        \            info (Dict): The information of the Python file.\n          \
        \  item_type (str): The type of item to get the string from.\n        Returns:\n\
        \            str: The string from the info.\n        "
      method_inputs:
      - info
      - item_type
      method_defaults: []
      method_returns:
      - ''''''
      - ''', ''.join(items)'
      method_calls:
      - item.strip
      - str(info[item_type]).split
      - str
      - ''', ''.join'
      method_call_inputs:
        item.strip: []
        str(info[item_type]).split:
        - ''','''
        str:
        - info[item_type]
        ''', ''.join':
        - items
      method_variables:
      - items
      method_decorators:
      - staticmethod
      method_annotations: []
      method_properties: []
    class_method_process_question_type:
      method_name: process_question_type
      method_code: "def process_question_type(self, question_type: str, question_id:\
        \ str, question_text: str) -> None:\n    \"\"\"\n        Process questions\
        \ related to a file, function, class, or method.\n        Args:\n        \
        \    question_type (str): The type of question to be processed.\n        \
        \    question_id (str): The ID of the question to be processed.\n        \
        \    question_text (str): The text of the question to be processed.\n    \
        \    Returns:\n            None\n        \"\"\"\n    if question_type == 'file':\n\
        \        query = question_text.format(filename=self.base_name)\n        info\
        \ = self.file_details['file_info']\n        context = self.file_details['file_info']['file_code']\n\
        \        self.process_question(question_type, question_id, query, context,\
        \ info)\n    elif question_type == 'method':\n        for class_name, class_info\
        \ in self.file_details['classes'].items():\n            for key, method_info\
        \ in class_info.items():\n                if key.startswith('class_method_'):\n\
        \                    method_name = key[len('class_method_'):]\n          \
        \          context = method_info['method_code']\n                    mapping\
        \ = {'class_name': class_name, 'method_name': method_name}\n             \
        \       query = question_text.format(filename=self.base_name, **mapping)\n\
        \                    self.process_question(question_type, question_id, query,\
        \ context, method_info)\n    else:\n        for name, info in self.file_details[self.question_mapping[question_type]].items():\n\
        \            context = info[f'{question_type}_code']\n            mapping\
        \ = {f'{question_type}_name': name}\n            if question_id == f'{question_type}_purpose'\
        \ and self.use_llm:\n                variables_string = self.get_string_from_info(info,\
        \ f'{question_type}_variables')\n                inputs_string = self.get_string_from_info(info,\
        \ f'{question_type}_inputs')\n                combined_string = ', '.join([s\
        \ for s in [variables_string, inputs_string] if s])\n                mapping[f'{question_type}_variables']\
        \ = clean_and_get_unique_elements(combined_string)\n                if question_type\
        \ == 'class':\n                    methods_string = self.get_string_from_info(info,\
        \ f'{question_type}_methods')\n                    mapping[f'{question_type}_methods']\
        \ = methods_string\n            query = question_text.format(filename=self.base_name,\
        \ **mapping)\n            self.process_question(question_type, question_id,\
        \ query, context, info)"
      method_ast: 'FunctionDef(name=''process_question_type'', args=arguments(posonlyargs=[],
        args=[arg(arg=''self''), arg(arg=''question_type'', annotation=Name(id=''str'',
        ctx=Load())), arg(arg=''question_id'', annotation=Name(id=''str'', ctx=Load())),
        arg(arg=''question_text'', annotation=Name(id=''str'', ctx=Load()))], kwonlyargs=[],
        kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Process
        questions related to a file, function, class, or method.\n        Args:\n            question_type
        (str): The type of question to be processed.\n            question_id (str):
        The ID of the question to be processed.\n            question_text (str):
        The text of the question to be processed.\n        Returns:\n            None\n        '')),
        If(test=Compare(left=Name(id=''question_type'', ctx=Load()), ops=[Eq()], comparators=[Constant(value=''file'')]),
        body=[Assign(targets=[Name(id=''query'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''question_text'',
        ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''filename'',
        value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'', ctx=Load()))])),
        Assign(targets=[Name(id=''info'', ctx=Store())], value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
        ctx=Load())), Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''file_info''),
        ctx=Load()), slice=Constant(value=''file_code''), ctx=Load())), Expr(value=Call(func=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'',
        ctx=Load()), Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()),
        Name(id=''context'', ctx=Load()), Name(id=''info'', ctx=Load())], keywords=[]))],
        orelse=[If(test=Compare(left=Name(id=''question_type'', ctx=Load()), ops=[Eq()],
        comparators=[Constant(value=''method'')]), body=[For(target=Tuple(elts=[Name(id=''class_name'',
        ctx=Store()), Name(id=''class_info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Load()), slice=Constant(value=''classes''),
        ctx=Load()), attr=''items'', ctx=Load()), args=[], keywords=[]), body=[For(target=Tuple(elts=[Name(id=''key'',
        ctx=Store()), Name(id=''method_info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Name(id=''class_info'',
        ctx=Load()), attr=''items'', ctx=Load()), args=[], keywords=[]), body=[If(test=Call(func=Attribute(value=Name(id=''key'',
        ctx=Load()), attr=''startswith'', ctx=Load()), args=[Constant(value=''class_method_'')],
        keywords=[]), body=[Assign(targets=[Name(id=''method_name'', ctx=Store())],
        value=Subscript(value=Name(id=''key'', ctx=Load()), slice=Slice(lower=Call(func=Name(id=''len'',
        ctx=Load()), args=[Constant(value=''class_method_'')], keywords=[])), ctx=Load())),
        Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Name(id=''method_info'',
        ctx=Load()), slice=Constant(value=''method_code''), ctx=Load())), Assign(targets=[Name(id=''mapping'',
        ctx=Store())], value=Dict(keys=[Constant(value=''class_name''), Constant(value=''method_name'')],
        values=[Name(id=''class_name'', ctx=Load()), Name(id=''method_name'', ctx=Load())])),
        Assign(targets=[Name(id=''query'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''question_text'',
        ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''filename'',
        value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'', ctx=Load())),
        keyword(value=Name(id=''mapping'', ctx=Load()))])), Expr(value=Call(func=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'',
        ctx=Load()), Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()),
        Name(id=''context'', ctx=Load()), Name(id=''method_info'', ctx=Load())], keywords=[]))],
        orelse=[])], orelse=[])], orelse=[])], orelse=[For(target=Tuple(elts=[Name(id=''name'',
        ctx=Store()), Name(id=''info'', ctx=Store())], ctx=Store()), iter=Call(func=Attribute(value=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''file_details'', ctx=Load()), slice=Subscript(value=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''question_mapping'', ctx=Load()), slice=Name(id=''question_type'',
        ctx=Load()), ctx=Load()), ctx=Load()), attr=''items'', ctx=Load()), args=[],
        keywords=[]), body=[Assign(targets=[Name(id=''context'', ctx=Store())], value=Subscript(value=Name(id=''info'',
        ctx=Load()), slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
        ctx=Load()), conversion=-1), Constant(value=''_code'')]), ctx=Load())), Assign(targets=[Name(id=''mapping'',
        ctx=Store())], value=Dict(keys=[JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
        ctx=Load()), conversion=-1), Constant(value=''_name'')])], values=[Name(id=''name'',
        ctx=Load())])), If(test=BoolOp(op=And(), values=[Compare(left=Name(id=''question_id'',
        ctx=Load()), ops=[Eq()], comparators=[JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
        ctx=Load()), conversion=-1), Constant(value=''_purpose'')])]), Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''use_llm'', ctx=Load())]), body=[Assign(targets=[Name(id=''variables_string'',
        ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
        attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
        JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
        conversion=-1), Constant(value=''_variables'')])], keywords=[])), Assign(targets=[Name(id=''inputs_string'',
        ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'', ctx=Load()),
        attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'', ctx=Load()),
        JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
        conversion=-1), Constant(value=''_inputs'')])], keywords=[])), Assign(targets=[Name(id=''combined_string'',
        ctx=Store())], value=Call(func=Attribute(value=Constant(value='', ''), attr=''join'',
        ctx=Load()), args=[ListComp(elt=Name(id=''s'', ctx=Load()), generators=[comprehension(target=Name(id=''s'',
        ctx=Store()), iter=List(elts=[Name(id=''variables_string'', ctx=Load()), Name(id=''inputs_string'',
        ctx=Load())], ctx=Load()), ifs=[Name(id=''s'', ctx=Load())], is_async=0)])],
        keywords=[])), Assign(targets=[Subscript(value=Name(id=''mapping'', ctx=Load()),
        slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'', ctx=Load()),
        conversion=-1), Constant(value=''_variables'')]), ctx=Store())], value=Call(func=Name(id=''clean_and_get_unique_elements'',
        ctx=Load()), args=[Name(id=''combined_string'', ctx=Load())], keywords=[])),
        If(test=Compare(left=Name(id=''question_type'', ctx=Load()), ops=[Eq()], comparators=[Constant(value=''class'')]),
        body=[Assign(targets=[Name(id=''methods_string'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''get_string_from_info'', ctx=Load()), args=[Name(id=''info'',
        ctx=Load()), JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
        ctx=Load()), conversion=-1), Constant(value=''_methods'')])], keywords=[])),
        Assign(targets=[Subscript(value=Name(id=''mapping'', ctx=Load()), slice=JoinedStr(values=[FormattedValue(value=Name(id=''question_type'',
        ctx=Load()), conversion=-1), Constant(value=''_methods'')]), ctx=Store())],
        value=Name(id=''methods_string'', ctx=Load()))], orelse=[])], orelse=[]),
        Assign(targets=[Name(id=''query'', ctx=Store())], value=Call(func=Attribute(value=Name(id=''question_text'',
        ctx=Load()), attr=''format'', ctx=Load()), args=[], keywords=[keyword(arg=''filename'',
        value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''base_name'', ctx=Load())),
        keyword(value=Name(id=''mapping'', ctx=Load()))])), Expr(value=Call(func=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''process_question'', ctx=Load()), args=[Name(id=''question_type'',
        ctx=Load()), Name(id=''question_id'', ctx=Load()), Name(id=''query'', ctx=Load()),
        Name(id=''context'', ctx=Load()), Name(id=''info'', ctx=Load())], keywords=[]))],
        orelse=[])])])], decorator_list=[], returns=Constant(value=None))'
      method_docstring: "\n        Process questions related to a file, function,\
        \ class, or method.\n        Args:\n            question_type (str): The type\
        \ of question to be processed.\n            question_id (str): The ID of the\
        \ question to be processed.\n            question_text (str): The text of\
        \ the question to be processed.\n        Returns:\n            None\n    \
        \    "
      method_inputs:
      - self
      - question_type
      - question_id
      - question_text
      method_defaults: []
      method_returns: []
      method_calls:
      - question_text.format
      - self.process_question
      - self.file_details['classes'].items
      - class_info.items
      - key.startswith
      - len
      - self.file_details[self.question_mapping[question_type]].items
      - self.get_string_from_info
      - ''', ''.join'
      - clean_and_get_unique_elements
      method_call_inputs:
        question_text.format: []
        self.process_question:
        - question_type
        - question_id
        - query
        - context
        - info
        self.file_details['classes'].items: []
        class_info.items: []
        key.startswith:
        - '''class_method_'''
        len:
        - '''class_method_'''
        self.file_details[self.question_mapping[question_type]].items: []
        self.get_string_from_info:
        - info
        - f'{question_type}_methods'
        ''', ''.join':
        - '[s for s in [variables_string, inputs_string] if s]'
        clean_and_get_unique_elements:
        - combined_string
      method_variables:
      - info
      - combined_string
      - inputs_string
      - context
      - methods_string
      - mapping
      - query
      - variables_string
      - method_name
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_generate:
      method_name: generate
      method_code: "def generate(self) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\
        \"\n        Generate responses for all the questions and returns the instruct_list.\n\
        \        Args:\n            None\n        Returns:\n            Tuple[List[Dict],\
        \ List[Dict]]: The generated question-answer pairs and instructions.\n   \
        \     \"\"\"\n    for question in self.questions:\n        self.process_question_type(question['type'],\
        \ question['id'], question['text'])\n    return self.instruct_list"
      method_ast: 'FunctionDef(name=''generate'', args=arguments(posonlyargs=[], args=[arg(arg=''self'')],
        kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Expr(value=Constant(value=''\n        Generate
        responses for all the questions and returns the instruct_list.\n        Args:\n            None\n        Returns:\n            Tuple[List[Dict],
        List[Dict]]: The generated question-answer pairs and instructions.\n        '')),
        For(target=Name(id=''question'', ctx=Store()), iter=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''questions'', ctx=Load()), body=[Expr(value=Call(func=Attribute(value=Name(id=''self'',
        ctx=Load()), attr=''process_question_type'', ctx=Load()), args=[Subscript(value=Name(id=''question'',
        ctx=Load()), slice=Constant(value=''type''), ctx=Load()), Subscript(value=Name(id=''question'',
        ctx=Load()), slice=Constant(value=''id''), ctx=Load()), Subscript(value=Name(id=''question'',
        ctx=Load()), slice=Constant(value=''text''), ctx=Load())], keywords=[]))],
        orelse=[]), Return(value=Attribute(value=Name(id=''self'', ctx=Load()), attr=''instruct_list'',
        ctx=Load()))], decorator_list=[], returns=Subscript(value=Name(id=''Tuple'',
        ctx=Load()), slice=Tuple(elts=[Subscript(value=Name(id=''List'', ctx=Load()),
        slice=Name(id=''Dict'', ctx=Load()), ctx=Load()), Subscript(value=Name(id=''List'',
        ctx=Load()), slice=Name(id=''Dict'', ctx=Load()), ctx=Load())], ctx=Load()),
        ctx=Load()))'
      method_docstring: "\n        Generate responses for all the questions and returns\
        \ the instruct_list.\n        Args:\n            None\n        Returns:\n\
        \            Tuple[List[Dict], List[Dict]]: The generated question-answer\
        \ pairs and instructions.\n        "
      method_inputs:
      - self
      method_defaults: []
      method_returns:
      - self.instruct_list
      method_calls:
      - self.process_question_type
      method_call_inputs:
        self.process_question_type:
        - question['type']
        - question['id']
        - question['text']
      method_variables: []
      method_decorators: []
      method_annotations: []
      method_properties: []
