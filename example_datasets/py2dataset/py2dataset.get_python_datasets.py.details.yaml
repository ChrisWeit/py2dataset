file_info:
  file_code: "\"\"\"\nGenerates JSON format question-answer pairs and instructions\
    \ for a Python file\nRequirements:\n[req01] The `DatasetGenerator` class shall:\n\
    \        a. Accept Python file path, file details, base name, list of questions,\n\
    \        and model configuration as input during instantiation.\n        b. Initialize\
    \ and store Python file path, file details, base name,\n        question list,\
    \ llm, and use_llm flag as class attributes.\n        d. Provide `get_response_from_llm`\
    \ method to retrieve llm response.\n        e. Provide `process_question` method\
    \ to process each question, generate\n        corresponding responses, and add\
    \ to the instruct_list.\n        f. Provide `process_question_type` method to\
    \ process questions\n        related to the file, functions, classes, and methods.\n\
    \        g. Provide `generate` method to generate responses for all questions\n\
    \        and return the instruct_list.\n        h. Internally manage question\
    \ mapping to file details.\n[req02] The `get_python_datasets` function shall:\n\
    \        a. Accept a Python file path, file details, base name, questions list,\n\
    \        and model config as input.\n        b. Instantiate `DatasetGenerator`\
    \ class using the provided input.\n        c. Generate instruct_list using `DatasetGenerator`\
    \ `generate` method.\n        d. Return the generated `instruct_list`.\n[req03]\
    \ The `clean_and_get_unique_elements` function shall:\n        a. Clean an input\
    \ string (str) and return a string of unique elements.\n\"\"\"\nimport logging\n\
    import re\nimport math\nimport json\nfrom typing import Dict, List, Tuple\n\n\n\
    def group_json(input_json):\n    \"\"\"\n    Group JSON formatted dictionary by\
    \ key.\n    Args:\n        input_json (Dict): The input JSON formatted dictionary.\n\
    \    Returns:\n        Dict: The grouped JSON formatted dictionary.\n    \"\"\"\
    \n    output_json = {\"Code Elements\": {}}   \n    for key, value in input_json[\"\
    Code Elements\"].items():\n        if \"`\" in key:\n            new_key = f\"\
    {key.split()[-1]} {key.split()[0]}\"\n            output_json[\"Code Elements\"\
    ].setdefault(new_key, []).append(value)\n        else:\n            output_json[\"\
    Code Elements\"].setdefault(key, []).append(value)\n    output_json[\"Code Elements\"\
    ] = {k: \", \".join(v) for k, v in output_json[\"Code Elements\"].items()}\n \
    \   output_json[\"Code Elements\"] = dict(sorted(output_json[\"Code Elements\"\
    ].items()))\n    return output_json\n\n\ndef clean_and_get_unique_elements(input_str:\
    \ str) -> str:\n    \"\"\"\n    Clean an input string (str) and return a string\
    \ of unique elements.\n    Args:\n        input_str (str): The input string to\
    \ be cleaned.\n    Returns:\n        str: The cleaned string.\n    \"\"\"\n  \
    \  def element_generator(input_str):\n        start, brace_level = 0, 0\n    \
    \    for i, char in enumerate(input_str):\n            if char in \"{}\":\n  \
    \              brace_level += 1 if char == \"{\" else -1\n            elif char\
    \ == \",\" and brace_level == 0:\n                yield input_str[start:i]\n \
    \               start = i + 1\n        yield input_str[start:]\n\n    input_str\
    \ = input_str.strip(\"[]'\\\"\").strip()\n    cleaned_elements = [\n        element.strip(\"\
    '\\\" \").strip()\n        for element in element_generator(input_str)\n     \
    \   if element.strip()\n    ]\n    return \", \".join(cleaned_elements)\n\n\n\
    class DatasetGenerator:\n    \"\"\"\n    Generate JSON formatted dictionary outputs\
    \ for a Python file.\n    Attributes:\n        file_path (str): The path to the\
    \ Python file.\n        file_details (Dict[str, Any]): Details of the Python file.\n\
    \        base_name (str): The base name of the Python file.\n        questions\
    \ (List[Dict[str, str]]): Questions for generating responses.\n        instruct_list\
    \ (List[Dict[str, str]]): Storage for generated instructions.\n        question_mapping\
    \ (Dict[str, str]): Mapping of question types to keys in file details.\n     \
    \   use_llm (bool): Flag indicating if a language model should be used.\n    \
    \    llm (object): The language model for generating responses.\n        prompt\
    \ (str): The prompt format for querying the language model.\n    Methods:\n  \
    \      add_to_list(list_to_update: List[Dict], query: str, response: str,\n  \
    \      additional_field=None) -> List[Dict]:\n            Add response to the\
    \ instruct list.\n        get_response_from_llm(query: str, context: str) -> str:\n\
    \            Get language model response to query for given context.\n       \
    \ process_question(question_type: str, question_id: str, query: str,\n       \
    \ context: str, info: Dict) -> None:\n            Process question and add generated\
    \ response to the instruct_list.\n        process_question_type(question_type:\
    \ str, question_id: str,\n        question_text: str) -> None:\n            Process\
    \ question related to file, function, class, or method.\n        generate() ->\
    \ Tuple[List[Dict], List[Dict]]:\n            Generate responses for all the questions\
    \ and return the instruct_list.\n    \"\"\"\n\n    def __init__(\n        self,\n\
    \        file_path: str,\n        file_details: Dict,\n        base_name: str,\n\
    \        questions: List[Dict],\n        model_config: Dict,\n        detailed:\
    \ bool,\n    ) -> None:\n        \"\"\"\n        Initialize the DatasetGenerator\
    \ class.\n        Args:\n            file_path (str): The path to the Python file.\n\
    \            file_details (Dict[str, Any]): Details of the Python file.\n    \
    \        base_name (str): The base name of the Python file.\n            questions\
    \ (List[Dict[str, str]]): Questions for generating responses.\n            model_config\
    \ (Dict): Configuration for the language model.\n        Returns:\n          \
    \  None\n        \"\"\"\n        self.file_path = file_path\n        self.file_details\
    \ = file_details\n        self.base_name = base_name\n        self.questions =\
    \ questions\n        self.model_config = model_config\n        if model_config\
    \ is not None:\n            self.llm = model_config[\"model\"]\n            self.use_llm\
    \ = True\n            self.detailed = detailed\n        else:\n            self.llm\
    \ = None\n            self.use_llm = False\n            self.detailed = False\n\
    \        self.instruct_list = []\n        self.question_mapping = {\n        \
    \    \"file\": \"file\",\n            \"function\": \"functions\",\n         \
    \   \"class\": \"classes\",\n            \"method\": \"classes\",\n        }\n\
    \n    def add_to_list(\n        self,\n        list_to_update: List[Dict],\n \
    \       query: str,\n        response: str,\n        additional_field=None,\n\
    \    ) -> List[Dict]:\n        \"\"\"\n        Add response to the instruct list.\n\
    \        Args:\n            list_to_update (List[Dict]): The list to update.\n\
    \            query (str): The query to be added.\n            response (str):\
    \ The response to be added.\n            additional_field (Any): Additional field\
    \ to be added.\n        Returns:\n            List[Dict]: The updated list.\n\
    \        \"\"\"\n        list_to_update.append(\n            {\"instruction\"\
    : query, \"input\": additional_field, \"output\": response}\n        )\n     \
    \   return list_to_update\n\n    def get_response_from_llm(self, query: str, context:\
    \ str) -> str:\n        \"\"\"\n        Get language model response to query for\
    \ given context.\n        Args:\n            query (str): The query to be used\
    \ for generating the response.\n            context (str): The context to be used\
    \ for generating the response.\n        Returns:\n            str: The generated\
    \ response.\n        \"\"\"\n\n        def get_context_and_prompt(query, context,\
    \ code_qa_list):\n            full_context = f\"{context}\\nCODE Q and A:\\n{code_qa_list}\"\
    \n            prompt = self.model_config[\"prompt_template\"].format(\n      \
    \          context=full_context, query=query\n            )\n            context_size\
    \ = len(self.llm.tokenize(prompt))\n            return prompt, context_size\n\n\
    \        # Creating a list of dictionaries for Q and A pairs to be used as additional\
    \ LLM context\n        excluded_instructions = [\"Call code graph\", \"Docstring\"\
    ]\n        code_qa_list = [\n            {item[\"instruction\"].split(\" in Python\
    \ file:\")[0]: item[\"output\"]}\n            for item in self.instruct_list\n\
    \            if not any(\n                item[\"instruction\"].startswith(prefix)\n\
    \                for prefix in excluded_instructions\n            )\n        ]\n\
    \n        # Manage context length for LLM starting with the longest and most comprehensive\n\
    \        context_strategies = [\n            lambda: \"```python\\n\" + str(context)\
    \ + \"\\n```\",\n            lambda: \"```python\\n\"\n            + str(self.file_details[\"\
    file_info\"][\"file_code_simplified\"])\n            + \"\\n```\",\n         \
    \   lambda: self.get_string_from_info(\n                self.file_details[\"file_info\"\
    ], \"file_summary\"\n            ),\n            lambda: \"\",\n        ]\n  \
    \      max_context_length = self.model_config[\"inference_model\"][\"model_params\"\
    ][\n            \"context_length\"\n        ]\n        for strategy in context_strategies:\n\
    \            context = strategy()\n            prompt, context_size = get_context_and_prompt(query,\
    \ context, code_qa_list)\n            if context_size <= 0.70 * max_context_length:\n\
    \                break\n            else:\n                logging.error(\n  \
    \                  f\"Model response failed, increase py2dataset_model_config.yaml\
    \ context_length > {math.ceil(context_size/0.70)}\"\n                )\n     \
    \           return \"\"\n\n        response = \"\"\n        try:  # get response\
    \ from LLM\n            response = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(prompt))\n\
    \            code_elements_combined = {}\n            for item in code_qa_list:\n\
    \                code_elements_combined.update(item)\n            code_elements_json\
    \ = json.dumps(\n                {\"Code Elements\": code_elements_combined},\
    \ indent=4\n            )\n            code_elements_json = json.dumps(group_json(json.loads(code_elements_json)),\
    \ indent = 4)\n            response += \"\\n\" + code_elements_json  # Appending\
    \ the JSON formatted string\n            logging.info(f\"***Overall Response:\
    \ {response}\")\n\n        except Exception as error:\n            logging.error(f\"\
    Failed to generate model response: {error}\")\n\n        if self.detailed:  #\
    \ Get llm response for each code_qa_list item\n            for item in code_qa_list:\n\
    \                instruction = (\n                    f\"Describe the purpose\
    \ and significance of these objects within the code: '{item}'\"\n            \
    \    )\n                item_prompt = f\"\\n### Instruction:\\nUsing this context:\\\
    n{context}\\n\\n{instruction}.\\n### Response:\"\n                try:\n     \
    \               item_response = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(item_prompt))\n\
    \                    logging.info(\n                        f\"\\n***Itemized\
    \ Response: {instruction}\\n{item_response}\"\n                    )\n       \
    \         except Exception as error:\n                    logging.error(f\"Failed\
    \ to generate model response: {error}\")\n\n                # replace the output\
    \ value in self.instruct_list with the item.key + this_response\n            \
    \    for i, instruct_item in enumerate(self.instruct_list):\n                \
    \    if instruct_item[\"instruction\"].startswith(list(item.keys())[0]):\n   \
    \                     instruct_item[\"output\"] = (\n                        \
    \    list(item.keys())[0]\n                            + \":\"\n             \
    \               + list(item.values())[0]\n                            + \"\\n\\\
    n\"\n                            + \"Purpose and Significance:\\n\"\n        \
    \                    + item_response\n                        )\n            \
    \            break\n\n        return response\n\n    def process_question(\n \
    \       self, question_type: str, question_id: str, query: str, context: str,\
    \ info: Dict\n    ) -> None:\n        \"\"\"\n        Process question and add\
    \ the generated response to the instruct_list.\n        Args:\n            question_type\
    \ (str): The type of question to be processed.\n            question_id (str):\
    \ The ID of the question to be processed.\n            query (str): The query\
    \ to be processed.\n            context (str): The context to be used for generating\
    \ the response.`\n            info (Dict): The information of the Python file.\n\
    \        Returns:\n            None\n        \"\"\"\n        if question_id.endswith(\"\
    code_graph\") or question_id.endswith(\"docstring\"):\n            response =\
    \ info.get(question_id, {})\n        elif self.use_llm and question_id.endswith(\"\
    purpose\"):\n            response = self.get_response_from_llm(query, context)\n\
    \        else:\n            response = clean_and_get_unique_elements(str(info.get(question_id,\
    \ \"\")))\n\n        if question_type == \"file\":\n            context = (\n\
    \                \"```python\\n\"\n                + str(self.file_details[\"\
    file_info\"][\"file_code\"])\n                + \"\\n```\"\n            )\n  \
    \      if response and response != \"None\":\n            response_str = str(response).strip()\n\
    \            if response_str:\n                self.instruct_list.append(\n  \
    \                  {\"instruction\": query, \"input\": context, \"output\": response_str}\n\
    \                )\n\n    @staticmethod\n    def get_string_from_info(info, item_type):\n\
    \        \"\"\"\n        Get string from info dictionary.\n        Args:\n   \
    \         info (Dict): The information of the Python file.\n            item_type\
    \ (str): The type of item to get the string from.\n        Returns:\n        \
    \    str: The string from the info.\n        \"\"\"\n        if info[item_type]:\n\
    \            items = [item.strip() for item in str(info[item_type]).split(\",\"\
    ) if item]\n            return \", \".join(items)\n        return \"\"\n\n   \
    \ def process_question_type(\n        self, question_type: str, question_id: str,\
    \ question_text: str\n    ) -> None:\n        \"\"\"\n        Process questions\
    \ related to a file, function, class, or method.\n        Args:\n            question_type\
    \ (str): The type of question to be processed.\n            question_id (str):\
    \ The ID of the question to be processed.\n            question_text (str): The\
    \ text of the question to be processed.\n        Returns:\n            None\n\
    \        \"\"\"\n        if question_type == \"file\":\n            query = question_text.format(filename=self.base_name)\n\
    \            info = self.file_details[\"file_info\"]\n            context = self.file_details[\"\
    file_info\"][\"file_code\"]\n            self.process_question(question_type,\
    \ question_id, query, context, info)\n        elif question_type == \"method\"\
    :\n            for class_name, class_info in self.file_details[\"classes\"].items():\n\
    \                for key, method_info in class_info.items():\n               \
    \     if key.startswith(\"class_method_\"):\n                        method_name\
    \ = key[len(\"class_method_\") :]\n                        context = method_info[\"\
    method_code\"]\n                        mapping = {\"class_name\": class_name,\
    \ \"method_name\": method_name}\n                        query = question_text.format(filename=self.base_name,\
    \ **mapping)\n                        self.process_question(\n               \
    \             question_type, question_id, query, context, method_info\n      \
    \                  )\n        else:  # if question_type == 'function' or question_type\
    \ == 'class'\n            for name, info in self.file_details[\n             \
    \   self.question_mapping[question_type]\n            ].items():\n           \
    \     context = info[f\"{question_type}_code\"]\n                mapping = {f\"\
    {question_type}_name\": name}\n                if question_id == f\"{question_type}_purpose\"\
    \ and self.use_llm:\n                    variables_string = self.get_string_from_info(\n\
    \                        info, f\"{question_type}_variables\"\n              \
    \      )\n                    inputs_string = self.get_string_from_info(\n   \
    \                     info, f\"{question_type}_inputs\"\n                    )\n\
    \                    combined_string = \", \".join(\n                        [s\
    \ for s in [variables_string, inputs_string] if s]\n                    )\n  \
    \                  mapping[\n                        f\"{question_type}_variables\"\
    \n                    ] = clean_and_get_unique_elements(combined_string)\n   \
    \                 \n                    if question_type == \"class\":\n     \
    \                   methods_string = self.get_string_from_info(\n            \
    \                info, f\"{question_type}_methods\"\n                        )\n\
    \                        mapping[f\"{question_type}_methods\"] = methods_string\n\
    \n                query = question_text.format(filename=self.base_name, **mapping)\n\
    \                self.process_question(question_type, question_id, query, context,\
    \ info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n        \"\
    \"\"\n        Generate responses for all the questions and returns the instruct_list.\n\
    \        Args:\n            None\n        Returns:\n            Tuple[List[Dict],\
    \ List[Dict]]: The generated question-answer pairs and instructions.\n       \
    \ \"\"\"\n        for question in self.questions:\n            self.process_question_type(\n\
    \                question[\"type\"], question[\"id\"], question[\"text\"]\n  \
    \          )\n        return self.instruct_list\n\n\ndef get_python_datasets(\n\
    \    file_path: str,\n    file_details: Dict,\n    base_name: str,\n    questions:\
    \ List[Dict],\n    model_config: Dict,\n    detailed: bool,\n) -> Tuple[List[Dict],\
    \ List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return\
    \ it in JSON format.\n    Args:\n        file_path (str): The path to the Python\
    \ file.\n        file_details (Dict): The details of the file.\n        base_name\
    \ (str): The base Python code filename.\n        questions (List[Dict]): The list\
    \ of questions.\n        llm (object): The language model to be used for generating\
    \ responses.\n        prompt (str): The prompt to be used for generating responses.\n\
    \        detailed (bool): Flag indicating if detailed information should be extracted.\n\
    \    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in\
    \ JSON format.\n    \"\"\"\n    return DatasetGenerator(\n        file_path, file_details,\
    \ base_name, questions, model_config, detailed\n    ).generate()\n"
  file_dependencies:
  - re
  - math
  - json
  - logging
  - typing
  file_functions:
  - group_json
  - clean_and_get_unique_elements
  - element_generator
  - get_python_datasets
  file_classes:
  - DatasetGenerator
  file_constants: []
  file_summary: '{dependencies: [re, math, json, logging, typing], function_defs:
    [{group_json: {inputs: [input_json], calls: [input_json[''Code Elements''].items,
    key.split, output_json[''Code Elements''].setdefault(new_key, []).append, output_json[''Code
    Elements''].setdefault, output_json[''Code Elements''].setdefault(key, []).append,
    '', ''.join, output_json[''Code Elements''].items, dict, sorted], call_inputs:
    {input_json[''Code Elements''].items: [], key.split: [], output_json[''Code Elements''].setdefault(new_key,
    []).append: [value], output_json[''Code Elements''].setdefault: [new_key, [],
    key, []], output_json[''Code Elements''].setdefault(key, []).append: [value],
    '', ''.join: [v], output_json[''Code Elements''].items: [], dict: [sorted(output_json[''Code
    Elements''].items())], sorted: [output_json[''Code Elements''].items()]}, returns:
    [output_json]}}, {clean_and_get_unique_elements: {inputs: [input_str], calls:
    [enumerate, input_str.strip(''[]\\''\'').strip, input_str.strip, element.strip(''\\''\
    '').strip, element.strip, element_generator, '', ''.join], call_inputs: {enumerate:
    [input_str], input_str.strip(''[]\\''\'').strip: [], input_str.strip: [''[]\\''\''],
    element.strip(''\\''\ '').strip: [], element.strip: [''\\''\ ''], element_generator:
    [input_str], '', ''.join: [cleaned_elements]}, returns: ['', ''.join(cleaned_elements)]}},
    {element_generator: {inputs: [input_str], calls: [enumerate], call_inputs: {enumerate:
    [input_str]}, returns: []}}, {get_python_datasets: {inputs: [file_path, file_details,
    base_name, questions, model_config, detailed], calls: [DatasetGenerator(file_path,
    file_details, base_name, questions, model_config, detailed).generate, DatasetGenerator],
    call_inputs: {DatasetGenerator(file_path, file_details, base_name, questions,
    model_config, detailed).generate: [], DatasetGenerator: [file_path, file_details,
    base_name, questions, model_config, detailed]}, returns: [DatasetGenerator(file_path,
    file_details, base_name, questions, model_config, detailed).generate()]}}], class_defs:
    [{DatasetGenerator: {method_defs: {__init__: {inputs: [self, file_path, file_details,
    base_name, questions, model_config, detailed], calls: [], call_inputs: {}, returns:
    []}, add_to_list: {inputs: [self, list_to_update, query, response, additional_field],
    calls: [list_to_update.append], call_inputs: {list_to_update.append: [{''instruction'':
    query, ''input'': additional_field, ''output'': response}]}, returns: [list_to_update]},
    get_response_from_llm: {inputs: [self, query, context], calls: [self.model_config[''prompt_template''].format,
    len, self.llm.tokenize, item[''instruction''].split, any, item[''instruction''].startswith,
    str, self.get_string_from_info, strategy, get_context_and_prompt, logging.error,
    math.ceil, re.sub, self.llm, code_elements_combined.update, json.dumps, group_json,
    json.loads, logging.info, enumerate, instruct_item[''instruction''].startswith,
    list, item.keys, item.values], call_inputs: {self.model_config[''prompt_template''].format:
    [], len: [self.llm.tokenize(prompt)], self.llm.tokenize: [prompt], item[''instruction''].split:
    ['' in Python file:''], any: [(item[''instruction''].startswith(prefix) for prefix
    in excluded_instructions)], item[''instruction''].startswith: [prefix], str: [context,
    self.file_details[''file_info''][''file_code_simplified'']], self.get_string_from_info:
    [self.file_details[''file_info''], ''file_summary''], strategy: [], get_context_and_prompt:
    [query, context, code_qa_list], logging.error: [f''Model response failed, increase
    py2dataset_model_config.yaml context_length > {math.ceil(context_size / 0.7)}'',
    f''Failed to generate model response: {error}'', f''Failed to generate model response:
    {error}''], math.ceil: [context_size / 0.7], re.sub: [''\\\\n\\\\s*\\\\n'', ''\\n\\n'',
    self.llm(prompt), ''\\\\n\\\\s*\\\\n'', ''\\n\\n'', self.llm(item_prompt)], self.llm:
    [prompt, item_prompt], code_elements_combined.update: [item], json.dumps: [{''Code
    Elements'': code_elements_combined}, group_json(json.loads(code_elements_json))],
    group_json: [json.loads(code_elements_json)], json.loads: [code_elements_json],
    logging.info: [f''***Overall Response: {response}'', f''\\n***Itemized Response:
    {instruction}\\n{item_response}''], enumerate: [self.instruct_list], instruct_item[''instruction''].startswith:
    [list(item.keys())[0]], list: [item.keys(), item.keys(), item.values()], item.keys:
    [], item.values: []}, returns: [response, (prompt, context_size), '''']}, get_context_and_prompt:
    {inputs: [query, context, code_qa_list], calls: [self.model_config[''prompt_template''].format,
    len, self.llm.tokenize], call_inputs: {self.model_config[''prompt_template''].format:
    [], len: [self.llm.tokenize(prompt)], self.llm.tokenize: [prompt]}, returns: [(prompt,
    context_size)]}, process_question: {inputs: [self, question_type, question_id,
    query, context, info], calls: [question_id.endswith, info.get, self.get_response_from_llm,
    clean_and_get_unique_elements, str, str(response).strip, self.instruct_list.append],
    call_inputs: {question_id.endswith: [''code_graph'', ''docstring'', ''purpose''],
    info.get: [question_id, {}, question_id, ''''], self.get_response_from_llm: [query,
    context], clean_and_get_unique_elements: [str(info.get(question_id, ''''))], str:
    [info.get(question_id, ''''), self.file_details[''file_info''][''file_code''],
    response], str(response).strip: [], self.instruct_list.append: [{''instruction'':
    query, ''input'': context, ''output'': response_str}]}, returns: []}, get_string_from_info:
    {inputs: [info, item_type], calls: [item.strip, str(info[item_type]).split, str,
    '', ''.join], call_inputs: {item.strip: [], str(info[item_type]).split: ['',''],
    str: [info[item_type]], '', ''.join: [items]}, returns: ['''', '', ''.join(items)]},
    process_question_type: {inputs: [self, question_type, question_id, question_text],
    calls: [question_text.format, self.process_question, self.file_details[''classes''].items,
    class_info.items, key.startswith, len, self.file_details[self.question_mapping[question_type]].items,
    self.get_string_from_info, '', ''.join, clean_and_get_unique_elements], call_inputs:
    {question_text.format: [], self.process_question: [question_type, question_id,
    query, context, info, question_type, question_id, query, context, method_info,
    question_type, question_id, query, context, info], self.file_details[''classes''].items:
    [], class_info.items: [], key.startswith: [''class_method_''], len: [''class_method_''],
    self.file_details[self.question_mapping[question_type]].items: [], self.get_string_from_info:
    [info, f''{question_type}_variables'', info, f''{question_type}_inputs'', info,
    f''{question_type}_methods''], '', ''.join: [[s for s in [variables_string, inputs_string]
    if s]], clean_and_get_unique_elements: [combined_string]}, returns: []}, generate:
    {inputs: [self], calls: [self.process_question_type], call_inputs: {self.process_question_type:
    [question[''type''], question[''id''], question[''text'']]}, returns: [self.instruct_list]}}}}]}'
  file_code_simplified: "import logging\nimport re\nimport math\nimport json\nfrom\
    \ typing import Dict, List, Tuple\n\ndef group_json(input_json):\n    output_json\
    \ = {'Code Elements': {}}\n    for key, value in input_json['Code Elements'].items():\n\
    \        if '`' in key:\n            new_key = f'{key.split()[-1]} {key.split()[0]}'\n\
    \            output_json['Code Elements'].setdefault(new_key, []).append(value)\n\
    \        else:\n            output_json['Code Elements'].setdefault(key, []).append(value)\n\
    \    output_json['Code Elements'] = {k: ', '.join(v) for k, v in output_json['Code\
    \ Elements'].items()}\n    output_json['Code Elements'] = dict(sorted(output_json['Code\
    \ Elements'].items()))\n    return output_json\n\ndef clean_and_get_unique_elements(input_str:\
    \ str) -> str:\n\n    def element_generator(input_str):\n        start, brace_level\
    \ = (0, 0)\n        for i, char in enumerate(input_str):\n            if char\
    \ in '{}':\n                brace_level += 1 if char == '{' else -1\n        \
    \    elif char == ',' and brace_level == 0:\n                yield input_str[start:i]\n\
    \                start = i + 1\n        yield input_str[start:]\n    input_str\
    \ = input_str.strip('[]\\'\"').strip()\n    cleaned_elements = [element.strip('\\\
    '\" ').strip() for element in element_generator(input_str) if element.strip()]\n\
    \    return ', '.join(cleaned_elements)\n\nclass DatasetGenerator:\n\n    def\
    \ __init__(self, file_path: str, file_details: Dict, base_name: str, questions:\
    \ List[Dict], model_config: Dict, detailed: bool) -> None:\n        self.file_path\
    \ = file_path\n        self.file_details = file_details\n        self.base_name\
    \ = base_name\n        self.questions = questions\n        self.model_config =\
    \ model_config\n        if model_config is not None:\n            self.llm = model_config['model']\n\
    \            self.use_llm = True\n            self.detailed = detailed\n     \
    \   else:\n            self.llm = None\n            self.use_llm = False\n   \
    \         self.detailed = False\n        self.instruct_list = []\n        self.question_mapping\
    \ = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n\
    \n    def add_to_list(self, list_to_update: List[Dict], query: str, response:\
    \ str, additional_field=None) -> List[Dict]:\n        list_to_update.append({'instruction':\
    \ query, 'input': additional_field, 'output': response})\n        return list_to_update\n\
    \n    def get_response_from_llm(self, query: str, context: str) -> str:\n\n  \
    \      def get_context_and_prompt(query, context, code_qa_list):\n           \
    \ full_context = f'{context}\\nCODE Q and A:\\n{code_qa_list}'\n            prompt\
    \ = self.model_config['prompt_template'].format(context=full_context, query=query)\n\
    \            context_size = len(self.llm.tokenize(prompt))\n            return\
    \ (prompt, context_size)\n        excluded_instructions = ['Call code graph',\
    \ 'Docstring']\n        code_qa_list = [{item['instruction'].split(' in Python\
    \ file:')[0]: item['output']} for item in self.instruct_list if not any((item['instruction'].startswith(prefix)\
    \ for prefix in excluded_instructions))]\n        context_strategies = [lambda:\
    \ '```python\\n' + str(context) + '\\n```', lambda: '```python\\n' + str(self.file_details['file_info']['file_code_simplified'])\
    \ + '\\n```', lambda: self.get_string_from_info(self.file_details['file_info'],\
    \ 'file_summary'), lambda: '']\n        max_context_length = self.model_config['inference_model']['model_params']['context_length']\n\
    \        for strategy in context_strategies:\n            context = strategy()\n\
    \            prompt, context_size = get_context_and_prompt(query, context, code_qa_list)\n\
    \            if context_size <= 0.7 * max_context_length:\n                break\n\
    \            else:\n                logging.error(f'Model response failed, increase\
    \ py2dataset_model_config.yaml context_length > {math.ceil(context_size / 0.7)}')\n\
    \                return ''\n        response = ''\n        try:\n            response\
    \ = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n            code_elements_combined\
    \ = {}\n            for item in code_qa_list:\n                code_elements_combined.update(item)\n\
    \            code_elements_json = json.dumps({'Code Elements': code_elements_combined},\
    \ indent=4)\n            code_elements_json = json.dumps(group_json(json.loads(code_elements_json)),\
    \ indent=4)\n            response += '\\n' + code_elements_json\n            logging.info(f'***Overall\
    \ Response: {response}')\n        except Exception as error:\n            logging.error(f'Failed\
    \ to generate model response: {error}')\n        if self.detailed:\n         \
    \   for item in code_qa_list:\n                instruction = f\"Describe the purpose\
    \ and significance of these objects within the code: '{item}'\"\n            \
    \    item_prompt = f'\\n### Instruction:\\nUsing this context:\\n{context}\\n\\\
    n{instruction}.\\n### Response:'\n                try:\n                    item_response\
    \ = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(item_prompt))\n            \
    \        logging.info(f'\\n***Itemized Response: {instruction}\\n{item_response}')\n\
    \                except Exception as error:\n                    logging.error(f'Failed\
    \ to generate model response: {error}')\n                for i, instruct_item\
    \ in enumerate(self.instruct_list):\n                    if instruct_item['instruction'].startswith(list(item.keys())[0]):\n\
    \                        instruct_item['output'] = list(item.keys())[0] + ':'\
    \ + list(item.values())[0] + '\\n\\n' + 'Purpose and Significance:\\n' + item_response\n\
    \                        break\n        return response\n\n    def process_question(self,\
    \ question_type: str, question_id: str, query: str, context: str, info: Dict)\
    \ -> None:\n        if question_id.endswith('code_graph') or question_id.endswith('docstring'):\n\
    \            response = info.get(question_id, {})\n        elif self.use_llm and\
    \ question_id.endswith('purpose'):\n            response = self.get_response_from_llm(query,\
    \ context)\n        else:\n            response = clean_and_get_unique_elements(str(info.get(question_id,\
    \ '')))\n        if question_type == 'file':\n            context = '```python\\\
    n' + str(self.file_details['file_info']['file_code']) + '\\n```'\n        if response\
    \ and response != 'None':\n            response_str = str(response).strip()\n\
    \            if response_str:\n                self.instruct_list.append({'instruction':\
    \ query, 'input': context, 'output': response_str})\n\n    @staticmethod\n   \
    \ def get_string_from_info(info, item_type):\n        if info[item_type]:\n  \
    \          items = [item.strip() for item in str(info[item_type]).split(',') if\
    \ item]\n            return ', '.join(items)\n        return ''\n\n    def process_question_type(self,\
    \ question_type: str, question_id: str, question_text: str) -> None:\n       \
    \ if question_type == 'file':\n            query = question_text.format(filename=self.base_name)\n\
    \            info = self.file_details['file_info']\n            context = self.file_details['file_info']['file_code']\n\
    \            self.process_question(question_type, question_id, query, context,\
    \ info)\n        elif question_type == 'method':\n            for class_name,\
    \ class_info in self.file_details['classes'].items():\n                for key,\
    \ method_info in class_info.items():\n                    if key.startswith('class_method_'):\n\
    \                        method_name = key[len('class_method_'):]\n          \
    \              context = method_info['method_code']\n                        mapping\
    \ = {'class_name': class_name, 'method_name': method_name}\n                 \
    \       query = question_text.format(filename=self.base_name, **mapping)\n   \
    \                     self.process_question(question_type, question_id, query,\
    \ context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n\
    \                context = info[f'{question_type}_code']\n                mapping\
    \ = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_purpose'\
    \ and self.use_llm:\n                    variables_string = self.get_string_from_info(info,\
    \ f'{question_type}_variables')\n                    inputs_string = self.get_string_from_info(info,\
    \ f'{question_type}_inputs')\n                    combined_string = ', '.join([s\
    \ for s in [variables_string, inputs_string] if s])\n                    mapping[f'{question_type}_variables']\
    \ = clean_and_get_unique_elements(combined_string)\n                    if question_type\
    \ == 'class':\n                        methods_string = self.get_string_from_info(info,\
    \ f'{question_type}_methods')\n                        mapping[f'{question_type}_methods']\
    \ = methods_string\n                query = question_text.format(filename=self.base_name,\
    \ **mapping)\n                self.process_question(question_type, question_id,\
    \ query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n\
    \        for question in self.questions:\n            self.process_question_type(question['type'],\
    \ question['id'], question['text'])\n        return self.instruct_list\n\ndef\
    \ get_python_datasets(file_path: str, file_details: Dict, base_name: str, questions:\
    \ List[Dict], model_config: Dict, detailed: bool) -> Tuple[List[Dict], List[Dict]]:\n\
    \    return DatasetGenerator(file_path, file_details, base_name, questions, model_config,\
    \ detailed).generate()"
  entire_code_graph:
    nodes:
    - DatasetGenerator
    - DatasetGenerator.__init__
    - DatasetGenerator.add_to_list
    - DatasetGenerator.get_response_from_llm
    - DatasetGenerator.get_context_and_prompt
    - DatasetGenerator.process_question
    - DatasetGenerator.get_string_from_info
    - DatasetGenerator.process_question_type
    - DatasetGenerator.generate
    - group_json
    - clean_and_get_unique_elements
    - element_generator
    - get_python_datasets
    - input_json['Code Elements'].items
    - key.split
    - output_json['Code Elements'].setdefault(new_key, []).append
    - output_json['Code Elements'].setdefault
    - output_json['Code Elements'].setdefault(key, []).append
    - ''', ''.join'
    - output_json['Code Elements'].items
    - dict
    - sorted
    - enumerate
    - input_str.strip('[]\'"').strip
    - input_str.strip
    - element.strip('\'" ').strip
    - element.strip
    - DatasetGenerator(file_path, file_details, base_name, questions, model_config,
      detailed).generate
    - list_to_update.append
    - self.model_config['prompt_template'].format
    - len
    - self.llm.tokenize
    - item['instruction'].split
    - any
    - item['instruction'].startswith
    - str
    - strategy
    - get_context_and_prompt
    - logging.error
    - math.ceil
    - re.sub
    - self.llm
    - code_elements_combined.update
    - json.dumps
    - json.loads
    - logging.info
    - instruct_item['instruction'].startswith
    - list
    - item.keys
    - item.values
    - question_id.endswith
    - info.get
    - str(response).strip
    - self.instruct_list.append
    - item.strip
    - str(info[item_type]).split
    - question_text.format
    - self.file_details['classes'].items
    - class_info.items
    - key.startswith
    - self.file_details[self.question_mapping[question_type]].items
    edges:
    - source: DatasetGenerator
      target: DatasetGenerator.__init__
      target_inputs:
      - self
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      - detailed
      target_returns: []
    - source: DatasetGenerator
      target: DatasetGenerator.add_to_list
      target_inputs:
      - self
      - list_to_update
      - query
      - response
      - additional_field
      target_returns:
      - list_to_update
    - source: DatasetGenerator
      target: DatasetGenerator.get_response_from_llm
      target_inputs:
      - self
      - query
      - context
      target_returns:
      - (prompt, context_size)
      - response
      - ''''''
    - source: DatasetGenerator
      target: DatasetGenerator.get_context_and_prompt
      target_inputs:
      - query
      - context
      - code_qa_list
      target_returns:
      - (prompt, context_size)
    - source: DatasetGenerator
      target: DatasetGenerator.process_question
      target_inputs:
      - self
      - question_type
      - question_id
      - query
      - context
      - info
      target_returns: []
    - source: DatasetGenerator
      target: DatasetGenerator.get_string_from_info
      target_inputs:
      - info
      - item_type
      target_returns:
      - ''', ''.join(items)'
      - ''''''
    - source: DatasetGenerator
      target: DatasetGenerator.process_question_type
      target_inputs:
      - self
      - question_type
      - question_id
      - question_text
      target_returns: []
    - source: DatasetGenerator
      target: DatasetGenerator.generate
      target_inputs:
      - self
      target_returns:
      - self.instruct_list
    - source: DatasetGenerator.add_to_list
      target: list_to_update.append
      target_inputs:
      - '{''instruction'': query, ''input'': additional_field, ''output'': response}'
    - source: DatasetGenerator.get_response_from_llm
      target: self.model_config['prompt_template'].format
      target_inputs: []
    - source: DatasetGenerator.get_response_from_llm
      target: len
      target_inputs:
      - self.llm.tokenize(prompt)
    - source: DatasetGenerator.get_response_from_llm
      target: self.llm.tokenize
      target_inputs:
      - prompt
    - source: DatasetGenerator.get_response_from_llm
      target: item['instruction'].split
      target_inputs:
      - ''' in Python file:'''
    - source: DatasetGenerator.get_response_from_llm
      target: any
      target_inputs:
      - (item['instruction'].startswith(prefix) for prefix in excluded_instructions)
    - source: DatasetGenerator.get_response_from_llm
      target: item['instruction'].startswith
      target_inputs:
      - prefix
    - source: DatasetGenerator.get_response_from_llm
      target: str
      target_inputs:
      - context
      - self.file_details['file_info']['file_code_simplified']
    - source: DatasetGenerator.get_response_from_llm
      target: DatasetGenerator.get_string_from_info
      target_inputs:
      - info
      - item_type
      target_returns:
      - ''', ''.join(items)'
      - ''''''
    - source: DatasetGenerator.get_response_from_llm
      target: strategy
      target_inputs: []
    - source: DatasetGenerator.get_response_from_llm
      target: get_context_and_prompt
      target_inputs:
      - query
      - context
      - code_qa_list
    - source: DatasetGenerator.get_response_from_llm
      target: logging.error
      target_inputs:
      - f'Model response failed, increase py2dataset_model_config.yaml context_length
        > {math.ceil(context_size / 0.7)}'
      - 'f''Failed to generate model response: {error}'''
      - 'f''Failed to generate model response: {error}'''
    - source: DatasetGenerator.get_response_from_llm
      target: math.ceil
      target_inputs:
      - context_size / 0.7
    - source: DatasetGenerator.get_response_from_llm
      target: re.sub
      target_inputs:
      - '''\\n\\s*\\n'''
      - '''\n\n'''
      - self.llm(prompt)
      - '''\\n\\s*\\n'''
      - '''\n\n'''
      - self.llm(item_prompt)
    - source: DatasetGenerator.get_response_from_llm
      target: self.llm
      target_inputs:
      - prompt
      - item_prompt
    - source: DatasetGenerator.get_response_from_llm
      target: code_elements_combined.update
      target_inputs:
      - item
    - source: DatasetGenerator.get_response_from_llm
      target: json.dumps
      target_inputs:
      - '{''Code Elements'': code_elements_combined}'
      - group_json(json.loads(code_elements_json))
    - source: DatasetGenerator.get_response_from_llm
      target: group_json
      target_inputs:
      - json.loads(code_elements_json)
      target_returns:
      - output_json
    - source: DatasetGenerator.get_response_from_llm
      target: json.loads
      target_inputs:
      - code_elements_json
    - source: DatasetGenerator.get_response_from_llm
      target: logging.info
      target_inputs:
      - 'f''***Overall Response: {response}'''
      - 'f''\n***Itemized Response: {instruction}\n{item_response}'''
    - source: DatasetGenerator.get_response_from_llm
      target: enumerate
      target_inputs:
      - self.instruct_list
    - source: DatasetGenerator.get_response_from_llm
      target: instruct_item['instruction'].startswith
      target_inputs:
      - list(item.keys())[0]
    - source: DatasetGenerator.get_response_from_llm
      target: list
      target_inputs:
      - item.keys()
      - item.keys()
      - item.values()
    - source: DatasetGenerator.get_response_from_llm
      target: item.keys
      target_inputs: []
    - source: DatasetGenerator.get_response_from_llm
      target: item.values
      target_inputs: []
    - source: DatasetGenerator.get_context_and_prompt
      target: self.model_config['prompt_template'].format
      target_inputs: []
    - source: DatasetGenerator.get_context_and_prompt
      target: len
      target_inputs:
      - self.llm.tokenize(prompt)
    - source: DatasetGenerator.get_context_and_prompt
      target: self.llm.tokenize
      target_inputs:
      - prompt
    - source: DatasetGenerator.process_question
      target: question_id.endswith
      target_inputs:
      - '''code_graph'''
      - '''docstring'''
      - '''purpose'''
    - source: DatasetGenerator.process_question
      target: info.get
      target_inputs:
      - question_id
      - '{}'
      - question_id
      - ''''''
    - source: DatasetGenerator.process_question
      target: DatasetGenerator.get_response_from_llm
      target_inputs:
      - self
      - query
      - context
      target_returns:
      - (prompt, context_size)
      - response
      - ''''''
    - source: DatasetGenerator.process_question
      target: clean_and_get_unique_elements
      target_inputs:
      - str(info.get(question_id, ''))
      target_returns:
      - ''', ''.join(cleaned_elements)'
    - source: DatasetGenerator.process_question
      target: str
      target_inputs:
      - info.get(question_id, '')
      - self.file_details['file_info']['file_code']
      - response
    - source: DatasetGenerator.process_question
      target: str(response).strip
      target_inputs: []
    - source: DatasetGenerator.process_question
      target: self.instruct_list.append
      target_inputs:
      - '{''instruction'': query, ''input'': context, ''output'': response_str}'
    - source: DatasetGenerator.get_string_from_info
      target: item.strip
      target_inputs: []
    - source: DatasetGenerator.get_string_from_info
      target: str(info[item_type]).split
      target_inputs:
      - ''','''
    - source: DatasetGenerator.get_string_from_info
      target: str
      target_inputs:
      - info[item_type]
    - source: DatasetGenerator.get_string_from_info
      target: ''', ''.join'
      target_inputs:
      - items
    - source: DatasetGenerator.process_question_type
      target: question_text.format
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: DatasetGenerator.process_question
      target_inputs:
      - self
      - question_type
      - question_id
      - query
      - context
      - info
      target_returns: []
    - source: DatasetGenerator.process_question_type
      target: self.file_details['classes'].items
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: class_info.items
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: key.startswith
      target_inputs:
      - '''class_method_'''
    - source: DatasetGenerator.process_question_type
      target: len
      target_inputs:
      - '''class_method_'''
    - source: DatasetGenerator.process_question_type
      target: self.file_details[self.question_mapping[question_type]].items
      target_inputs: []
    - source: DatasetGenerator.process_question_type
      target: DatasetGenerator.get_string_from_info
      target_inputs:
      - info
      - item_type
      target_returns:
      - ''', ''.join(items)'
      - ''''''
    - source: DatasetGenerator.process_question_type
      target: ''', ''.join'
      target_inputs:
      - '[s for s in [variables_string, inputs_string] if s]'
    - source: DatasetGenerator.process_question_type
      target: clean_and_get_unique_elements
      target_inputs:
      - combined_string
      target_returns:
      - ''', ''.join(cleaned_elements)'
    - source: DatasetGenerator.generate
      target: DatasetGenerator.process_question_type
      target_inputs:
      - self
      - question_type
      - question_id
      - question_text
      target_returns: []
    - source: group_json
      target: input_json['Code Elements'].items
      target_inputs: []
    - source: group_json
      target: key.split
      target_inputs: []
    - source: group_json
      target: output_json['Code Elements'].setdefault(new_key, []).append
      target_inputs:
      - value
    - source: group_json
      target: output_json['Code Elements'].setdefault
      target_inputs:
      - new_key
      - '[]'
      - key
      - '[]'
    - source: group_json
      target: output_json['Code Elements'].setdefault(key, []).append
      target_inputs:
      - value
    - source: group_json
      target: ''', ''.join'
      target_inputs:
      - v
    - source: group_json
      target: output_json['Code Elements'].items
      target_inputs: []
    - source: group_json
      target: dict
      target_inputs:
      - sorted(output_json['Code Elements'].items())
    - source: group_json
      target: sorted
      target_inputs:
      - output_json['Code Elements'].items()
    - source: clean_and_get_unique_elements
      target: enumerate
      target_inputs:
      - input_str
    - source: clean_and_get_unique_elements
      target: input_str.strip('[]\'"').strip
      target_inputs: []
    - source: clean_and_get_unique_elements
      target: input_str.strip
      target_inputs:
      - '''[]\''"'''
    - source: clean_and_get_unique_elements
      target: element.strip('\'" ').strip
      target_inputs: []
    - source: clean_and_get_unique_elements
      target: element.strip
      target_inputs:
      - '''\''" '''
    - source: clean_and_get_unique_elements
      target: element_generator
      target_inputs:
      - input_str
      target_returns: []
    - source: clean_and_get_unique_elements
      target: ''', ''.join'
      target_inputs:
      - cleaned_elements
    - source: element_generator
      target: enumerate
      target_inputs:
      - input_str
    - source: get_python_datasets
      target: DatasetGenerator(file_path, file_details, base_name, questions, model_config,
        detailed).generate
      target_inputs: []
    - source: get_python_datasets
      target: DatasetGenerator
      target_inputs:
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      - detailed
      target_returns: []
  control_flow_structure:
  - '''\nGenerates JSON format question-answer pairs and instructions for a Python
    file\nRequirements:\n[req01] The `DatasetGenerator` class shall:\n        a. Accept
    Python file path, file details, base name, list of questions,\n        and model
    configuration as input during instantiation.\n        b. Initialize and store
    Python file path, file details, base name,\n        question list, llm, and use_llm
    flag as class attributes.\n        d. Provide `get_response_from_llm` method to
    retrieve llm response.\n        e. Provide `process_question` method to process
    each question, generate\n        corresponding responses, and add to the instruct_list.\n        f.
    Provide `process_question_type` method to process questions\n        related to
    the file, functions, classes, and methods.\n        g. Provide `generate` method
    to generate responses for all questions\n        and return the instruct_list.\n        h.
    Internally manage question mapping to file details.\n[req02] The `get_python_datasets`
    function shall:\n        a. Accept a Python file path, file details, base name,
    questions list,\n        and model config as input.\n        b. Instantiate `DatasetGenerator`
    class using the provided input.\n        c. Generate instruct_list using `DatasetGenerator`
    `generate` method.\n        d. Return the generated `instruct_list`.\n[req03]
    The `clean_and_get_unique_elements` function shall:\n        a. Clean an input
    string (str) and return a string of unique elements.\n'''
  - ? 'def get_python_datasets(file_path: str, file_details: Dict, base_name: str,
      questions: List[Dict], model_config: Dict, detailed: bool)'
    : - '''\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path
        (str): The path to the Python file.\n        file_details (Dict): The details
        of the file.\n        base_name (str): The base Python code filename.\n        questions
        (List[Dict]): The list of questions.\n        llm (object): The language model
        to be used for generating responses.\n        prompt (str): The prompt to
        be used for generating responses.\n        detailed (bool): Flag indicating
        if detailed information should be extracted.\n    Returns:\n        Tuple[List[Dict],
        List[Dict]]: Extracted information in JSON format.\n    '''
      - return:
        - DatasetGenerator(file_path, file_details, base_name, questions, model_config,
          detailed).generate()
  - '''\nGenerates JSON format question-answer pairs and instructions for a Python
    file\nRequirements:\n[req01] The `DatasetGenerator` class shall:\n        a. Accept
    Python file path, file details, base name, list of questions,\n        and model
    configuration as input during instantiation.\n        b. Initialize and store
    Python file path, file details, base name,\n        question list, llm, and use_llm
    flag as class attributes.\n        d. Provide `get_response_from_llm` method to
    retrieve llm response.\n        e. Provide `process_question` method to process
    each question, generate\n        corresponding responses, and add to the instruct_list.\n        f.
    Provide `process_question_type` method to process questions\n        related to
    the file, functions, classes, and methods.\n        g. Provide `generate` method
    to generate responses for all questions\n        and return the instruct_list.\n        h.
    Internally manage question mapping to file details.\n[req02] The `get_python_datasets`
    function shall:\n        a. Accept a Python file path, file details, base name,
    questions list,\n        and model config as input.\n        b. Instantiate `DatasetGenerator`
    class using the provided input.\n        c. Generate instruct_list using `DatasetGenerator`
    `generate` method.\n        d. Return the generated `instruct_list`.\n[req03]
    The `clean_and_get_unique_elements` function shall:\n        a. Clean an input
    string (str) and return a string of unique elements.\n'''
  - import logging
  - import re
  - import math
  - import json
  - from typing import Dict, List, Tuple
  - def group_json(input_json):
    - '''\n    Group JSON formatted dictionary by key.\n    Args:\n        input_json
      (Dict): The input JSON formatted dictionary.\n    Returns:\n        Dict: The
      grouped JSON formatted dictionary.\n    '''
    - 'output_json = {''Code Elements'': {}}'
    - for (key, value) in input_json['Code Elements'].items():
      - if '`' in key:
        - new_key = f'{key.split()[-1]} {key.split()[0]}'
        - output_json['Code Elements'].setdefault(new_key, []).append(value)
        else:
        - output_json['Code Elements'].setdefault(key, []).append(value)
    - 'output_json[''Code Elements''] = {k: '', ''.join(v) for k, v in output_json[''Code
      Elements''].items()}'
    - output_json['Code Elements'] = dict(sorted(output_json['Code Elements'].items()))
    - return:
      - output_json
  - 'def clean_and_get_unique_elements(input_str: str)':
    - '''\n    Clean an input string (str) and return a string of unique elements.\n    Args:\n        input_str
      (str): The input string to be cleaned.\n    Returns:\n        str: The cleaned
      string.\n    '''
    - def element_generator(input_str):
      - start, brace_level = (0, 0)
      - for (i, char) in enumerate(input_str):
        - if char in '{}':
          - brace_level += 1 if char == '{' else -1
          elif char == ',' and brace_level == 0:
          - yield input_str[start:i]
          - start = i + 1
      - yield input_str[start:]
    - input_str = input_str.strip('[]\'"').strip()
    - cleaned_elements = [element.strip('\'" ').strip() for element in element_generator(input_str)
      if element.strip()]
    - return:
      - ''', ''.join(cleaned_elements)'
  - class DatasetGenerator:
    - '''\n    Generate JSON formatted dictionary outputs for a Python file.\n    Attributes:\n        file_path
      (str): The path to the Python file.\n        file_details (Dict[str, Any]):
      Details of the Python file.\n        base_name (str): The base name of the Python
      file.\n        questions (List[Dict[str, str]]): Questions for generating responses.\n        instruct_list
      (List[Dict[str, str]]): Storage for generated instructions.\n        question_mapping
      (Dict[str, str]): Mapping of question types to keys in file details.\n        use_llm
      (bool): Flag indicating if a language model should be used.\n        llm (object):
      The language model for generating responses.\n        prompt (str): The prompt
      format for querying the language model.\n    Methods:\n        add_to_list(list_to_update:
      List[Dict], query: str, response: str,\n        additional_field=None) -> List[Dict]:\n            Add
      response to the instruct list.\n        get_response_from_llm(query: str, context:
      str) -> str:\n            Get language model response to query for given context.\n        process_question(question_type:
      str, question_id: str, query: str,\n        context: str, info: Dict) -> None:\n            Process
      question and add generated response to the instruct_list.\n        process_question_type(question_type:
      str, question_id: str,\n        question_text: str) -> None:\n            Process
      question related to file, function, class, or method.\n        generate() ->
      Tuple[List[Dict], List[Dict]]:\n            Generate responses for all the questions
      and return the instruct_list.\n    '''
    - ? 'def __init__(self, file_path: str, file_details: Dict, base_name: str, questions:
        List[Dict], model_config: Dict, detailed: bool)'
      : - '''\n        Initialize the DatasetGenerator class.\n        Args:\n            file_path
          (str): The path to the Python file.\n            file_details (Dict[str,
          Any]): Details of the Python file.\n            base_name (str): The base
          name of the Python file.\n            questions (List[Dict[str, str]]):
          Questions for generating responses.\n            model_config (Dict): Configuration
          for the language model.\n        Returns:\n            None\n        '''
        - self.file_path = file_path
        - self.file_details = file_details
        - self.base_name = base_name
        - self.questions = questions
        - self.model_config = model_config
        - if model_config is not None:
          - self.llm = model_config['model']
          - self.use_llm = True
          - self.detailed = detailed
          else:
          - self.llm = None
          - self.use_llm = False
          - self.detailed = False
        - self.instruct_list = []
        - 'self.question_mapping = {''file'': ''file'', ''function'': ''functions'',
          ''class'': ''classes'', ''method'': ''classes''}'
    - 'def add_to_list(self, list_to_update: List[Dict], query: str, response: str, additional_field)':
      - '''\n        Add response to the instruct list.\n        Args:\n            list_to_update
        (List[Dict]): The list to update.\n            query (str): The query to be
        added.\n            response (str): The response to be added.\n            additional_field
        (Any): Additional field to be added.\n        Returns:\n            List[Dict]:
        The updated list.\n        '''
      - 'list_to_update.append({''instruction'': query, ''input'': additional_field,
        ''output'': response})'
      - return:
        - list_to_update
    - 'def get_response_from_llm(self, query: str, context: str)':
      - '''\n        Get language model response to query for given context.\n        Args:\n            query
        (str): The query to be used for generating the response.\n            context
        (str): The context to be used for generating the response.\n        Returns:\n            str:
        The generated response.\n        '''
      - def get_context_and_prompt(query, context, code_qa_list):
        - full_context = f'{context}\nCODE Q and A:\n{code_qa_list}'
        - prompt = self.model_config['prompt_template'].format(context=full_context,
          query=query)
        - context_size = len(self.llm.tokenize(prompt))
        - return:
          - (prompt, context_size)
      - excluded_instructions = ['Call code graph', 'Docstring']
      - 'code_qa_list = [{item[''instruction''].split('' in Python file:'')[0]: item[''output'']}
        for item in self.instruct_list if not any((item[''instruction''].startswith(prefix)
        for prefix in excluded_instructions))]'
      - 'context_strategies = [lambda: ''```python\n'' + str(context) + ''\n```'',
        lambda: ''```python\n'' + str(self.file_details[''file_info''][''file_code_simplified''])
        + ''\n```'', lambda: self.get_string_from_info(self.file_details[''file_info''],
        ''file_summary''), lambda: '''']'
      - max_context_length = self.model_config['inference_model']['model_params']['context_length']
      - for strategy in context_strategies:
        - context = strategy()
        - prompt, context_size = get_context_and_prompt(query, context, code_qa_list)
        - if context_size <= 0.7 * max_context_length:
          - break
          else:
          - logging.error(f'Model response failed, increase py2dataset_model_config.yaml
            context_length > {math.ceil(context_size / 0.7)}')
          - return:
            - ''''''
      - response = ''
      - try:
        - response = re.sub('\\n\\s*\\n', '\n\n', self.llm(prompt))
        - code_elements_combined = {}
        - for item in code_qa_list:
          - code_elements_combined.update(item)
        - 'code_elements_json = json.dumps({''Code Elements'': code_elements_combined},
          indent=4)'
        - code_elements_json = json.dumps(group_json(json.loads(code_elements_json)),
          indent=4)
        - response += '\n' + code_elements_json
        - 'logging.info(f''***Overall Response: {response}'')'
        except:
        - 'except Exception as :':
          - 'logging.error(f''Failed to generate model response: {error}'')'
      - if self.detailed:
        - for item in code_qa_list:
          - 'instruction = f"Describe the purpose and significance of these objects
            within the code: ''{item}''"'
          - item_prompt = f'\n### Instruction:\nUsing this context:\n{context}\n\n{instruction}.\n###
            Response:'
          - try:
            - item_response = re.sub('\\n\\s*\\n', '\n\n', self.llm(item_prompt))
            - 'logging.info(f''\n***Itemized Response: {instruction}\n{item_response}'')'
            except:
            - 'except Exception as :':
              - 'logging.error(f''Failed to generate model response: {error}'')'
          - for (i, instruct_item) in enumerate(self.instruct_list):
            - if instruct_item['instruction'].startswith(list(item.keys())[0]):
              - instruct_item['output'] = list(item.keys())[0] + ':' + list(item.values())[0]
                + '\n\n' + 'Purpose and Significance:\n' + item_response
              - break
      - return:
        - response
    - 'def process_question(self, question_type: str, question_id: str, query: str, context: str, info: Dict)':
      - '''\n        Process question and add the generated response to the instruct_list.\n        Args:\n            question_type
        (str): The type of question to be processed.\n            question_id (str):
        The ID of the question to be processed.\n            query (str): The query
        to be processed.\n            context (str): The context to be used for generating
        the response.`\n            info (Dict): The information of the Python file.\n        Returns:\n            None\n        '''
      - if question_id.endswith('code_graph') or question_id.endswith('docstring'):
        - response = info.get(question_id, {})
        elif self.use_llm and question_id.endswith('purpose'):
        - response = self.get_response_from_llm(query, context)
        else:
        - response = clean_and_get_unique_elements(str(info.get(question_id, '')))
      - if question_type == 'file':
        - context = '```python\n' + str(self.file_details['file_info']['file_code'])
          + '\n```'
      - if response and response != 'None':
        - response_str = str(response).strip()
        - if response_str:
          - 'self.instruct_list.append({''instruction'': query, ''input'': context,
            ''output'': response_str})'
    - def get_string_from_info(info, item_type):
      - '''\n        Get string from info dictionary.\n        Args:\n            info
        (Dict): The information of the Python file.\n            item_type (str):
        The type of item to get the string from.\n        Returns:\n            str:
        The string from the info.\n        '''
      - if info[item_type]:
        - items = [item.strip() for item in str(info[item_type]).split(',') if item]
        - return:
          - ''', ''.join(items)'
      - return:
        - ''''''
    - 'def process_question_type(self, question_type: str, question_id: str, question_text: str)':
      - '''\n        Process questions related to a file, function, class, or method.\n        Args:\n            question_type
        (str): The type of question to be processed.\n            question_id (str):
        The ID of the question to be processed.\n            question_text (str):
        The text of the question to be processed.\n        Returns:\n            None\n        '''
      - if question_type == 'file':
        - query = question_text.format(filename=self.base_name)
        - info = self.file_details['file_info']
        - context = self.file_details['file_info']['file_code']
        - self.process_question(question_type, question_id, query, context, info)
        elif question_type == 'method':
        - for (class_name, class_info) in self.file_details['classes'].items():
          - for (key, method_info) in class_info.items():
            - if key.startswith('class_method_'):
              - method_name = key[len('class_method_'):]
              - context = method_info['method_code']
              - 'mapping = {''class_name'': class_name, ''method_name'': method_name}'
              - query = question_text.format(filename=self.base_name, **mapping)
              - self.process_question(question_type, question_id, query, context,
                method_info)
        else:
        - for (name, info) in self.file_details[self.question_mapping[question_type]].items():
          - context = info[f'{question_type}_code']
          - 'mapping = {f''{question_type}_name'': name}'
          - if question_id == f'{question_type}_purpose' and self.use_llm:
            - variables_string = self.get_string_from_info(info, f'{question_type}_variables')
            - inputs_string = self.get_string_from_info(info, f'{question_type}_inputs')
            - combined_string = ', '.join([s for s in [variables_string, inputs_string]
              if s])
            - mapping[f'{question_type}_variables'] = clean_and_get_unique_elements(combined_string)
            - if question_type == 'class':
              - methods_string = self.get_string_from_info(info, f'{question_type}_methods')
              - mapping[f'{question_type}_methods'] = methods_string
          - query = question_text.format(filename=self.base_name, **mapping)
          - self.process_question(question_type, question_id, query, context, info)
    - def generate(self):
      - '''\n        Generate responses for all the questions and returns the instruct_list.\n        Args:\n            None\n        Returns:\n            Tuple[List[Dict],
        List[Dict]]: The generated question-answer pairs and instructions.\n        '''
      - for question in self.questions:
        - self.process_question_type(question['type'], question['id'], question['text'])
      - return:
        - self.instruct_list
  plantUML: "@startuml\n  :'\\nGenerates JSON format question-answer pairs and instructions\
    \ for a Python file\\nRequirements:\\n[req01] The `DatasetGenerator` class shall:\\\
    n        a. Accept Python file path, file details, base name, list of questions,\\\
    n        and model configuration as input during instantiation.\\n        b. Initialize\
    \ and store Python file path, file details, base name,\\n        question list,\
    \ llm, and use_llm flag as class attributes.\\n        d. Provide `get_response_from_llm`\
    \ method to retrieve llm response.\\n        e. Provide `process_question` method\
    \ to process each question, generate\\n        corresponding responses, and add\
    \ to the instruct_list.\\n        f. Provide `process_question_type` method to\
    \ process questions\\n        related to the file, functions, classes, and methods.\\\
    n        g. Provide `generate` method to generate responses for all questions\\\
    n        and return the instruct_list.\\n        h. Internally manage question\
    \ mapping to file details.\\n[req02] The `get_python_datasets` function shall:\\\
    n        a. Accept a Python file path, file details, base name, questions list,\\\
    n        and model config as input.\\n        b. Instantiate `DatasetGenerator`\
    \ class using the provided input.\\n        c. Generate instruct_list using `DatasetGenerator`\
    \ `generate` method.\\n        d. Return the generated `instruct_list`.\\n[req03]\
    \ The `clean_and_get_unique_elements` function shall:\\n        a. Clean an input\
    \ string (str) and return a string of unique elements.\\n';\n  class [\"'\\\\\
    n    Extract information from a Python file and return it in JSON format.\\\\\
    n    Args:\\\\n        file_path (str): The path to the Python file.\\\\n    \
    \    file_details (Dict): The details of the file.\\\\n        base_name (str):\
    \ The base Python code filename.\\\\n        questions (List[Dict]): The list\
    \ of questions.\\\\n        llm (object): The language model to be used for generating\
    \ responses.\\\\n        prompt (str): The prompt to be used for generating responses.\\\
    \\n        detailed (bool): Flag indicating if detailed information should be\
    \ extracted.\\\\n    Returns:\\\\n        Tuple[List[Dict], List[Dict]]: Extracted\
    \ information in JSON format.\\\\n    '\", {'return': ['DatasetGenerator(file_path,\
    \ file_details, base_name, questions, model_config, detailed).generate()']}] {\n\
    \    :'\\n    Extract information from a Python file and return it in JSON format.\\\
    n    Args:\\n        file_path (str): The path to the Python file.\\n        file_details\
    \ (Dict): The details of the file.\\n        base_name (str): The base Python\
    \ code filename.\\n        questions (List[Dict]): The list of questions.\\n \
    \       llm (object): The language model to be used for generating responses.\\\
    n        prompt (str): The prompt to be used for generating responses.\\n    \
    \    detailed (bool): Flag indicating if detailed information should be extracted.\\\
    n    Returns:\\n        Tuple[List[Dict], List[Dict]]: Extracted information in\
    \ JSON format.\\n    ';\n    :return;\n    :DatasetGenerator(file_path, file_details,\
    \ base_name, questions, model_config, detailed).generate();\n  }\n  :'\\nGenerates\
    \ JSON format question-answer pairs and instructions for a Python file\\nRequirements:\\\
    n[req01] The `DatasetGenerator` class shall:\\n        a. Accept Python file path,\
    \ file details, base name, list of questions,\\n        and model configuration\
    \ as input during instantiation.\\n        b. Initialize and store Python file\
    \ path, file details, base name,\\n        question list, llm, and use_llm flag\
    \ as class attributes.\\n        d. Provide `get_response_from_llm` method to\
    \ retrieve llm response.\\n        e. Provide `process_question` method to process\
    \ each question, generate\\n        corresponding responses, and add to the instruct_list.\\\
    n        f. Provide `process_question_type` method to process questions\\n   \
    \     related to the file, functions, classes, and methods.\\n        g. Provide\
    \ `generate` method to generate responses for all questions\\n        and return\
    \ the instruct_list.\\n        h. Internally manage question mapping to file details.\\\
    n[req02] The `get_python_datasets` function shall:\\n        a. Accept a Python\
    \ file path, file details, base name, questions list,\\n        and model config\
    \ as input.\\n        b. Instantiate `DatasetGenerator` class using the provided\
    \ input.\\n        c. Generate instruct_list using `DatasetGenerator` `generate`\
    \ method.\\n        d. Return the generated `instruct_list`.\\n[req03] The `clean_and_get_unique_elements`\
    \ function shall:\\n        a. Clean an input string (str) and return a string\
    \ of unique elements.\\n';\n  :import logging;\n  :import re;\n  :import math;\n\
    \  :import json;\n  :from typing import Dict, List, Tuple;\n  class [\"'\\\\n\
    \    Group JSON formatted dictionary by key.\\\\n    Args:\\\\n        input_json\
    \ (Dict): The input JSON formatted dictionary.\\\\n    Returns:\\\\n        Dict:\
    \ The grouped JSON formatted dictionary.\\\\n    '\", \"output_json = {'Code Elements':\
    \ {}}\", {\"for (key, value) in input_json['Code Elements'].items()\": [{\"if\
    \ '`' in key\": [\"new_key = f'{key.split()[-1]} {key.split()[0]}'\", \"output_json['Code\
    \ Elements'].setdefault(new_key, []).append(value)\"], 'else': [\"output_json['Code\
    \ Elements'].setdefault(key, []).append(value)\"]}]}, \"output_json['Code Elements']\
    \ = {k: ', '.join(v) for k, v in output_json['Code Elements'].items()}\", \"output_json['Code\
    \ Elements'] = dict(sorted(output_json['Code Elements'].items()))\", {'return':\
    \ ['output_json']}] {\n    :'\\n    Group JSON formatted dictionary by key.\\\
    n    Args:\\n        input_json (Dict): The input JSON formatted dictionary.\\\
    n    Returns:\\n        Dict: The grouped JSON formatted dictionary.\\n    ';\n\
    \    :output_json = {'Code Elements': {}};\n    while ([{\"if '`' in key\": [\"\
    new_key = f'{key.split()[-1]} {key.split()[0]}'\", \"output_json['Code Elements'].setdefault(new_key,\
    \ []).append(value)\"], 'else': [\"output_json['Code Elements'].setdefault(key,\
    \ []).append(value)\"]}]) {\n      if ([\"new_key = f'{key.split()[-1]} {key.split()[0]}'\"\
    , \"output_json['Code Elements'].setdefault(new_key, []).append(value)\"]) {\n\
    \        :new_key = f'{key.split()[-1]} {key.split()[0]}';\n        :output_json['Code\
    \ Elements'].setdefault(new_key, []).append(value);\n      }\n    }\n    :output_json['Code\
    \ Elements'] = {k: ', '.join(v) for k, v in output_json['Code Elements'].items()};\n\
    \    :output_json['Code Elements'] = dict(sorted(output_json['Code Elements'].items()));\n\
    \    :return;\n    :output_json;\n  }\n  class [\"'\\\\n    Clean an input string\
    \ (str) and return a string of unique elements.\\\\n    Args:\\\\n        input_str\
    \ (str): The input string to be cleaned.\\\\n    Returns:\\\\n        str: The\
    \ cleaned string.\\\\n    '\", {'def element_generator(input_str)': ['start, brace_level\
    \ = (0, 0)', {'for (i, char) in enumerate(input_str)': [{\"if char in '{}'\":\
    \ [\"brace_level += 1 if char == '{' else -1\"], \"elif char == ',' and brace_level\
    \ == 0\": ['yield input_str[start:i]', 'start = i + 1']}]}, 'yield input_str[start:]']},\
    \ 'input_str = input_str.strip(\\'[]\\\\\\'\"\\').strip()', 'cleaned_elements\
    \ = [element.strip(\\'\\\\\\'\" \\').strip() for element in element_generator(input_str)\
    \ if element.strip()]', {'return': [\"', '.join(cleaned_elements)\"]}] {\n   \
    \ :'\\n    Clean an input string (str) and return a string of unique elements.\\\
    n    Args:\\n        input_str (str): The input string to be cleaned.\\n    Returns:\\\
    n        str: The cleaned string.\\n    ';\n    class ['start, brace_level = (0,\
    \ 0)', {'for (i, char) in enumerate(input_str)': [{\"if char in '{}'\": [\"brace_level\
    \ += 1 if char == '{' else -1\"], \"elif char == ',' and brace_level == 0\": ['yield\
    \ input_str[start:i]', 'start = i + 1']}]}, 'yield input_str[start:]'] {\n   \
    \   :start, brace_level = (0, 0);\n      while ([{\"if char in '{}'\": [\"brace_level\
    \ += 1 if char == '{' else -1\"], \"elif char == ',' and brace_level == 0\": ['yield\
    \ input_str[start:i]', 'start = i + 1']}]) {\n        if ([\"brace_level += 1\
    \ if char == '{' else -1\"]) {\n          :brace_level += 1 if char == '{' else\
    \ -1;\n        }\n      }\n      :yield input_str[start:];\n    }\n    :input_str\
    \ = input_str.strip('[]\\'\"').strip();\n    :cleaned_elements = [element.strip('\\\
    '\" ').strip() for element in element_generator(input_str) if element.strip()];\n\
    \    :return;\n    :', '.join(cleaned_elements);\n  }\n  class [\"'\\\\n    Generate\
    \ JSON formatted dictionary outputs for a Python file.\\\\n    Attributes:\\\\\
    n        file_path (str): The path to the Python file.\\\\n        file_details\
    \ (Dict[str, Any]): Details of the Python file.\\\\n        base_name (str): The\
    \ base name of the Python file.\\\\n        questions (List[Dict[str, str]]):\
    \ Questions for generating responses.\\\\n        instruct_list (List[Dict[str,\
    \ str]]): Storage for generated instructions.\\\\n        question_mapping (Dict[str,\
    \ str]): Mapping of question types to keys in file details.\\\\n        use_llm\
    \ (bool): Flag indicating if a language model should be used.\\\\n        llm\
    \ (object): The language model for generating responses.\\\\n        prompt (str):\
    \ The prompt format for querying the language model.\\\\n    Methods:\\\\n   \
    \     add_to_list(list_to_update: List[Dict], query: str, response: str,\\\\n\
    \        additional_field=None) -> List[Dict]:\\\\n            Add response to\
    \ the instruct list.\\\\n        get_response_from_llm(query: str, context: str)\
    \ -> str:\\\\n            Get language model response to query for given context.\\\
    \\n        process_question(question_type: str, question_id: str, query: str,\\\
    \\n        context: str, info: Dict) -> None:\\\\n            Process question\
    \ and add generated response to the instruct_list.\\\\n        process_question_type(question_type:\
    \ str, question_id: str,\\\\n        question_text: str) -> None:\\\\n       \
    \     Process question related to file, function, class, or method.\\\\n     \
    \   generate() -> Tuple[List[Dict], List[Dict]]:\\\\n            Generate responses\
    \ for all the questions and return the instruct_list.\\\\n    '\", {'def __init__(self,\
    \ file_path: str, file_details: Dict, base_name: str, questions: List[Dict], model_config:\
    \ Dict, detailed: bool)': [\"'\\\\n        Initialize the DatasetGenerator class.\\\
    \\n        Args:\\\\n            file_path (str): The path to the Python file.\\\
    \\n            file_details (Dict[str, Any]): Details of the Python file.\\\\\
    n            base_name (str): The base name of the Python file.\\\\n         \
    \   questions (List[Dict[str, str]]): Questions for generating responses.\\\\\
    n            model_config (Dict): Configuration for the language model.\\\\n \
    \       Returns:\\\\n            None\\\\n        '\", 'self.file_path = file_path',\
    \ 'self.file_details = file_details', 'self.base_name = base_name', 'self.questions\
    \ = questions', 'self.model_config = model_config', {'if model_config is not None':\
    \ [\"self.llm = model_config['model']\", 'self.use_llm = True', 'self.detailed\
    \ = detailed'], 'else': ['self.llm = None', 'self.use_llm = False', 'self.detailed\
    \ = False']}, 'self.instruct_list = []', \"self.question_mapping = {'file': 'file',\
    \ 'function': 'functions', 'class': 'classes', 'method': 'classes'}\"]}, {'def\
    \ add_to_list(self, list_to_update: List[Dict], query: str, response: str, additional_field)':\
    \ [\"'\\\\n        Add response to the instruct list.\\\\n        Args:\\\\n \
    \           list_to_update (List[Dict]): The list to update.\\\\n            query\
    \ (str): The query to be added.\\\\n            response (str): The response to\
    \ be added.\\\\n            additional_field (Any): Additional field to be added.\\\
    \\n        Returns:\\\\n            List[Dict]: The updated list.\\\\n       \
    \ '\", \"list_to_update.append({'instruction': query, 'input': additional_field,\
    \ 'output': response})\", {'return': ['list_to_update']}]}, {'def get_response_from_llm(self,\
    \ query: str, context: str)': [\"'\\\\n        Get language model response to\
    \ query for given context.\\\\n        Args:\\\\n            query (str): The\
    \ query to be used for generating the response.\\\\n            context (str):\
    \ The context to be used for generating the response.\\\\n        Returns:\\\\\
    n            str: The generated response.\\\\n        '\", {'def get_context_and_prompt(query,\
    \ context, code_qa_list)': [\"full_context = f'{context}\\\\nCODE Q and A:\\\\\
    n{code_qa_list}'\", \"prompt = self.model_config['prompt_template'].format(context=full_context,\
    \ query=query)\", 'context_size = len(self.llm.tokenize(prompt))', {'return':\
    \ ['(prompt, context_size)']}]}, \"excluded_instructions = ['Call code graph',\
    \ 'Docstring']\", \"code_qa_list = [{item['instruction'].split(' in Python file:')[0]:\
    \ item['output']} for item in self.instruct_list if not any((item['instruction'].startswith(prefix)\
    \ for prefix in excluded_instructions))]\", \"context_strategies = [lambda: '```python\\\
    \\n' + str(context) + '\\\\n```', lambda: '```python\\\\n' + str(self.file_details['file_info']['file_code_simplified'])\
    \ + '\\\\n```', lambda: self.get_string_from_info(self.file_details['file_info'],\
    \ 'file_summary'), lambda: '']\", \"max_context_length = self.model_config['inference_model']['model_params']['context_length']\"\
    , {'for strategy in context_strategies': ['context = strategy()', 'prompt, context_size\
    \ = get_context_and_prompt(query, context, code_qa_list)', {'if context_size <=\
    \ 0.7 * max_context_length': ['break'], 'else': [\"logging.error(f'Model response\
    \ failed, increase py2dataset_model_config.yaml context_length > {math.ceil(context_size\
    \ / 0.7)}')\", {'return': [\"''\"]}]}]}, \"response = ''\", {'try': [\"response\
    \ = re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n', '\\\\n\\\\n', self.llm(prompt))\",\
    \ 'code_elements_combined = {}', {'for item in code_qa_list': ['code_elements_combined.update(item)']},\
    \ \"code_elements_json = json.dumps({'Code Elements': code_elements_combined},\
    \ indent=4)\", 'code_elements_json = json.dumps(group_json(json.loads(code_elements_json)),\
    \ indent=4)', \"response += '\\\\n' + code_elements_json\", \"logging.info(f'***Overall\
    \ Response: {response}')\"], 'except': [{'except Exception as :': [\"logging.error(f'Failed\
    \ to generate model response: {error}')\"]}]}, {'if self.detailed': [{'for item\
    \ in code_qa_list': ['instruction = f\"Describe the purpose and significance of\
    \ these objects within the code: \\'{item}\\'\"', \"item_prompt = f'\\\\n### Instruction:\\\
    \\nUsing this context:\\\\n{context}\\\\n\\\\n{instruction}.\\\\n### Response:'\"\
    , {'try': [\"item_response = re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n', '\\\\n\\\\\
    n', self.llm(item_prompt))\", \"logging.info(f'\\\\n***Itemized Response: {instruction}\\\
    \\n{item_response}')\"], 'except': [{'except Exception as :': [\"logging.error(f'Failed\
    \ to generate model response: {error}')\"]}]}, {'for (i, instruct_item) in enumerate(self.instruct_list)':\
    \ [{\"if instruct_item['instruction'].startswith(list(item.keys())[0])\": [\"\
    instruct_item['output'] = list(item.keys())[0] + ':' + list(item.values())[0]\
    \ + '\\\\n\\\\n' + 'Purpose and Significance:\\\\n' + item_response\", 'break']}]}]}]},\
    \ {'return': ['response']}]}, {'def process_question(self, question_type: str,\
    \ question_id: str, query: str, context: str, info: Dict)': [\"'\\\\n        Process\
    \ question and add the generated response to the instruct_list.\\\\n        Args:\\\
    \\n            question_type (str): The type of question to be processed.\\\\\
    n            question_id (str): The ID of the question to be processed.\\\\n \
    \           query (str): The query to be processed.\\\\n            context (str):\
    \ The context to be used for generating the response.`\\\\n            info (Dict):\
    \ The information of the Python file.\\\\n        Returns:\\\\n            None\\\
    \\n        '\", {\"if question_id.endswith('code_graph') or question_id.endswith('docstring')\"\
    : ['response = info.get(question_id, {})'], \"elif self.use_llm and question_id.endswith('purpose')\"\
    : ['response = self.get_response_from_llm(query, context)'], 'else': [\"response\
    \ = clean_and_get_unique_elements(str(info.get(question_id, '')))\"]}, {\"if question_type\
    \ == 'file'\": [\"context = '```python\\\\n' + str(self.file_details['file_info']['file_code'])\
    \ + '\\\\n```'\"]}, {\"if response and response != 'None'\": ['response_str =\
    \ str(response).strip()', {'if response_str': [\"self.instruct_list.append({'instruction':\
    \ query, 'input': context, 'output': response_str})\"]}]}]}, {'def get_string_from_info(info,\
    \ item_type)': [\"'\\\\n        Get string from info dictionary.\\\\n        Args:\\\
    \\n            info (Dict): The information of the Python file.\\\\n         \
    \   item_type (str): The type of item to get the string from.\\\\n        Returns:\\\
    \\n            str: The string from the info.\\\\n        '\", {'if info[item_type]':\
    \ [\"items = [item.strip() for item in str(info[item_type]).split(',') if item]\"\
    , {'return': [\"', '.join(items)\"]}]}, {'return': [\"''\"]}]}, {'def process_question_type(self,\
    \ question_type: str, question_id: str, question_text: str)': [\"'\\\\n      \
    \  Process questions related to a file, function, class, or method.\\\\n     \
    \   Args:\\\\n            question_type (str): The type of question to be processed.\\\
    \\n            question_id (str): The ID of the question to be processed.\\\\\
    n            question_text (str): The text of the question to be processed.\\\\\
    n        Returns:\\\\n            None\\\\n        '\", {\"if question_type ==\
    \ 'file'\": ['query = question_text.format(filename=self.base_name)', \"info =\
    \ self.file_details['file_info']\", \"context = self.file_details['file_info']['file_code']\"\
    , 'self.process_question(question_type, question_id, query, context, info)'],\
    \ \"elif question_type == 'method'\": [{\"for (class_name, class_info) in self.file_details['classes'].items()\"\
    : [{'for (key, method_info) in class_info.items()': [{\"if key.startswith('class_method_')\"\
    : [\"method_name = key[len('class_method_'):]\", \"context = method_info['method_code']\"\
    , \"mapping = {'class_name': class_name, 'method_name': method_name}\", 'query\
    \ = question_text.format(filename=self.base_name, **mapping)', 'self.process_question(question_type,\
    \ question_id, query, context, method_info)']}]}]}], 'else': [{'for (name, info)\
    \ in self.file_details[self.question_mapping[question_type]].items()': [\"context\
    \ = info[f'{question_type}_code']\", \"mapping = {f'{question_type}_name': name}\"\
    , {\"if question_id == f'{question_type}_purpose' and self.use_llm\": [\"variables_string\
    \ = self.get_string_from_info(info, f'{question_type}_variables')\", \"inputs_string\
    \ = self.get_string_from_info(info, f'{question_type}_inputs')\", \"combined_string\
    \ = ', '.join([s for s in [variables_string, inputs_string] if s])\", \"mapping[f'{question_type}_variables']\
    \ = clean_and_get_unique_elements(combined_string)\", {\"if question_type == 'class'\"\
    : [\"methods_string = self.get_string_from_info(info, f'{question_type}_methods')\"\
    , \"mapping[f'{question_type}_methods'] = methods_string\"]}]}, 'query = question_text.format(filename=self.base_name,\
    \ **mapping)', 'self.process_question(question_type, question_id, query, context,\
    \ info)']}]}]}, {'def generate(self)': [\"'\\\\n        Generate responses for\
    \ all the questions and returns the instruct_list.\\\\n        Args:\\\\n    \
    \        None\\\\n        Returns:\\\\n            Tuple[List[Dict], List[Dict]]:\
    \ The generated question-answer pairs and instructions.\\\\n        '\", {'for\
    \ question in self.questions': [\"self.process_question_type(question['type'],\
    \ question['id'], question['text'])\"]}, {'return': ['self.instruct_list']}]}]\
    \ {\n    :'\\n    Generate JSON formatted dictionary outputs for a Python file.\\\
    n    Attributes:\\n        file_path (str): The path to the Python file.\\n  \
    \      file_details (Dict[str, Any]): Details of the Python file.\\n        base_name\
    \ (str): The base name of the Python file.\\n        questions (List[Dict[str,\
    \ str]]): Questions for generating responses.\\n        instruct_list (List[Dict[str,\
    \ str]]): Storage for generated instructions.\\n        question_mapping (Dict[str,\
    \ str]): Mapping of question types to keys in file details.\\n        use_llm\
    \ (bool): Flag indicating if a language model should be used.\\n        llm (object):\
    \ The language model for generating responses.\\n        prompt (str): The prompt\
    \ format for querying the language model.\\n    Methods:\\n        add_to_list(list_to_update:\
    \ List[Dict], query: str, response: str,\\n        additional_field=None) -> List[Dict]:\\\
    n            Add response to the instruct list.\\n        get_response_from_llm(query:\
    \ str, context: str) -> str:\\n            Get language model response to query\
    \ for given context.\\n        process_question(question_type: str, question_id:\
    \ str, query: str,\\n        context: str, info: Dict) -> None:\\n           \
    \ Process question and add generated response to the instruct_list.\\n       \
    \ process_question_type(question_type: str, question_id: str,\\n        question_text:\
    \ str) -> None:\\n            Process question related to file, function, class,\
    \ or method.\\n        generate() -> Tuple[List[Dict], List[Dict]]:\\n       \
    \     Generate responses for all the questions and return the instruct_list.\\\
    n    ';\n    class [\"'\\\\n        Initialize the DatasetGenerator class.\\\\\
    n        Args:\\\\n            file_path (str): The path to the Python file.\\\
    \\n            file_details (Dict[str, Any]): Details of the Python file.\\\\\
    n            base_name (str): The base name of the Python file.\\\\n         \
    \   questions (List[Dict[str, str]]): Questions for generating responses.\\\\\
    n            model_config (Dict): Configuration for the language model.\\\\n \
    \       Returns:\\\\n            None\\\\n        '\", 'self.file_path = file_path',\
    \ 'self.file_details = file_details', 'self.base_name = base_name', 'self.questions\
    \ = questions', 'self.model_config = model_config', {'if model_config is not None':\
    \ [\"self.llm = model_config['model']\", 'self.use_llm = True', 'self.detailed\
    \ = detailed'], 'else': ['self.llm = None', 'self.use_llm = False', 'self.detailed\
    \ = False']}, 'self.instruct_list = []', \"self.question_mapping = {'file': 'file',\
    \ 'function': 'functions', 'class': 'classes', 'method': 'classes'}\"] {\n   \
    \   :'\\n        Initialize the DatasetGenerator class.\\n        Args:\\n   \
    \         file_path (str): The path to the Python file.\\n            file_details\
    \ (Dict[str, Any]): Details of the Python file.\\n            base_name (str):\
    \ The base name of the Python file.\\n            questions (List[Dict[str, str]]):\
    \ Questions for generating responses.\\n            model_config (Dict): Configuration\
    \ for the language model.\\n        Returns:\\n            None\\n        ';\n\
    \      :self.file_path = file_path;\n      :self.file_details = file_details;\n\
    \      :self.base_name = base_name;\n      :self.questions = questions;\n    \
    \  :self.model_config = model_config;\n      if ([\"self.llm = model_config['model']\"\
    , 'self.use_llm = True', 'self.detailed = detailed']) {\n        :self.llm = model_config['model'];\n\
    \        :self.use_llm = True;\n        :self.detailed = detailed;\n      }\n\
    \      :self.instruct_list = [];\n      :self.question_mapping = {'file': 'file',\
    \ 'function': 'functions', 'class': 'classes', 'method': 'classes'};\n    }\n\
    \    class [\"'\\\\n        Add response to the instruct list.\\\\n        Args:\\\
    \\n            list_to_update (List[Dict]): The list to update.\\\\n         \
    \   query (str): The query to be added.\\\\n            response (str): The response\
    \ to be added.\\\\n            additional_field (Any): Additional field to be\
    \ added.\\\\n        Returns:\\\\n            List[Dict]: The updated list.\\\\\
    n        '\", \"list_to_update.append({'instruction': query, 'input': additional_field,\
    \ 'output': response})\", {'return': ['list_to_update']}] {\n      :'\\n     \
    \   Add response to the instruct list.\\n        Args:\\n            list_to_update\
    \ (List[Dict]): The list to update.\\n            query (str): The query to be\
    \ added.\\n            response (str): The response to be added.\\n          \
    \  additional_field (Any): Additional field to be added.\\n        Returns:\\\
    n            List[Dict]: The updated list.\\n        ';\n      :list_to_update.append({'instruction':\
    \ query, 'input': additional_field, 'output': response});\n      :return;\n  \
    \    :list_to_update;\n    }\n    class [\"'\\\\n        Get language model response\
    \ to query for given context.\\\\n        Args:\\\\n            query (str): The\
    \ query to be used for generating the response.\\\\n            context (str):\
    \ The context to be used for generating the response.\\\\n        Returns:\\\\\
    n            str: The generated response.\\\\n        '\", {'def get_context_and_prompt(query,\
    \ context, code_qa_list)': [\"full_context = f'{context}\\\\nCODE Q and A:\\\\\
    n{code_qa_list}'\", \"prompt = self.model_config['prompt_template'].format(context=full_context,\
    \ query=query)\", 'context_size = len(self.llm.tokenize(prompt))', {'return':\
    \ ['(prompt, context_size)']}]}, \"excluded_instructions = ['Call code graph',\
    \ 'Docstring']\", \"code_qa_list = [{item['instruction'].split(' in Python file:')[0]:\
    \ item['output']} for item in self.instruct_list if not any((item['instruction'].startswith(prefix)\
    \ for prefix in excluded_instructions))]\", \"context_strategies = [lambda: '```python\\\
    \\n' + str(context) + '\\\\n```', lambda: '```python\\\\n' + str(self.file_details['file_info']['file_code_simplified'])\
    \ + '\\\\n```', lambda: self.get_string_from_info(self.file_details['file_info'],\
    \ 'file_summary'), lambda: '']\", \"max_context_length = self.model_config['inference_model']['model_params']['context_length']\"\
    , {'for strategy in context_strategies': ['context = strategy()', 'prompt, context_size\
    \ = get_context_and_prompt(query, context, code_qa_list)', {'if context_size <=\
    \ 0.7 * max_context_length': ['break'], 'else': [\"logging.error(f'Model response\
    \ failed, increase py2dataset_model_config.yaml context_length > {math.ceil(context_size\
    \ / 0.7)}')\", {'return': [\"''\"]}]}]}, \"response = ''\", {'try': [\"response\
    \ = re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n', '\\\\n\\\\n', self.llm(prompt))\",\
    \ 'code_elements_combined = {}', {'for item in code_qa_list': ['code_elements_combined.update(item)']},\
    \ \"code_elements_json = json.dumps({'Code Elements': code_elements_combined},\
    \ indent=4)\", 'code_elements_json = json.dumps(group_json(json.loads(code_elements_json)),\
    \ indent=4)', \"response += '\\\\n' + code_elements_json\", \"logging.info(f'***Overall\
    \ Response: {response}')\"], 'except': [{'except Exception as :': [\"logging.error(f'Failed\
    \ to generate model response: {error}')\"]}]}, {'if self.detailed': [{'for item\
    \ in code_qa_list': ['instruction = f\"Describe the purpose and significance of\
    \ these objects within the code: \\'{item}\\'\"', \"item_prompt = f'\\\\n### Instruction:\\\
    \\nUsing this context:\\\\n{context}\\\\n\\\\n{instruction}.\\\\n### Response:'\"\
    , {'try': [\"item_response = re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n', '\\\\n\\\\\
    n', self.llm(item_prompt))\", \"logging.info(f'\\\\n***Itemized Response: {instruction}\\\
    \\n{item_response}')\"], 'except': [{'except Exception as :': [\"logging.error(f'Failed\
    \ to generate model response: {error}')\"]}]}, {'for (i, instruct_item) in enumerate(self.instruct_list)':\
    \ [{\"if instruct_item['instruction'].startswith(list(item.keys())[0])\": [\"\
    instruct_item['output'] = list(item.keys())[0] + ':' + list(item.values())[0]\
    \ + '\\\\n\\\\n' + 'Purpose and Significance:\\\\n' + item_response\", 'break']}]}]}]},\
    \ {'return': ['response']}] {\n      :'\\n        Get language model response\
    \ to query for given context.\\n        Args:\\n            query (str): The query\
    \ to be used for generating the response.\\n            context (str): The context\
    \ to be used for generating the response.\\n        Returns:\\n            str:\
    \ The generated response.\\n        ';\n      class [\"full_context = f'{context}\\\
    \\nCODE Q and A:\\\\n{code_qa_list}'\", \"prompt = self.model_config['prompt_template'].format(context=full_context,\
    \ query=query)\", 'context_size = len(self.llm.tokenize(prompt))', {'return':\
    \ ['(prompt, context_size)']}] {\n        :full_context = f'{context}\\nCODE Q\
    \ and A:\\n{code_qa_list}';\n        :prompt = self.model_config['prompt_template'].format(context=full_context,\
    \ query=query);\n        :context_size = len(self.llm.tokenize(prompt));\n   \
    \     :return;\n        :(prompt, context_size);\n      }\n      :excluded_instructions\
    \ = ['Call code graph', 'Docstring'];\n      :code_qa_list = [{item['instruction'].split('\
    \ in Python file:')[0]: item['output']} for item in self.instruct_list if not\
    \ any((item['instruction'].startswith(prefix) for prefix in excluded_instructions))];\n\
    \      :context_strategies = [lambda: '```python\\n' + str(context) + '\\n```',\
    \ lambda: '```python\\n' + str(self.file_details['file_info']['file_code_simplified'])\
    \ + '\\n```', lambda: self.get_string_from_info(self.file_details['file_info'],\
    \ 'file_summary'), lambda: ''];\n      :max_context_length = self.model_config['inference_model']['model_params']['context_length'];\n\
    \      while (['context = strategy()', 'prompt, context_size = get_context_and_prompt(query,\
    \ context, code_qa_list)', {'if context_size <= 0.7 * max_context_length': ['break'],\
    \ 'else': [\"logging.error(f'Model response failed, increase py2dataset_model_config.yaml\
    \ context_length > {math.ceil(context_size / 0.7)}')\", {'return': [\"''\"]}]}])\
    \ {\n        :context = strategy();\n        :prompt, context_size = get_context_and_prompt(query,\
    \ context, code_qa_list);\n        if (['break']) {\n          :break;\n     \
    \   }\n      }\n      :response = '';\n      :try;\n      :response = re.sub('\\\
    \\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt));\n      :code_elements_combined =\
    \ {};\n      while (['code_elements_combined.update(item)']) {\n        :code_elements_combined.update(item);\n\
    \      }\n      :code_elements_json = json.dumps({'Code Elements': code_elements_combined},\
    \ indent=4);\n      :code_elements_json = json.dumps(group_json(json.loads(code_elements_json)),\
    \ indent=4);\n      :response += '\\n' + code_elements_json;\n      :logging.info(f'***Overall\
    \ Response: {response}');\n      if ([{'for item in code_qa_list': ['instruction\
    \ = f\"Describe the purpose and significance of these objects within the code:\
    \ \\'{item}\\'\"', \"item_prompt = f'\\\\n### Instruction:\\\\nUsing this context:\\\
    \\n{context}\\\\n\\\\n{instruction}.\\\\n### Response:'\", {'try': [\"item_response\
    \ = re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n', '\\\\n\\\\n', self.llm(item_prompt))\"\
    , \"logging.info(f'\\\\n***Itemized Response: {instruction}\\\\n{item_response}')\"\
    ], 'except': [{'except Exception as :': [\"logging.error(f'Failed to generate\
    \ model response: {error}')\"]}]}, {'for (i, instruct_item) in enumerate(self.instruct_list)':\
    \ [{\"if instruct_item['instruction'].startswith(list(item.keys())[0])\": [\"\
    instruct_item['output'] = list(item.keys())[0] + ':' + list(item.values())[0]\
    \ + '\\\\n\\\\n' + 'Purpose and Significance:\\\\n' + item_response\", 'break']}]}]}])\
    \ {\n        while (['instruction = f\"Describe the purpose and significance of\
    \ these objects within the code: \\'{item}\\'\"', \"item_prompt = f'\\\\n### Instruction:\\\
    \\nUsing this context:\\\\n{context}\\\\n\\\\n{instruction}.\\\\n### Response:'\"\
    , {'try': [\"item_response = re.sub('\\\\\\\\n\\\\\\\\s*\\\\\\\\n', '\\\\n\\\\\
    n', self.llm(item_prompt))\", \"logging.info(f'\\\\n***Itemized Response: {instruction}\\\
    \\n{item_response}')\"], 'except': [{'except Exception as :': [\"logging.error(f'Failed\
    \ to generate model response: {error}')\"]}]}, {'for (i, instruct_item) in enumerate(self.instruct_list)':\
    \ [{\"if instruct_item['instruction'].startswith(list(item.keys())[0])\": [\"\
    instruct_item['output'] = list(item.keys())[0] + ':' + list(item.values())[0]\
    \ + '\\\\n\\\\n' + 'Purpose and Significance:\\\\n' + item_response\", 'break']}]}])\
    \ {\n          :instruction = f\"Describe the purpose and significance of these\
    \ objects within the code: '{item}'\";\n          :item_prompt = f'\\n### Instruction:\\\
    nUsing this context:\\n{context}\\n\\n{instruction}.\\n### Response:';\n     \
    \     :try;\n          :item_response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(item_prompt));\n\
    \          :logging.info(f'\\n***Itemized Response: {instruction}\\n{item_response}');\n\
    \          while ([{\"if instruct_item['instruction'].startswith(list(item.keys())[0])\"\
    : [\"instruct_item['output'] = list(item.keys())[0] + ':' + list(item.values())[0]\
    \ + '\\\\n\\\\n' + 'Purpose and Significance:\\\\n' + item_response\", 'break']}])\
    \ {\n            if ([\"instruct_item['output'] = list(item.keys())[0] + ':' +\
    \ list(item.values())[0] + '\\\\n\\\\n' + 'Purpose and Significance:\\\\n' + item_response\"\
    , 'break']) {\n              :instruct_item['output'] = list(item.keys())[0] +\
    \ ':' + list(item.values())[0] + '\\n\\n' + 'Purpose and Significance:\\n' + item_response;\n\
    \              :break;\n            }\n          }\n        }\n      }\n     \
    \ :return;\n      :response;\n    }\n    class [\"'\\\\n        Process question\
    \ and add the generated response to the instruct_list.\\\\n        Args:\\\\n\
    \            question_type (str): The type of question to be processed.\\\\n \
    \           question_id (str): The ID of the question to be processed.\\\\n  \
    \          query (str): The query to be processed.\\\\n            context (str):\
    \ The context to be used for generating the response.`\\\\n            info (Dict):\
    \ The information of the Python file.\\\\n        Returns:\\\\n            None\\\
    \\n        '\", {\"if question_id.endswith('code_graph') or question_id.endswith('docstring')\"\
    : ['response = info.get(question_id, {})'], \"elif self.use_llm and question_id.endswith('purpose')\"\
    : ['response = self.get_response_from_llm(query, context)'], 'else': [\"response\
    \ = clean_and_get_unique_elements(str(info.get(question_id, '')))\"]}, {\"if question_type\
    \ == 'file'\": [\"context = '```python\\\\n' + str(self.file_details['file_info']['file_code'])\
    \ + '\\\\n```'\"]}, {\"if response and response != 'None'\": ['response_str =\
    \ str(response).strip()', {'if response_str': [\"self.instruct_list.append({'instruction':\
    \ query, 'input': context, 'output': response_str})\"]}]}] {\n      :'\\n    \
    \    Process question and add the generated response to the instruct_list.\\n\
    \        Args:\\n            question_type (str): The type of question to be processed.\\\
    n            question_id (str): The ID of the question to be processed.\\n   \
    \         query (str): The query to be processed.\\n            context (str):\
    \ The context to be used for generating the response.`\\n            info (Dict):\
    \ The information of the Python file.\\n        Returns:\\n            None\\\
    n        ';\n      if (['response = info.get(question_id, {})']) {\n        :response\
    \ = info.get(question_id, {});\n      }\n      if ([\"context = '```python\\\\\
    n' + str(self.file_details['file_info']['file_code']) + '\\\\n```'\"]) {\n   \
    \     :context = '```python\\n' + str(self.file_details['file_info']['file_code'])\
    \ + '\\n```';\n      }\n      if (['response_str = str(response).strip()', {'if\
    \ response_str': [\"self.instruct_list.append({'instruction': query, 'input':\
    \ context, 'output': response_str})\"]}]) {\n        :response_str = str(response).strip();\n\
    \        if ([\"self.instruct_list.append({'instruction': query, 'input': context,\
    \ 'output': response_str})\"]) {\n          :self.instruct_list.append({'instruction':\
    \ query, 'input': context, 'output': response_str});\n        }\n      }\n   \
    \ }\n    class [\"'\\\\n        Get string from info dictionary.\\\\n        Args:\\\
    \\n            info (Dict): The information of the Python file.\\\\n         \
    \   item_type (str): The type of item to get the string from.\\\\n        Returns:\\\
    \\n            str: The string from the info.\\\\n        '\", {'if info[item_type]':\
    \ [\"items = [item.strip() for item in str(info[item_type]).split(',') if item]\"\
    , {'return': [\"', '.join(items)\"]}]}, {'return': [\"''\"]}] {\n      :'\\n \
    \       Get string from info dictionary.\\n        Args:\\n            info (Dict):\
    \ The information of the Python file.\\n            item_type (str): The type\
    \ of item to get the string from.\\n        Returns:\\n            str: The string\
    \ from the info.\\n        ';\n      if ([\"items = [item.strip() for item in\
    \ str(info[item_type]).split(',') if item]\", {'return': [\"', '.join(items)\"\
    ]}]) {\n        :items = [item.strip() for item in str(info[item_type]).split(',')\
    \ if item];\n        :return;\n        :', '.join(items);\n      }\n      :return;\n\
    \      :'';\n    }\n    class [\"'\\\\n        Process questions related to a\
    \ file, function, class, or method.\\\\n        Args:\\\\n            question_type\
    \ (str): The type of question to be processed.\\\\n            question_id (str):\
    \ The ID of the question to be processed.\\\\n            question_text (str):\
    \ The text of the question to be processed.\\\\n        Returns:\\\\n        \
    \    None\\\\n        '\", {\"if question_type == 'file'\": ['query = question_text.format(filename=self.base_name)',\
    \ \"info = self.file_details['file_info']\", \"context = self.file_details['file_info']['file_code']\"\
    , 'self.process_question(question_type, question_id, query, context, info)'],\
    \ \"elif question_type == 'method'\": [{\"for (class_name, class_info) in self.file_details['classes'].items()\"\
    : [{'for (key, method_info) in class_info.items()': [{\"if key.startswith('class_method_')\"\
    : [\"method_name = key[len('class_method_'):]\", \"context = method_info['method_code']\"\
    , \"mapping = {'class_name': class_name, 'method_name': method_name}\", 'query\
    \ = question_text.format(filename=self.base_name, **mapping)', 'self.process_question(question_type,\
    \ question_id, query, context, method_info)']}]}]}], 'else': [{'for (name, info)\
    \ in self.file_details[self.question_mapping[question_type]].items()': [\"context\
    \ = info[f'{question_type}_code']\", \"mapping = {f'{question_type}_name': name}\"\
    , {\"if question_id == f'{question_type}_purpose' and self.use_llm\": [\"variables_string\
    \ = self.get_string_from_info(info, f'{question_type}_variables')\", \"inputs_string\
    \ = self.get_string_from_info(info, f'{question_type}_inputs')\", \"combined_string\
    \ = ', '.join([s for s in [variables_string, inputs_string] if s])\", \"mapping[f'{question_type}_variables']\
    \ = clean_and_get_unique_elements(combined_string)\", {\"if question_type == 'class'\"\
    : [\"methods_string = self.get_string_from_info(info, f'{question_type}_methods')\"\
    , \"mapping[f'{question_type}_methods'] = methods_string\"]}]}, 'query = question_text.format(filename=self.base_name,\
    \ **mapping)', 'self.process_question(question_type, question_id, query, context,\
    \ info)']}]}] {\n      :'\\n        Process questions related to a file, function,\
    \ class, or method.\\n        Args:\\n            question_type (str): The type\
    \ of question to be processed.\\n            question_id (str): The ID of the\
    \ question to be processed.\\n            question_text (str): The text of the\
    \ question to be processed.\\n        Returns:\\n            None\\n        ';\n\
    \      if (['query = question_text.format(filename=self.base_name)', \"info =\
    \ self.file_details['file_info']\", \"context = self.file_details['file_info']['file_code']\"\
    , 'self.process_question(question_type, question_id, query, context, info)'])\
    \ {\n        :query = question_text.format(filename=self.base_name);\n       \
    \ :info = self.file_details['file_info'];\n        :context = self.file_details['file_info']['file_code'];\n\
    \        :self.process_question(question_type, question_id, query, context, info);\n\
    \      }\n    }\n    class [\"'\\\\n        Generate responses for all the questions\
    \ and returns the instruct_list.\\\\n        Args:\\\\n            None\\\\n \
    \       Returns:\\\\n            Tuple[List[Dict], List[Dict]]: The generated\
    \ question-answer pairs and instructions.\\\\n        '\", {'for question in self.questions':\
    \ [\"self.process_question_type(question['type'], question['id'], question['text'])\"\
    ]}, {'return': ['self.instruct_list']}] {\n      :'\\n        Generate responses\
    \ for all the questions and returns the instruct_list.\\n        Args:\\n    \
    \        None\\n        Returns:\\n            Tuple[List[Dict], List[Dict]]:\
    \ The generated question-answer pairs and instructions.\\n        ';\n      while\
    \ ([\"self.process_question_type(question['type'], question['id'], question['text'])\"\
    ]) {\n        :self.process_question_type(question['type'], question['id'], question['text']);\n\
    \      }\n      :return;\n      :self.instruct_list;\n    }\n  }\nend\n@enduml"
functions:
  group_json:
    function_name: group_json
    function_code: "def group_json(input_json):\n    \"\"\"\n    Group JSON formatted\
      \ dictionary by key.\n    Args:\n        input_json (Dict): The input JSON formatted\
      \ dictionary.\n    Returns:\n        Dict: The grouped JSON formatted dictionary.\n\
      \    \"\"\"\n    output_json = {'Code Elements': {}}\n    for key, value in\
      \ input_json['Code Elements'].items():\n        if '`' in key:\n           \
      \ new_key = f'{key.split()[-1]} {key.split()[0]}'\n            output_json['Code\
      \ Elements'].setdefault(new_key, []).append(value)\n        else:\n        \
      \    output_json['Code Elements'].setdefault(key, []).append(value)\n    output_json['Code\
      \ Elements'] = {k: ', '.join(v) for k, v in output_json['Code Elements'].items()}\n\
      \    output_json['Code Elements'] = dict(sorted(output_json['Code Elements'].items()))\n\
      \    return output_json"
    function_docstring: "\n    Group JSON formatted dictionary by key.\n    Args:\n\
      \        input_json (Dict): The input JSON formatted dictionary.\n    Returns:\n\
      \        Dict: The grouped JSON formatted dictionary.\n    "
    function_inputs:
    - input_json
    function_defaults: []
    function_returns:
    - output_json
    function_calls:
    - input_json['Code Elements'].items
    - key.split
    - output_json['Code Elements'].setdefault(new_key, []).append
    - output_json['Code Elements'].setdefault
    - output_json['Code Elements'].setdefault(key, []).append
    - ''', ''.join'
    - output_json['Code Elements'].items
    - dict
    - sorted
    function_call_inputs:
      input_json['Code Elements'].items: []
      key.split: []
      output_json['Code Elements'].setdefault(new_key, []).append:
      - value
      output_json['Code Elements'].setdefault:
      - new_key
      - '[]'
      - key
      - '[]'
      output_json['Code Elements'].setdefault(key, []).append:
      - value
      ''', ''.join':
      - v
      output_json['Code Elements'].items: []
      dict:
      - sorted(output_json['Code Elements'].items())
      sorted:
      - output_json['Code Elements'].items()
    function_variables:
    - new_key
    - output_json
    function_decorators: []
    function_annotations: []
    function_properties: []
  clean_and_get_unique_elements:
    function_name: clean_and_get_unique_elements
    function_code: "def clean_and_get_unique_elements(input_str: str) -> str:\n  \
      \  \"\"\"\n    Clean an input string (str) and return a string of unique elements.\n\
      \    Args:\n        input_str (str): The input string to be cleaned.\n    Returns:\n\
      \        str: The cleaned string.\n    \"\"\"\n\n    def element_generator(input_str):\n\
      \        start, brace_level = (0, 0)\n        for i, char in enumerate(input_str):\n\
      \            if char in '{}':\n                brace_level += 1 if char == '{'\
      \ else -1\n            elif char == ',' and brace_level == 0:\n            \
      \    yield input_str[start:i]\n                start = i + 1\n        yield\
      \ input_str[start:]\n    input_str = input_str.strip('[]\\'\"').strip()\n  \
      \  cleaned_elements = [element.strip('\\'\" ').strip() for element in element_generator(input_str)\
      \ if element.strip()]\n    return ', '.join(cleaned_elements)"
    function_docstring: "\n    Clean an input string (str) and return a string of\
      \ unique elements.\n    Args:\n        input_str (str): The input string to\
      \ be cleaned.\n    Returns:\n        str: The cleaned string.\n    "
    function_inputs:
    - input_str
    function_defaults: []
    function_returns:
    - ''', ''.join(cleaned_elements)'
    function_calls:
    - enumerate
    - input_str.strip('[]\'"').strip
    - input_str.strip
    - element.strip('\'" ').strip
    - element.strip
    - element_generator
    - ''', ''.join'
    function_call_inputs:
      enumerate:
      - input_str
      input_str.strip('[]\'"').strip: []
      input_str.strip:
      - '''[]\''"'''
      element.strip('\'" ').strip: []
      element.strip:
      - '''\''" '''
      element_generator:
      - input_str
      ''', ''.join':
      - cleaned_elements
    function_variables:
    - cleaned_elements
    - input_str
    - start
    function_decorators: []
    function_annotations: []
    function_properties: []
  element_generator:
    function_name: element_generator
    function_code: "def element_generator(input_str):\n    start, brace_level = (0,\
      \ 0)\n    for i, char in enumerate(input_str):\n        if char in '{}':\n \
      \           brace_level += 1 if char == '{' else -1\n        elif char == ','\
      \ and brace_level == 0:\n            yield input_str[start:i]\n            start\
      \ = i + 1\n    yield input_str[start:]"
    function_docstring: null
    function_inputs:
    - input_str
    function_defaults: []
    function_returns: []
    function_calls:
    - enumerate
    function_call_inputs:
      enumerate:
      - input_str
    function_variables:
    - start
    function_decorators: []
    function_annotations: []
    function_properties: []
  get_python_datasets:
    function_name: get_python_datasets
    function_code: "def get_python_datasets(file_path: str, file_details: Dict, base_name:\
      \ str, questions: List[Dict], model_config: Dict, detailed: bool) -> Tuple[List[Dict],\
      \ List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return\
      \ it in JSON format.\n    Args:\n        file_path (str): The path to the Python\
      \ file.\n        file_details (Dict): The details of the file.\n        base_name\
      \ (str): The base Python code filename.\n        questions (List[Dict]): The\
      \ list of questions.\n        llm (object): The language model to be used for\
      \ generating responses.\n        prompt (str): The prompt to be used for generating\
      \ responses.\n        detailed (bool): Flag indicating if detailed information\
      \ should be extracted.\n    Returns:\n        Tuple[List[Dict], List[Dict]]:\
      \ Extracted information in JSON format.\n    \"\"\"\n    return DatasetGenerator(file_path,\
      \ file_details, base_name, questions, model_config, detailed).generate()"
    function_docstring: "\n    Extract information from a Python file and return it\
      \ in JSON format.\n    Args:\n        file_path (str): The path to the Python\
      \ file.\n        file_details (Dict): The details of the file.\n        base_name\
      \ (str): The base Python code filename.\n        questions (List[Dict]): The\
      \ list of questions.\n        llm (object): The language model to be used for\
      \ generating responses.\n        prompt (str): The prompt to be used for generating\
      \ responses.\n        detailed (bool): Flag indicating if detailed information\
      \ should be extracted.\n    Returns:\n        Tuple[List[Dict], List[Dict]]:\
      \ Extracted information in JSON format.\n    "
    function_inputs:
    - file_path
    - file_details
    - base_name
    - questions
    - model_config
    - detailed
    function_defaults: []
    function_returns:
    - DatasetGenerator(file_path, file_details, base_name, questions, model_config,
      detailed).generate()
    function_calls:
    - DatasetGenerator(file_path, file_details, base_name, questions, model_config,
      detailed).generate
    - DatasetGenerator
    function_call_inputs:
      DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate: []
      DatasetGenerator:
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      - detailed
    function_variables: []
    function_decorators: []
    function_annotations: []
    function_properties: []
classes:
  DatasetGenerator:
    class_name: DatasetGenerator
    class_code: "class DatasetGenerator:\n    \"\"\"\n    Generate JSON formatted\
      \ dictionary outputs for a Python file.\n    Attributes:\n        file_path\
      \ (str): The path to the Python file.\n        file_details (Dict[str, Any]):\
      \ Details of the Python file.\n        base_name (str): The base name of the\
      \ Python file.\n        questions (List[Dict[str, str]]): Questions for generating\
      \ responses.\n        instruct_list (List[Dict[str, str]]): Storage for generated\
      \ instructions.\n        question_mapping (Dict[str, str]): Mapping of question\
      \ types to keys in file details.\n        use_llm (bool): Flag indicating if\
      \ a language model should be used.\n        llm (object): The language model\
      \ for generating responses.\n        prompt (str): The prompt format for querying\
      \ the language model.\n    Methods:\n        add_to_list(list_to_update: List[Dict],\
      \ query: str, response: str,\n        additional_field=None) -> List[Dict]:\n\
      \            Add response to the instruct list.\n        get_response_from_llm(query:\
      \ str, context: str) -> str:\n            Get language model response to query\
      \ for given context.\n        process_question(question_type: str, question_id:\
      \ str, query: str,\n        context: str, info: Dict) -> None:\n           \
      \ Process question and add generated response to the instruct_list.\n      \
      \  process_question_type(question_type: str, question_id: str,\n        question_text:\
      \ str) -> None:\n            Process question related to file, function, class,\
      \ or method.\n        generate() -> Tuple[List[Dict], List[Dict]]:\n       \
      \     Generate responses for all the questions and return the instruct_list.\n\
      \    \"\"\"\n\n    def __init__(self, file_path: str, file_details: Dict, base_name:\
      \ str, questions: List[Dict], model_config: Dict, detailed: bool) -> None:\n\
      \        \"\"\"\n        Initialize the DatasetGenerator class.\n        Args:\n\
      \            file_path (str): The path to the Python file.\n            file_details\
      \ (Dict[str, Any]): Details of the Python file.\n            base_name (str):\
      \ The base name of the Python file.\n            questions (List[Dict[str, str]]):\
      \ Questions for generating responses.\n            model_config (Dict): Configuration\
      \ for the language model.\n        Returns:\n            None\n        \"\"\"\
      \n        self.file_path = file_path\n        self.file_details = file_details\n\
      \        self.base_name = base_name\n        self.questions = questions\n  \
      \      self.model_config = model_config\n        if model_config is not None:\n\
      \            self.llm = model_config['model']\n            self.use_llm = True\n\
      \            self.detailed = detailed\n        else:\n            self.llm =\
      \ None\n            self.use_llm = False\n            self.detailed = False\n\
      \        self.instruct_list = []\n        self.question_mapping = {'file': 'file',\
      \ 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n\n    def\
      \ add_to_list(self, list_to_update: List[Dict], query: str, response: str, additional_field=None)\
      \ -> List[Dict]:\n        \"\"\"\n        Add response to the instruct list.\n\
      \        Args:\n            list_to_update (List[Dict]): The list to update.\n\
      \            query (str): The query to be added.\n            response (str):\
      \ The response to be added.\n            additional_field (Any): Additional\
      \ field to be added.\n        Returns:\n            List[Dict]: The updated\
      \ list.\n        \"\"\"\n        list_to_update.append({'instruction': query,\
      \ 'input': additional_field, 'output': response})\n        return list_to_update\n\
      \n    def get_response_from_llm(self, query: str, context: str) -> str:\n  \
      \      \"\"\"\n        Get language model response to query for given context.\n\
      \        Args:\n            query (str): The query to be used for generating\
      \ the response.\n            context (str): The context to be used for generating\
      \ the response.\n        Returns:\n            str: The generated response.\n\
      \        \"\"\"\n\n        def get_context_and_prompt(query, context, code_qa_list):\n\
      \            full_context = f'{context}\\nCODE Q and A:\\n{code_qa_list}'\n\
      \            prompt = self.model_config['prompt_template'].format(context=full_context,\
      \ query=query)\n            context_size = len(self.llm.tokenize(prompt))\n\
      \            return (prompt, context_size)\n        excluded_instructions =\
      \ ['Call code graph', 'Docstring']\n        code_qa_list = [{item['instruction'].split('\
      \ in Python file:')[0]: item['output']} for item in self.instruct_list if not\
      \ any((item['instruction'].startswith(prefix) for prefix in excluded_instructions))]\n\
      \        context_strategies = [lambda: '```python\\n' + str(context) + '\\n```',\
      \ lambda: '```python\\n' + str(self.file_details['file_info']['file_code_simplified'])\
      \ + '\\n```', lambda: self.get_string_from_info(self.file_details['file_info'],\
      \ 'file_summary'), lambda: '']\n        max_context_length = self.model_config['inference_model']['model_params']['context_length']\n\
      \        for strategy in context_strategies:\n            context = strategy()\n\
      \            prompt, context_size = get_context_and_prompt(query, context, code_qa_list)\n\
      \            if context_size <= 0.7 * max_context_length:\n                break\n\
      \            else:\n                logging.error(f'Model response failed, increase\
      \ py2dataset_model_config.yaml context_length > {math.ceil(context_size / 0.7)}')\n\
      \                return ''\n        response = ''\n        try:\n          \
      \  response = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n     \
      \       code_elements_combined = {}\n            for item in code_qa_list:\n\
      \                code_elements_combined.update(item)\n            code_elements_json\
      \ = json.dumps({'Code Elements': code_elements_combined}, indent=4)\n      \
      \      code_elements_json = json.dumps(group_json(json.loads(code_elements_json)),\
      \ indent=4)\n            response += '\\n' + code_elements_json\n          \
      \  logging.info(f'***Overall Response: {response}')\n        except Exception\
      \ as error:\n            logging.error(f'Failed to generate model response:\
      \ {error}')\n        if self.detailed:\n            for item in code_qa_list:\n\
      \                instruction = f\"Describe the purpose and significance of these\
      \ objects within the code: '{item}'\"\n                item_prompt = f'\\n###\
      \ Instruction:\\nUsing this context:\\n{context}\\n\\n{instruction}.\\n### Response:'\n\
      \                try:\n                    item_response = re.sub('\\\\n\\\\\
      s*\\\\n', '\\n\\n', self.llm(item_prompt))\n                    logging.info(f'\\\
      n***Itemized Response: {instruction}\\n{item_response}')\n                except\
      \ Exception as error:\n                    logging.error(f'Failed to generate\
      \ model response: {error}')\n                for i, instruct_item in enumerate(self.instruct_list):\n\
      \                    if instruct_item['instruction'].startswith(list(item.keys())[0]):\n\
      \                        instruct_item['output'] = list(item.keys())[0] + ':'\
      \ + list(item.values())[0] + '\\n\\n' + 'Purpose and Significance:\\n' + item_response\n\
      \                        break\n        return response\n\n    def process_question(self,\
      \ question_type: str, question_id: str, query: str, context: str, info: Dict)\
      \ -> None:\n        \"\"\"\n        Process question and add the generated response\
      \ to the instruct_list.\n        Args:\n            question_type (str): The\
      \ type of question to be processed.\n            question_id (str): The ID of\
      \ the question to be processed.\n            query (str): The query to be processed.\n\
      \            context (str): The context to be used for generating the response.`\n\
      \            info (Dict): The information of the Python file.\n        Returns:\n\
      \            None\n        \"\"\"\n        if question_id.endswith('code_graph')\
      \ or question_id.endswith('docstring'):\n            response = info.get(question_id,\
      \ {})\n        elif self.use_llm and question_id.endswith('purpose'):\n    \
      \        response = self.get_response_from_llm(query, context)\n        else:\n\
      \            response = clean_and_get_unique_elements(str(info.get(question_id,\
      \ '')))\n        if question_type == 'file':\n            context = '```python\\\
      n' + str(self.file_details['file_info']['file_code']) + '\\n```'\n        if\
      \ response and response != 'None':\n            response_str = str(response).strip()\n\
      \            if response_str:\n                self.instruct_list.append({'instruction':\
      \ query, 'input': context, 'output': response_str})\n\n    @staticmethod\n \
      \   def get_string_from_info(info, item_type):\n        \"\"\"\n        Get\
      \ string from info dictionary.\n        Args:\n            info (Dict): The\
      \ information of the Python file.\n            item_type (str): The type of\
      \ item to get the string from.\n        Returns:\n            str: The string\
      \ from the info.\n        \"\"\"\n        if info[item_type]:\n            items\
      \ = [item.strip() for item in str(info[item_type]).split(',') if item]\n   \
      \         return ', '.join(items)\n        return ''\n\n    def process_question_type(self,\
      \ question_type: str, question_id: str, question_text: str) -> None:\n     \
      \   \"\"\"\n        Process questions related to a file, function, class, or\
      \ method.\n        Args:\n            question_type (str): The type of question\
      \ to be processed.\n            question_id (str): The ID of the question to\
      \ be processed.\n            question_text (str): The text of the question to\
      \ be processed.\n        Returns:\n            None\n        \"\"\"\n      \
      \  if question_type == 'file':\n            query = question_text.format(filename=self.base_name)\n\
      \            info = self.file_details['file_info']\n            context = self.file_details['file_info']['file_code']\n\
      \            self.process_question(question_type, question_id, query, context,\
      \ info)\n        elif question_type == 'method':\n            for class_name,\
      \ class_info in self.file_details['classes'].items():\n                for key,\
      \ method_info in class_info.items():\n                    if key.startswith('class_method_'):\n\
      \                        method_name = key[len('class_method_'):]\n        \
      \                context = method_info['method_code']\n                    \
      \    mapping = {'class_name': class_name, 'method_name': method_name}\n    \
      \                    query = question_text.format(filename=self.base_name, **mapping)\n\
      \                        self.process_question(question_type, question_id, query,\
      \ context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n\
      \                context = info[f'{question_type}_code']\n                mapping\
      \ = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_purpose'\
      \ and self.use_llm:\n                    variables_string = self.get_string_from_info(info,\
      \ f'{question_type}_variables')\n                    inputs_string = self.get_string_from_info(info,\
      \ f'{question_type}_inputs')\n                    combined_string = ', '.join([s\
      \ for s in [variables_string, inputs_string] if s])\n                    mapping[f'{question_type}_variables']\
      \ = clean_and_get_unique_elements(combined_string)\n                    if question_type\
      \ == 'class':\n                        methods_string = self.get_string_from_info(info,\
      \ f'{question_type}_methods')\n                        mapping[f'{question_type}_methods']\
      \ = methods_string\n                query = question_text.format(filename=self.base_name,\
      \ **mapping)\n                self.process_question(question_type, question_id,\
      \ query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n\
      \        \"\"\"\n        Generate responses for all the questions and returns\
      \ the instruct_list.\n        Args:\n            None\n        Returns:\n  \
      \          Tuple[List[Dict], List[Dict]]: The generated question-answer pairs\
      \ and instructions.\n        \"\"\"\n        for question in self.questions:\n\
      \            self.process_question_type(question['type'], question['id'], question['text'])\n\
      \        return self.instruct_list"
    class_docstring: "\n    Generate JSON formatted dictionary outputs for a Python\
      \ file.\n    Attributes:\n        file_path (str): The path to the Python file.\n\
      \        file_details (Dict[str, Any]): Details of the Python file.\n      \
      \  base_name (str): The base name of the Python file.\n        questions (List[Dict[str,\
      \ str]]): Questions for generating responses.\n        instruct_list (List[Dict[str,\
      \ str]]): Storage for generated instructions.\n        question_mapping (Dict[str,\
      \ str]): Mapping of question types to keys in file details.\n        use_llm\
      \ (bool): Flag indicating if a language model should be used.\n        llm (object):\
      \ The language model for generating responses.\n        prompt (str): The prompt\
      \ format for querying the language model.\n    Methods:\n        add_to_list(list_to_update:\
      \ List[Dict], query: str, response: str,\n        additional_field=None) ->\
      \ List[Dict]:\n            Add response to the instruct list.\n        get_response_from_llm(query:\
      \ str, context: str) -> str:\n            Get language model response to query\
      \ for given context.\n        process_question(question_type: str, question_id:\
      \ str, query: str,\n        context: str, info: Dict) -> None:\n           \
      \ Process question and add generated response to the instruct_list.\n      \
      \  process_question_type(question_type: str, question_id: str,\n        question_text:\
      \ str) -> None:\n            Process question related to file, function, class,\
      \ or method.\n        generate() -> Tuple[List[Dict], List[Dict]]:\n       \
      \     Generate responses for all the questions and return the instruct_list.\n\
      \    "
    class_inputs: null
    class_defaults: null
    class_returns:
    - list_to_update
    - response
    - ''''''
    - self.instruct_list
    - (prompt, context_size)
    - ''', ''.join(items)'
    - ''''''
    class_calls:
    - list_to_update.append
    - self.model_config['prompt_template'].format
    - len
    - self.llm.tokenize
    - item['instruction'].split
    - any
    - item['instruction'].startswith
    - str
    - self.get_string_from_info
    - strategy
    - get_context_and_prompt
    - logging.error
    - math.ceil
    - re.sub
    - self.llm
    - code_elements_combined.update
    - json.dumps
    - group_json
    - json.loads
    - logging.info
    - enumerate
    - instruct_item['instruction'].startswith
    - list
    - item.keys
    - item.values
    - question_id.endswith
    - info.get
    - self.get_response_from_llm
    - clean_and_get_unique_elements
    - str(response).strip
    - self.instruct_list.append
    - item.strip
    - str(info[item_type]).split
    - ''', ''.join'
    - question_text.format
    - self.process_question
    - self.file_details['classes'].items
    - class_info.items
    - key.startswith
    - self.file_details[self.question_mapping[question_type]].items
    - self.process_question_type
    class_call_inputs:
      list_to_update.append:
      - '{''instruction'': query, ''input'': additional_field, ''output'': response}'
      self.model_config['prompt_template'].format: []
      len:
      - self.llm.tokenize(prompt)
      - '''class_method_'''
      self.llm.tokenize:
      - prompt
      item['instruction'].split:
      - ''' in Python file:'''
      any:
      - (item['instruction'].startswith(prefix) for prefix in excluded_instructions)
      item['instruction'].startswith:
      - prefix
      str:
      - context
      - self.file_details['file_info']['file_code_simplified']
      - info.get(question_id, '')
      - self.file_details['file_info']['file_code']
      - response
      - info[item_type]
      self.get_string_from_info:
      - self.file_details['file_info']
      - '''file_summary'''
      - info
      - f'{question_type}_variables'
      - info
      - f'{question_type}_inputs'
      - info
      - f'{question_type}_methods'
      strategy: []
      get_context_and_prompt:
      - query
      - context
      - code_qa_list
      logging.error:
      - f'Model response failed, increase py2dataset_model_config.yaml context_length
        > {math.ceil(context_size / 0.7)}'
      - 'f''Failed to generate model response: {error}'''
      - 'f''Failed to generate model response: {error}'''
      math.ceil:
      - context_size / 0.7
      re.sub:
      - '''\\n\\s*\\n'''
      - '''\n\n'''
      - self.llm(prompt)
      - '''\\n\\s*\\n'''
      - '''\n\n'''
      - self.llm(item_prompt)
      self.llm:
      - prompt
      - item_prompt
      code_elements_combined.update:
      - item
      json.dumps:
      - '{''Code Elements'': code_elements_combined}'
      - group_json(json.loads(code_elements_json))
      group_json:
      - json.loads(code_elements_json)
      json.loads:
      - code_elements_json
      logging.info:
      - 'f''***Overall Response: {response}'''
      - 'f''\n***Itemized Response: {instruction}\n{item_response}'''
      enumerate:
      - self.instruct_list
      instruct_item['instruction'].startswith:
      - list(item.keys())[0]
      list:
      - item.keys()
      - item.keys()
      - item.values()
      item.keys: []
      item.values: []
      question_id.endswith:
      - '''code_graph'''
      - '''docstring'''
      - '''purpose'''
      info.get:
      - question_id
      - '{}'
      - question_id
      - ''''''
      self.get_response_from_llm:
      - query
      - context
      clean_and_get_unique_elements:
      - str(info.get(question_id, ''))
      - combined_string
      str(response).strip: []
      self.instruct_list.append:
      - '{''instruction'': query, ''input'': context, ''output'': response_str}'
      item.strip: []
      str(info[item_type]).split:
      - ''','''
      ''', ''.join':
      - items
      - '[s for s in [variables_string, inputs_string] if s]'
      question_text.format: []
      self.process_question:
      - question_type
      - question_id
      - query
      - context
      - info
      - question_type
      - question_id
      - query
      - context
      - method_info
      - question_type
      - question_id
      - query
      - context
      - info
      self.file_details['classes'].items: []
      class_info.items: []
      key.startswith:
      - '''class_method_'''
      self.file_details[self.question_mapping[question_type]].items: []
      self.process_question_type:
      - question['type']
      - question['id']
      - question['text']
    class_variables:
    - max_context_length
    - context
    - info
    - response_str
    - code_qa_list
    - excluded_instructions
    - context_strategies
    - context_size
    - combined_string
    - query
    - mapping
    - instruction
    - inputs_string
    - code_elements_combined
    - method_name
    - prompt
    - full_context
    - code_elements_json
    - variables_string
    - methods_string
    - item_prompt
    - response
    - item_response
    - items
    class_decorators: []
    class_annotations: []
    class_properties:
    - self.base_name
    - self.questions
    - self.question_mapping
    - self.model_config
    - self.instruct_list
    - self.file_path
    - self.file_details
    - self.llm
    - self.use_llm
    - self.detailed
    class_attributes:
    - file_path
    - file_details
    - base_name
    - questions
    - model_config
    - instruct_list
    - question_mapping
    - llm
    - use_llm
    - detailed
    - llm
    - use_llm
    - detailed
    class_methods:
    - add_to_list
    - get_response_from_llm
    - process_question
    - get_string_from_info
    - process_question_type
    - generate
    class_inheritance: []
    class_static_methods:
    - get_string_from_info
    class_method___init__:
      method_name: __init__
      method_code: "def __init__(self, file_path: str, file_details: Dict, base_name:\
        \ str, questions: List[Dict], model_config: Dict, detailed: bool) -> None:\n\
        \    \"\"\"\n        Initialize the DatasetGenerator class.\n        Args:\n\
        \            file_path (str): The path to the Python file.\n            file_details\
        \ (Dict[str, Any]): Details of the Python file.\n            base_name (str):\
        \ The base name of the Python file.\n            questions (List[Dict[str,\
        \ str]]): Questions for generating responses.\n            model_config (Dict):\
        \ Configuration for the language model.\n        Returns:\n            None\n\
        \        \"\"\"\n    self.file_path = file_path\n    self.file_details = file_details\n\
        \    self.base_name = base_name\n    self.questions = questions\n    self.model_config\
        \ = model_config\n    if model_config is not None:\n        self.llm = model_config['model']\n\
        \        self.use_llm = True\n        self.detailed = detailed\n    else:\n\
        \        self.llm = None\n        self.use_llm = False\n        self.detailed\
        \ = False\n    self.instruct_list = []\n    self.question_mapping = {'file':\
        \ 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}"
      method_docstring: "\n        Initialize the DatasetGenerator class.\n      \
        \  Args:\n            file_path (str): The path to the Python file.\n    \
        \        file_details (Dict[str, Any]): Details of the Python file.\n    \
        \        base_name (str): The base name of the Python file.\n            questions\
        \ (List[Dict[str, str]]): Questions for generating responses.\n          \
        \  model_config (Dict): Configuration for the language model.\n        Returns:\n\
        \            None\n        "
      method_inputs:
      - self
      - file_path
      - file_details
      - base_name
      - questions
      - model_config
      - detailed
      method_defaults: []
      method_returns: []
      method_calls: []
      method_call_inputs: {}
      method_variables: []
      method_decorators: []
      method_annotations: []
      method_properties:
      - self.base_name
      - self.questions
      - self.question_mapping
      - self.model_config
      - self.instruct_list
      - self.file_path
      - self.file_details
      - self.llm
      - self.use_llm
      - self.detailed
    class_method_add_to_list:
      method_name: add_to_list
      method_code: "def add_to_list(self, list_to_update: List[Dict], query: str,\
        \ response: str, additional_field=None) -> List[Dict]:\n    \"\"\"\n     \
        \   Add response to the instruct list.\n        Args:\n            list_to_update\
        \ (List[Dict]): The list to update.\n            query (str): The query to\
        \ be added.\n            response (str): The response to be added.\n     \
        \       additional_field (Any): Additional field to be added.\n        Returns:\n\
        \            List[Dict]: The updated list.\n        \"\"\"\n    list_to_update.append({'instruction':\
        \ query, 'input': additional_field, 'output': response})\n    return list_to_update"
      method_docstring: "\n        Add response to the instruct list.\n        Args:\n\
        \            list_to_update (List[Dict]): The list to update.\n          \
        \  query (str): The query to be added.\n            response (str): The response\
        \ to be added.\n            additional_field (Any): Additional field to be\
        \ added.\n        Returns:\n            List[Dict]: The updated list.\n  \
        \      "
      method_inputs:
      - self
      - list_to_update
      - query
      - response
      - additional_field
      method_defaults:
      - None
      method_returns:
      - list_to_update
      method_calls:
      - list_to_update.append
      method_call_inputs:
        list_to_update.append:
        - '{''instruction'': query, ''input'': additional_field, ''output'': response}'
      method_variables: []
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_get_response_from_llm:
      method_name: get_response_from_llm
      method_code: "def get_response_from_llm(self, query: str, context: str) -> str:\n\
        \    \"\"\"\n        Get language model response to query for given context.\n\
        \        Args:\n            query (str): The query to be used for generating\
        \ the response.\n            context (str): The context to be used for generating\
        \ the response.\n        Returns:\n            str: The generated response.\n\
        \        \"\"\"\n\n    def get_context_and_prompt(query, context, code_qa_list):\n\
        \        full_context = f'{context}\\nCODE Q and A:\\n{code_qa_list}'\n  \
        \      prompt = self.model_config['prompt_template'].format(context=full_context,\
        \ query=query)\n        context_size = len(self.llm.tokenize(prompt))\n  \
        \      return (prompt, context_size)\n    excluded_instructions = ['Call code\
        \ graph', 'Docstring']\n    code_qa_list = [{item['instruction'].split(' in\
        \ Python file:')[0]: item['output']} for item in self.instruct_list if not\
        \ any((item['instruction'].startswith(prefix) for prefix in excluded_instructions))]\n\
        \    context_strategies = [lambda: '```python\\n' + str(context) + '\\n```',\
        \ lambda: '```python\\n' + str(self.file_details['file_info']['file_code_simplified'])\
        \ + '\\n```', lambda: self.get_string_from_info(self.file_details['file_info'],\
        \ 'file_summary'), lambda: '']\n    max_context_length = self.model_config['inference_model']['model_params']['context_length']\n\
        \    for strategy in context_strategies:\n        context = strategy()\n \
        \       prompt, context_size = get_context_and_prompt(query, context, code_qa_list)\n\
        \        if context_size <= 0.7 * max_context_length:\n            break\n\
        \        else:\n            logging.error(f'Model response failed, increase\
        \ py2dataset_model_config.yaml context_length > {math.ceil(context_size /\
        \ 0.7)}')\n            return ''\n    response = ''\n    try:\n        response\
        \ = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(prompt))\n        code_elements_combined\
        \ = {}\n        for item in code_qa_list:\n            code_elements_combined.update(item)\n\
        \        code_elements_json = json.dumps({'Code Elements': code_elements_combined},\
        \ indent=4)\n        code_elements_json = json.dumps(group_json(json.loads(code_elements_json)),\
        \ indent=4)\n        response += '\\n' + code_elements_json\n        logging.info(f'***Overall\
        \ Response: {response}')\n    except Exception as error:\n        logging.error(f'Failed\
        \ to generate model response: {error}')\n    if self.detailed:\n        for\
        \ item in code_qa_list:\n            instruction = f\"Describe the purpose\
        \ and significance of these objects within the code: '{item}'\"\n        \
        \    item_prompt = f'\\n### Instruction:\\nUsing this context:\\n{context}\\\
        n\\n{instruction}.\\n### Response:'\n            try:\n                item_response\
        \ = re.sub('\\\\n\\\\s*\\\\n', '\\n\\n', self.llm(item_prompt))\n        \
        \        logging.info(f'\\n***Itemized Response: {instruction}\\n{item_response}')\n\
        \            except Exception as error:\n                logging.error(f'Failed\
        \ to generate model response: {error}')\n            for i, instruct_item\
        \ in enumerate(self.instruct_list):\n                if instruct_item['instruction'].startswith(list(item.keys())[0]):\n\
        \                    instruct_item['output'] = list(item.keys())[0] + ':'\
        \ + list(item.values())[0] + '\\n\\n' + 'Purpose and Significance:\\n' + item_response\n\
        \                    break\n    return response"
      method_docstring: "\n        Get language model response to query for given\
        \ context.\n        Args:\n            query (str): The query to be used for\
        \ generating the response.\n            context (str): The context to be used\
        \ for generating the response.\n        Returns:\n            str: The generated\
        \ response.\n        "
      method_inputs:
      - self
      - query
      - context
      method_defaults: []
      method_returns:
      - response
      - (prompt, context_size)
      - ''''''
      method_calls:
      - self.model_config['prompt_template'].format
      - len
      - self.llm.tokenize
      - item['instruction'].split
      - any
      - item['instruction'].startswith
      - str
      - self.get_string_from_info
      - strategy
      - get_context_and_prompt
      - logging.error
      - math.ceil
      - re.sub
      - self.llm
      - code_elements_combined.update
      - json.dumps
      - group_json
      - json.loads
      - logging.info
      - enumerate
      - instruct_item['instruction'].startswith
      - list
      - item.keys
      - item.values
      method_call_inputs:
        self.model_config['prompt_template'].format: []
        len:
        - self.llm.tokenize(prompt)
        self.llm.tokenize:
        - prompt
        item['instruction'].split:
        - ''' in Python file:'''
        any:
        - (item['instruction'].startswith(prefix) for prefix in excluded_instructions)
        item['instruction'].startswith:
        - prefix
        str:
        - context
        - self.file_details['file_info']['file_code_simplified']
        self.get_string_from_info:
        - self.file_details['file_info']
        - '''file_summary'''
        strategy: []
        get_context_and_prompt:
        - query
        - context
        - code_qa_list
        logging.error:
        - f'Model response failed, increase py2dataset_model_config.yaml context_length
          > {math.ceil(context_size / 0.7)}'
        - 'f''Failed to generate model response: {error}'''
        - 'f''Failed to generate model response: {error}'''
        math.ceil:
        - context_size / 0.7
        re.sub:
        - '''\\n\\s*\\n'''
        - '''\n\n'''
        - self.llm(prompt)
        - '''\\n\\s*\\n'''
        - '''\n\n'''
        - self.llm(item_prompt)
        self.llm:
        - prompt
        - item_prompt
        code_elements_combined.update:
        - item
        json.dumps:
        - '{''Code Elements'': code_elements_combined}'
        - group_json(json.loads(code_elements_json))
        group_json:
        - json.loads(code_elements_json)
        json.loads:
        - code_elements_json
        logging.info:
        - 'f''***Overall Response: {response}'''
        - 'f''\n***Itemized Response: {instruction}\n{item_response}'''
        enumerate:
        - self.instruct_list
        instruct_item['instruction'].startswith:
        - list(item.keys())[0]
        list:
        - item.keys()
        - item.keys()
        - item.values()
        item.keys: []
        item.values: []
      method_variables:
      - max_context_length
      - prompt
      - full_context
      - code_elements_json
      - instruction
      - item_prompt
      - context
      - code_qa_list
      - response
      - item_response
      - excluded_instructions
      - code_elements_combined
      - context_strategies
      - context_size
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_get_context_and_prompt:
      method_name: get_context_and_prompt
      method_code: "def get_context_and_prompt(query, context, code_qa_list):\n  \
        \  full_context = f'{context}\\nCODE Q and A:\\n{code_qa_list}'\n    prompt\
        \ = self.model_config['prompt_template'].format(context=full_context, query=query)\n\
        \    context_size = len(self.llm.tokenize(prompt))\n    return (prompt, context_size)"
      method_docstring: null
      method_inputs:
      - query
      - context
      - code_qa_list
      method_defaults: []
      method_returns:
      - (prompt, context_size)
      method_calls:
      - self.model_config['prompt_template'].format
      - len
      - self.llm.tokenize
      method_call_inputs:
        self.model_config['prompt_template'].format: []
        len:
        - self.llm.tokenize(prompt)
        self.llm.tokenize:
        - prompt
      method_variables:
      - context_size
      - prompt
      - full_context
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_process_question:
      method_name: process_question
      method_code: "def process_question(self, question_type: str, question_id: str,\
        \ query: str, context: str, info: Dict) -> None:\n    \"\"\"\n        Process\
        \ question and add the generated response to the instruct_list.\n        Args:\n\
        \            question_type (str): The type of question to be processed.\n\
        \            question_id (str): The ID of the question to be processed.\n\
        \            query (str): The query to be processed.\n            context\
        \ (str): The context to be used for generating the response.`\n          \
        \  info (Dict): The information of the Python file.\n        Returns:\n  \
        \          None\n        \"\"\"\n    if question_id.endswith('code_graph')\
        \ or question_id.endswith('docstring'):\n        response = info.get(question_id,\
        \ {})\n    elif self.use_llm and question_id.endswith('purpose'):\n      \
        \  response = self.get_response_from_llm(query, context)\n    else:\n    \
        \    response = clean_and_get_unique_elements(str(info.get(question_id, '')))\n\
        \    if question_type == 'file':\n        context = '```python\\n' + str(self.file_details['file_info']['file_code'])\
        \ + '\\n```'\n    if response and response != 'None':\n        response_str\
        \ = str(response).strip()\n        if response_str:\n            self.instruct_list.append({'instruction':\
        \ query, 'input': context, 'output': response_str})"
      method_docstring: "\n        Process question and add the generated response\
        \ to the instruct_list.\n        Args:\n            question_type (str): The\
        \ type of question to be processed.\n            question_id (str): The ID\
        \ of the question to be processed.\n            query (str): The query to\
        \ be processed.\n            context (str): The context to be used for generating\
        \ the response.`\n            info (Dict): The information of the Python file.\n\
        \        Returns:\n            None\n        "
      method_inputs:
      - self
      - question_type
      - question_id
      - query
      - context
      - info
      method_defaults: []
      method_returns: []
      method_calls:
      - question_id.endswith
      - info.get
      - self.get_response_from_llm
      - clean_and_get_unique_elements
      - str
      - str(response).strip
      - self.instruct_list.append
      method_call_inputs:
        question_id.endswith:
        - '''code_graph'''
        - '''docstring'''
        - '''purpose'''
        info.get:
        - question_id
        - '{}'
        - question_id
        - ''''''
        self.get_response_from_llm:
        - query
        - context
        clean_and_get_unique_elements:
        - str(info.get(question_id, ''))
        str:
        - info.get(question_id, '')
        - self.file_details['file_info']['file_code']
        - response
        str(response).strip: []
        self.instruct_list.append:
        - '{''instruction'': query, ''input'': context, ''output'': response_str}'
      method_variables:
      - context
      - response
      - response_str
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_get_string_from_info:
      method_name: get_string_from_info
      method_code: "@staticmethod\ndef get_string_from_info(info, item_type):\n  \
        \  \"\"\"\n        Get string from info dictionary.\n        Args:\n     \
        \       info (Dict): The information of the Python file.\n            item_type\
        \ (str): The type of item to get the string from.\n        Returns:\n    \
        \        str: The string from the info.\n        \"\"\"\n    if info[item_type]:\n\
        \        items = [item.strip() for item in str(info[item_type]).split(',')\
        \ if item]\n        return ', '.join(items)\n    return ''"
      method_docstring: "\n        Get string from info dictionary.\n        Args:\n\
        \            info (Dict): The information of the Python file.\n          \
        \  item_type (str): The type of item to get the string from.\n        Returns:\n\
        \            str: The string from the info.\n        "
      method_inputs:
      - info
      - item_type
      method_defaults: []
      method_returns:
      - ''''''
      - ''', ''.join(items)'
      method_calls:
      - item.strip
      - str(info[item_type]).split
      - str
      - ''', ''.join'
      method_call_inputs:
        item.strip: []
        str(info[item_type]).split:
        - ''','''
        str:
        - info[item_type]
        ''', ''.join':
        - items
      method_variables:
      - items
      method_decorators:
      - staticmethod
      method_annotations: []
      method_properties: []
    class_method_process_question_type:
      method_name: process_question_type
      method_code: "def process_question_type(self, question_type: str, question_id:\
        \ str, question_text: str) -> None:\n    \"\"\"\n        Process questions\
        \ related to a file, function, class, or method.\n        Args:\n        \
        \    question_type (str): The type of question to be processed.\n        \
        \    question_id (str): The ID of the question to be processed.\n        \
        \    question_text (str): The text of the question to be processed.\n    \
        \    Returns:\n            None\n        \"\"\"\n    if question_type == 'file':\n\
        \        query = question_text.format(filename=self.base_name)\n        info\
        \ = self.file_details['file_info']\n        context = self.file_details['file_info']['file_code']\n\
        \        self.process_question(question_type, question_id, query, context,\
        \ info)\n    elif question_type == 'method':\n        for class_name, class_info\
        \ in self.file_details['classes'].items():\n            for key, method_info\
        \ in class_info.items():\n                if key.startswith('class_method_'):\n\
        \                    method_name = key[len('class_method_'):]\n          \
        \          context = method_info['method_code']\n                    mapping\
        \ = {'class_name': class_name, 'method_name': method_name}\n             \
        \       query = question_text.format(filename=self.base_name, **mapping)\n\
        \                    self.process_question(question_type, question_id, query,\
        \ context, method_info)\n    else:\n        for name, info in self.file_details[self.question_mapping[question_type]].items():\n\
        \            context = info[f'{question_type}_code']\n            mapping\
        \ = {f'{question_type}_name': name}\n            if question_id == f'{question_type}_purpose'\
        \ and self.use_llm:\n                variables_string = self.get_string_from_info(info,\
        \ f'{question_type}_variables')\n                inputs_string = self.get_string_from_info(info,\
        \ f'{question_type}_inputs')\n                combined_string = ', '.join([s\
        \ for s in [variables_string, inputs_string] if s])\n                mapping[f'{question_type}_variables']\
        \ = clean_and_get_unique_elements(combined_string)\n                if question_type\
        \ == 'class':\n                    methods_string = self.get_string_from_info(info,\
        \ f'{question_type}_methods')\n                    mapping[f'{question_type}_methods']\
        \ = methods_string\n            query = question_text.format(filename=self.base_name,\
        \ **mapping)\n            self.process_question(question_type, question_id,\
        \ query, context, info)"
      method_docstring: "\n        Process questions related to a file, function,\
        \ class, or method.\n        Args:\n            question_type (str): The type\
        \ of question to be processed.\n            question_id (str): The ID of the\
        \ question to be processed.\n            question_text (str): The text of\
        \ the question to be processed.\n        Returns:\n            None\n    \
        \    "
      method_inputs:
      - self
      - question_type
      - question_id
      - question_text
      method_defaults: []
      method_returns: []
      method_calls:
      - question_text.format
      - self.process_question
      - self.file_details['classes'].items
      - class_info.items
      - key.startswith
      - len
      - self.file_details[self.question_mapping[question_type]].items
      - self.get_string_from_info
      - ''', ''.join'
      - clean_and_get_unique_elements
      method_call_inputs:
        question_text.format: []
        self.process_question:
        - question_type
        - question_id
        - query
        - context
        - info
        - question_type
        - question_id
        - query
        - context
        - method_info
        - question_type
        - question_id
        - query
        - context
        - info
        self.file_details['classes'].items: []
        class_info.items: []
        key.startswith:
        - '''class_method_'''
        len:
        - '''class_method_'''
        self.file_details[self.question_mapping[question_type]].items: []
        self.get_string_from_info:
        - info
        - f'{question_type}_variables'
        - info
        - f'{question_type}_inputs'
        - info
        - f'{question_type}_methods'
        ''', ''.join':
        - '[s for s in [variables_string, inputs_string] if s]'
        clean_and_get_unique_elements:
        - combined_string
      method_variables:
      - combined_string
      - query
      - mapping
      - variables_string
      - methods_string
      - context
      - info
      - inputs_string
      - method_name
      method_decorators: []
      method_annotations: []
      method_properties: []
    class_method_generate:
      method_name: generate
      method_code: "def generate(self) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\
        \"\n        Generate responses for all the questions and returns the instruct_list.\n\
        \        Args:\n            None\n        Returns:\n            Tuple[List[Dict],\
        \ List[Dict]]: The generated question-answer pairs and instructions.\n   \
        \     \"\"\"\n    for question in self.questions:\n        self.process_question_type(question['type'],\
        \ question['id'], question['text'])\n    return self.instruct_list"
      method_docstring: "\n        Generate responses for all the questions and returns\
        \ the instruct_list.\n        Args:\n            None\n        Returns:\n\
        \            Tuple[List[Dict], List[Dict]]: The generated question-answer\
        \ pairs and instructions.\n        "
      method_inputs:
      - self
      method_defaults: []
      method_returns:
      - self.instruct_list
      method_calls:
      - self.process_question_type
      method_call_inputs:
        self.process_question_type:
        - question['type']
        - question['id']
        - question['text']
      method_variables: []
      method_decorators: []
      method_annotations: []
      method_properties: []
