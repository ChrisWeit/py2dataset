[
    {
        "instruction": "Dependencies in Python file: `py2dataset.get_params.py`?",
        "input": "```python\n\"\"\"\nObtain data parameter and model from the py2dataset functions.\nRequirements:\n[req01] The `get_default_questions` function shall:\n        a. Return a list of default questions.\n        b. Ensure each question in the list is a dictionary.\n        c. Ensure each dictionary has the keys: id, text, and type.\n[req02] The `get_default_model_config` function shall:\n        a. Return a dictionary representing the default model configuration.\n[req03] The `get_output_dir` function shall:\n        a. Accept an optional output_dir argument.\n        b. Return the absolute path of the provided output_dir if it exists or can be created.\n        c. Return the default OUTPUT_DIR if the provided output_dir argument is not provided or invalid.\n[req04] The `get_questions` function shall:\n        a. Accept an optional questions_pathname argument.\n        b. Validate the question file provided by the questions_pathname.\n        c. Return the questions from the provided questions_pathname if valid.\n        d. Return default questions if the questions_pathname is not provided or invalid.\n[req05] The `instantiate_model` function shall:\n        a. Accept a model_config dictionary as an argument.\n        b. Import the specified module and class from the model_config.\n        c. Instantiate and return the model using the provided configuration.\n[req06] The `get_model` function shall:\n        a. Accept an optional model_config_pathname argument.\n        b. Validate the model config file provided by the model_config_pathname.\n        c. Return an instantiated model and a prompt template based on the provided configuration.\n        d. Return an instantiated model and a prompt template based on the default model configuration if the model_config_pathname is not provided or invalid.\n[req07] The `write_questions_file` function shall:\n        a. Accept an optional output_dir argument.\n        b. Write the default questions to the QUESTIONS_FILE in the specified directory.\n        c. Write the default questions to the QUESTIONS_FILE in the current working directory if the output_dir argument is not provided or invalid.\n[req08] The `write_model_config_file` function shall:\n        a. Accept an optional output_dir argument.\n        b. Write the default model configuration to the MODEL_CONFIG_FILE in the specified directory.\n        c. Write the default model configuration to the MODEL_CONFIG_FILE in the current working directory if the output_dir argument is not provided or invalid.\n\"\"\"\nimport os\nimport json\nimport logging\nimport importlib\nfrom typing import Dict, List\nfrom pathlib import Path\nimport yaml\n\n# Setting up a basic logger\nlogging.basicConfig(level=logging.INFO)\n\nQUESTIONS_FILE = \"py2dataset_questions.json\"\nMODEL_CONFIG_FILE = \"py2dataset_model_config.yaml\"\nOUTPUT_DIR = \"datasets\"\n\n\ndef get_default_questions() -> List[Dict]:\n    \"\"\"Return default question list\n    Args:\n        None\n    Returns:\n        List[Dict]: The default question list\n    \"\"\"\n    questions = [\n        {\n            \"id\": \"file_dependencies\",\n            \"text\": \"Dependencies of Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"entire_code_graph\",\n            \"text\": \"Call code graph of Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_functions\",\n            \"text\": \"Functions defined in Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_classes\",\n            \"text\": \"Classes defined in Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"function_inputs\",\n            \"text\": \"Inputs to function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_docstring\",\n            \"text\": \"Docstring of function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_calls\",\n            \"text\": \"Calls made in function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_variables\",\n            \"text\": \"Variables defined in function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_returns\",\n            \"text\": \"Returns from function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"class_methods\",\n            \"text\": \"Methods defined in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_docstring\",\n            \"text\": \"Docstring of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_attributes\",\n            \"text\": \"Attributes of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_variables\",\n            \"text\": \"Variables defined in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_inheritance\",\n            \"text\": \"Inheritance of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"method_inputs\",\n            \"text\": \"Inputs to method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_docstring\",\n            \"text\": \"Docstring of method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_calls\",\n            \"text\": \"Calls made in method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_returns\",\n            \"text\": \"Returns from method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"file_purpose\",\n            \"text\": \"1) Describe the Purpose and Processing Summary of Python file: '{filename}'; 2) Provide an itemized and detailed description of each applicable function, class, and method; 3) Explain what each input, output, and variable does in the code.\",\n            \"type\": \"file\",\n        },\n    ]\n    return questions\n\n\ndef get_default_model_config() -> Dict:\n    \"\"\"Return default model config dict\n    Args:\n        None\n    Returns:\n        Dict: The default model config dictionary\n    \"\"\"\n    model_config = {\n        \"prompt_template\": \"You are an AI coding instructor. Given this Context:\\n'{context}'\\n\\nPlease provide a very detailed, accurate, and insightful Response to this Instruction and include your reasoning step by step. \\n### Instruction:\\n{query}\\n### Response:\",\n        \"inference_model\": {\n            \"model_import_path\": \"ctransformers.AutoModelForCausalLM\",\n            \"model_inference_function\": \"from_pretrained\",\n            \"model_params\": {\n                \"model_path\": \"TheBloke/WizardCoder-Python-13B-V1.0-GGUF\",\n                \"model_type\": \"llama\",\n                \"local_files_only\": False,\n                ## MODEL CONFIGURATION PARAMETERS (set for current model with: GPU 4090-24GB VRAM, CPU 5950x-32 threads, 64GB RAM) \n                # avx2 and gpu_layers are not compatible\n                # \"lib\": \"avx2\",\n                \"threads\": 16,\n                \"batch_size\": 128,\n                \"context_length\": 12000,\n                \"max_new_tokens\": 12000,\n                \"gpu_layers\": 100,\n                \"reset\": True,\n            },\n        },\n    }\n    return model_config\n\n\ndef get_start_dir(start_dir: str = \"\") -> str:\n    \"\"\"\n    Returns the appropriate start directory.\n    Args:\n        start_dir (str): The directory to start the search from.\n    Returns:\n        str: The absolute path of the provided start_dir if it exists or can be created.\n    \"\"\"\n    if start_dir and not Path(start_dir).is_dir():\n        logging.info(f\"Setting Start Dir : {start_dir}\")\n        start_dir = os.getcwd()\n    else:\n        start_dir = os.path.abspath(start_dir)\n    return start_dir\n\n\ndef get_output_dir(output_dir: str = \"\") -> str:\n    \"\"\"Returns the appropriate output directory.\n    Args:\n        output_dir (str): The directory to write the output to.\n    Returns:\n        str: The absolute path of the provided output_dir if it exists or can be created.\n    \"\"\"\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f\"Using output directory: {output_dir}\")\n    return output_dir\n\n\ndef get_questions(questions_pathname: str) -> List[Dict]:\n    \"\"\"\n    Get questions from file or default\n    Args:\n        questions_pathname (str): The pathname of the questions file\n    Returns:\n        List[Dict]: The list of questions\n    \"\"\"\n    try:  # get questions from provided or default configuration file\n        if not questions_pathname:\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\n        with open(questions_pathname, \"r\") as f:\n            questions = json.load(f)\n        logging.info(f\"Using questions from file: {questions_pathname}\")\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\n        logging.info(\n            f\"Questions file not valid: {questions_pathname} Using default questions\"\n        )\n        questions = get_default_questions()\n    return questions\n\n\ndef instantiate_model(model_config: Dict) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): model configuration dictionary.\n    Returns:\n        object: An instance of the specified model class, or None if error.\n    \"\"\"\n    try:\n        module_name, class_name = model_config[\"model_import_path\"].rsplit(\".\", 1)\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\n        model_params = model_config[\"model_params\"]\n        inference_function_name = model_config[\"model_inference_function\"]\n        if inference_function_name != \"\":\n            inference_function = getattr(ModelClass, inference_function_name)\n            llm = inference_function(model_params.pop(\"model_path\"), **model_params)\n        else:\n            llm = ModelClass(model_params.pop(\"model_path\"), **model_params)\n        return llm\n    except (ImportError, AttributeError, Exception) as e:\n        logging.info(f\"Failed to instantiate the model. Error: {e}\")\n        return None\n\n\ndef get_model(model_config_pathname: str) -> tuple[object, str]:\n    \"\"\"\n    Returns an instantiated model and prompt template based on the model configuration.\n    Agrs:\n        model_config_pathname (str): The pathname of the model config file\n    Returns:\n        Tuple[object, str]: The instantiated model and prompt template\n    \"\"\"\n    try:\n        if not model_config_pathname:\n            model_config_pathname = os.path.join(os.getcwd(), MODEL_CONFIG_FILE)\n        with open(model_config_pathname, \"r\") as config_file:\n            model_config = yaml.safe_load(config_file)\n        logging.info(f\"Using model config from file: {model_config_pathname}\")\n    except (FileNotFoundError, yaml.YAMLError):\n        logging.info(\n            f\"Model config file not valid: {model_config_pathname} Using default model config\"\n        )\n        model_config = get_default_model_config()\n    model_config[\"model\"] = instantiate_model(model_config[\"inference_model\"])\n    return model_config\n\n\ndef write_questions_file(output_dir: str = \"\") -> None:\n    \"\"\"\n    Writes the default questions to a file in JSON format.\n    Args:\n        output_dir (str): The directory to write the questions file to.\n    Returns:\n        None\n    \"\"\"\n    questions = get_default_questions()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, QUESTIONS_FILE), \"w\") as file:\n        json.dump(questions, file, indent=4)\n\n\ndef write_model_config_file(output_dir: str = \"\") -> None:\n    \"\"\"\n    Writes the default model config to a file in YAML format.\n    Args:\n        output_dir (str): The directory to write the model config file to.\n    Returns:\n        None\n    \"\"\"\n    model_config = get_default_model_config()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, MODEL_CONFIG_FILE), \"w\") as file:\n        yaml.dump(model_config, file)\n\n```",
        "output": "Dependencies:importlib, typing, yaml, os, logging, pathlib, json\n\nPurpose and Significance:\nThe `get_default_questions` function returns a list of default questions with the required keys \"id\", \"text\", and \"type\". It also defines some Python code for validating each dictionary in the list.\r\n\nThe `get_default_model_config` function returns a dictionary representing the default model configuration. The prompt template is defined here, which will be used to generate the prompts for the AI language model.\r\n\nThe `get_output_dir` function takes an optional output directory argument and returns the absolute path of that directory if it exists or can be created. If no valid output directory is provided, it uses the default OUTPUT_DIR value as specified in the code.\r\n\nThe `get_questions` function takes an optional questions_pathname argument to specify a JSON file containing customized questions. If no such pathname is provided or if the file is not found/invalid, it returns the default questions from `get_default_questions`.\r\n\nThe `instantiate_model` function accepts a model configuration dictionary and imports the specified module and class to create an instance of that model.\r\n\nThe `get_model` function accepts an optional `model_config_pathname` argument and loads YAML content from the specified file if provided, else uses default parameters.\r\nIt returns an instantiated model based on the configuration along with a prompt template. If any error occurs during importing or creating the model instance, it returns None.\r\n\nThe `write_questions_file` function writes the default questions to a JSON file in the specified directory if provided, else uses the current working directory.\r\n\nThe `write_model_config_file` function writes the default model configuration YAML content to the provided directory, else it is saved to the OUTPUT_DIR folder of the codebase. "
    },
    {
        "instruction": "Call code graph in Python file: `py2dataset.get_params.py`?",
        "input": "```python\n\"\"\"\nObtain data parameter and model from the py2dataset functions.\nRequirements:\n[req01] The `get_default_questions` function shall:\n        a. Return a list of default questions.\n        b. Ensure each question in the list is a dictionary.\n        c. Ensure each dictionary has the keys: id, text, and type.\n[req02] The `get_default_model_config` function shall:\n        a. Return a dictionary representing the default model configuration.\n[req03] The `get_output_dir` function shall:\n        a. Accept an optional output_dir argument.\n        b. Return the absolute path of the provided output_dir if it exists or can be created.\n        c. Return the default OUTPUT_DIR if the provided output_dir argument is not provided or invalid.\n[req04] The `get_questions` function shall:\n        a. Accept an optional questions_pathname argument.\n        b. Validate the question file provided by the questions_pathname.\n        c. Return the questions from the provided questions_pathname if valid.\n        d. Return default questions if the questions_pathname is not provided or invalid.\n[req05] The `instantiate_model` function shall:\n        a. Accept a model_config dictionary as an argument.\n        b. Import the specified module and class from the model_config.\n        c. Instantiate and return the model using the provided configuration.\n[req06] The `get_model` function shall:\n        a. Accept an optional model_config_pathname argument.\n        b. Validate the model config file provided by the model_config_pathname.\n        c. Return an instantiated model and a prompt template based on the provided configuration.\n        d. Return an instantiated model and a prompt template based on the default model configuration if the model_config_pathname is not provided or invalid.\n[req07] The `write_questions_file` function shall:\n        a. Accept an optional output_dir argument.\n        b. Write the default questions to the QUESTIONS_FILE in the specified directory.\n        c. Write the default questions to the QUESTIONS_FILE in the current working directory if the output_dir argument is not provided or invalid.\n[req08] The `write_model_config_file` function shall:\n        a. Accept an optional output_dir argument.\n        b. Write the default model configuration to the MODEL_CONFIG_FILE in the specified directory.\n        c. Write the default model configuration to the MODEL_CONFIG_FILE in the current working directory if the output_dir argument is not provided or invalid.\n\"\"\"\nimport os\nimport json\nimport logging\nimport importlib\nfrom typing import Dict, List\nfrom pathlib import Path\nimport yaml\n\n# Setting up a basic logger\nlogging.basicConfig(level=logging.INFO)\n\nQUESTIONS_FILE = \"py2dataset_questions.json\"\nMODEL_CONFIG_FILE = \"py2dataset_model_config.yaml\"\nOUTPUT_DIR = \"datasets\"\n\n\ndef get_default_questions() -> List[Dict]:\n    \"\"\"Return default question list\n    Args:\n        None\n    Returns:\n        List[Dict]: The default question list\n    \"\"\"\n    questions = [\n        {\n            \"id\": \"file_dependencies\",\n            \"text\": \"Dependencies of Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"entire_code_graph\",\n            \"text\": \"Call code graph of Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_functions\",\n            \"text\": \"Functions defined in Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_classes\",\n            \"text\": \"Classes defined in Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"function_inputs\",\n            \"text\": \"Inputs to function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_docstring\",\n            \"text\": \"Docstring of function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_calls\",\n            \"text\": \"Calls made in function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_variables\",\n            \"text\": \"Variables defined in function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_returns\",\n            \"text\": \"Returns from function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"class_methods\",\n            \"text\": \"Methods defined in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_docstring\",\n            \"text\": \"Docstring of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_attributes\",\n            \"text\": \"Attributes of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_variables\",\n            \"text\": \"Variables defined in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_inheritance\",\n            \"text\": \"Inheritance of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"method_inputs\",\n            \"text\": \"Inputs to method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_docstring\",\n            \"text\": \"Docstring of method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_calls\",\n            \"text\": \"Calls made in method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_returns\",\n            \"text\": \"Returns from method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"file_purpose\",\n            \"text\": \"1) Describe the Purpose and Processing Summary of Python file: '{filename}'; 2) Provide an itemized and detailed description of each applicable function, class, and method; 3) Explain what each input, output, and variable does in the code.\",\n            \"type\": \"file\",\n        },\n    ]\n    return questions\n\n\ndef get_default_model_config() -> Dict:\n    \"\"\"Return default model config dict\n    Args:\n        None\n    Returns:\n        Dict: The default model config dictionary\n    \"\"\"\n    model_config = {\n        \"prompt_template\": \"You are an AI coding instructor. Given this Context:\\n'{context}'\\n\\nPlease provide a very detailed, accurate, and insightful Response to this Instruction and include your reasoning step by step. \\n### Instruction:\\n{query}\\n### Response:\",\n        \"inference_model\": {\n            \"model_import_path\": \"ctransformers.AutoModelForCausalLM\",\n            \"model_inference_function\": \"from_pretrained\",\n            \"model_params\": {\n                \"model_path\": \"TheBloke/WizardCoder-Python-13B-V1.0-GGUF\",\n                \"model_type\": \"llama\",\n                \"local_files_only\": False,\n                ## MODEL CONFIGURATION PARAMETERS (set for current model with: GPU 4090-24GB VRAM, CPU 5950x-32 threads, 64GB RAM) \n                # avx2 and gpu_layers are not compatible\n                # \"lib\": \"avx2\",\n                \"threads\": 16,\n                \"batch_size\": 128,\n                \"context_length\": 12000,\n                \"max_new_tokens\": 12000,\n                \"gpu_layers\": 100,\n                \"reset\": True,\n            },\n        },\n    }\n    return model_config\n\n\ndef get_start_dir(start_dir: str = \"\") -> str:\n    \"\"\"\n    Returns the appropriate start directory.\n    Args:\n        start_dir (str): The directory to start the search from.\n    Returns:\n        str: The absolute path of the provided start_dir if it exists or can be created.\n    \"\"\"\n    if start_dir and not Path(start_dir).is_dir():\n        logging.info(f\"Setting Start Dir : {start_dir}\")\n        start_dir = os.getcwd()\n    else:\n        start_dir = os.path.abspath(start_dir)\n    return start_dir\n\n\ndef get_output_dir(output_dir: str = \"\") -> str:\n    \"\"\"Returns the appropriate output directory.\n    Args:\n        output_dir (str): The directory to write the output to.\n    Returns:\n        str: The absolute path of the provided output_dir if it exists or can be created.\n    \"\"\"\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f\"Using output directory: {output_dir}\")\n    return output_dir\n\n\ndef get_questions(questions_pathname: str) -> List[Dict]:\n    \"\"\"\n    Get questions from file or default\n    Args:\n        questions_pathname (str): The pathname of the questions file\n    Returns:\n        List[Dict]: The list of questions\n    \"\"\"\n    try:  # get questions from provided or default configuration file\n        if not questions_pathname:\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\n        with open(questions_pathname, \"r\") as f:\n            questions = json.load(f)\n        logging.info(f\"Using questions from file: {questions_pathname}\")\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\n        logging.info(\n            f\"Questions file not valid: {questions_pathname} Using default questions\"\n        )\n        questions = get_default_questions()\n    return questions\n\n\ndef instantiate_model(model_config: Dict) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): model configuration dictionary.\n    Returns:\n        object: An instance of the specified model class, or None if error.\n    \"\"\"\n    try:\n        module_name, class_name = model_config[\"model_import_path\"].rsplit(\".\", 1)\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\n        model_params = model_config[\"model_params\"]\n        inference_function_name = model_config[\"model_inference_function\"]\n        if inference_function_name != \"\":\n            inference_function = getattr(ModelClass, inference_function_name)\n            llm = inference_function(model_params.pop(\"model_path\"), **model_params)\n        else:\n            llm = ModelClass(model_params.pop(\"model_path\"), **model_params)\n        return llm\n    except (ImportError, AttributeError, Exception) as e:\n        logging.info(f\"Failed to instantiate the model. Error: {e}\")\n        return None\n\n\ndef get_model(model_config_pathname: str) -> tuple[object, str]:\n    \"\"\"\n    Returns an instantiated model and prompt template based on the model configuration.\n    Agrs:\n        model_config_pathname (str): The pathname of the model config file\n    Returns:\n        Tuple[object, str]: The instantiated model and prompt template\n    \"\"\"\n    try:\n        if not model_config_pathname:\n            model_config_pathname = os.path.join(os.getcwd(), MODEL_CONFIG_FILE)\n        with open(model_config_pathname, \"r\") as config_file:\n            model_config = yaml.safe_load(config_file)\n        logging.info(f\"Using model config from file: {model_config_pathname}\")\n    except (FileNotFoundError, yaml.YAMLError):\n        logging.info(\n            f\"Model config file not valid: {model_config_pathname} Using default model config\"\n        )\n        model_config = get_default_model_config()\n    model_config[\"model\"] = instantiate_model(model_config[\"inference_model\"])\n    return model_config\n\n\ndef write_questions_file(output_dir: str = \"\") -> None:\n    \"\"\"\n    Writes the default questions to a file in JSON format.\n    Args:\n        output_dir (str): The directory to write the questions file to.\n    Returns:\n        None\n    \"\"\"\n    questions = get_default_questions()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, QUESTIONS_FILE), \"w\") as file:\n        json.dump(questions, file, indent=4)\n\n\ndef write_model_config_file(output_dir: str = \"\") -> None:\n    \"\"\"\n    Writes the default model config to a file in YAML format.\n    Args:\n        output_dir (str): The directory to write the model config file to.\n    Returns:\n        None\n    \"\"\"\n    model_config = get_default_model_config()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, MODEL_CONFIG_FILE), \"w\") as file:\n        yaml.dump(model_config, file)\n\n```",
        "output": "{'nodes': ['get_default_questions', 'get_default_model_config', 'get_start_dir', 'get_output_dir', 'get_questions', 'instantiate_model', 'get_model', 'write_questions_file', 'write_model_config_file', 'Path(start_dir).is_dir', 'Path', 'logging.info', 'os.getcwd', 'os.path.abspath', 'os.makedirs', 'os.path.join', 'open', 'json.load', \"model_config['model_import_path'].rsplit\", 'getattr', 'importlib.import_module', 'inference_function', 'model_params.pop', 'ModelClass', 'yaml.safe_load', 'Path(output_dir).is_dir', 'json.dump', 'yaml.dump'], 'edges': [{'source': 'get_start_dir', 'target': 'Path(start_dir).is_dir', 'target_inputs': []}, {'source': 'get_start_dir', 'target': 'Path', 'target_inputs': ['start_dir']}, {'source': 'get_start_dir', 'target': 'logging.info', 'target_inputs': [\"f'Setting Start Dir : {start_dir}'\"]}, {'source': 'get_start_dir', 'target': 'os.getcwd', 'target_inputs': []}, {'source': 'get_start_dir', 'target': 'os.path.abspath', 'target_inputs': ['start_dir']}, {'source': 'get_output_dir', 'target': 'os.path.abspath', 'target_inputs': ['output_dir or OUTPUT_DIR']}, {'source': 'get_output_dir', 'target': 'os.makedirs', 'target_inputs': ['output_dir']}, {'source': 'get_output_dir', 'target': 'logging.info', 'target_inputs': [\"f'Using output directory: {output_dir}'\"]}, {'source': 'get_questions', 'target': 'os.path.join', 'target_inputs': ['os.getcwd()', 'QUESTIONS_FILE']}, {'source': 'get_questions', 'target': 'os.getcwd', 'target_inputs': []}, {'source': 'get_questions', 'target': 'open', 'target_inputs': ['questions_pathname', \"'r'\"]}, {'source': 'get_questions', 'target': 'json.load', 'target_inputs': ['f']}, {'source': 'get_questions', 'target': 'logging.info', 'target_inputs': [\"f'Using questions from file: {questions_pathname}'\", \"f'Questions file not valid: {questions_pathname} Using default questions'\"]}, {'source': 'get_questions', 'target': 'get_default_questions', 'target_inputs': [], 'target_returns': ['questions']}, {'source': 'instantiate_model', 'target': \"model_config['model_import_path'].rsplit\", 'target_inputs': [\"'.'\", '1']}, {'source': 'instantiate_model', 'target': 'getattr', 'target_inputs': ['importlib.import_module(module_name)', 'class_name', 'ModelClass', 'inference_function_name']}, {'source': 'instantiate_model', 'target': 'importlib.import_module', 'target_inputs': ['module_name']}, {'source': 'instantiate_model', 'target': 'inference_function', 'target_inputs': [\"model_params.pop('model_path')\"]}, {'source': 'instantiate_model', 'target': 'model_params.pop', 'target_inputs': [\"'model_path'\", \"'model_path'\"]}, {'source': 'instantiate_model', 'target': 'ModelClass', 'target_inputs': [\"model_params.pop('model_path')\"]}, {'source': 'instantiate_model', 'target': 'logging.info', 'target_inputs': [\"f'Failed to instantiate the model. Error: {e}'\"]}, {'source': 'get_model', 'target': 'os.path.join', 'target_inputs': ['os.getcwd()', 'MODEL_CONFIG_FILE']}, {'source': 'get_model', 'target': 'os.getcwd', 'target_inputs': []}, {'source': 'get_model', 'target': 'open', 'target_inputs': ['model_config_pathname', \"'r'\"]}, {'source': 'get_model', 'target': 'yaml.safe_load', 'target_inputs': ['config_file']}, {'source': 'get_model', 'target': 'logging.info', 'target_inputs': [\"f'Using model config from file: {model_config_pathname}'\", \"f'Model config file not valid: {model_config_pathname} Using default model config'\"]}, {'source': 'get_model', 'target': 'get_default_model_config', 'target_inputs': [], 'target_returns': ['model_config']}, {'source': 'get_model', 'target': 'instantiate_model', 'target_inputs': [\"model_config['inference_model']\"], 'target_returns': ['llm', 'None']}, {'source': 'write_questions_file', 'target': 'get_default_questions', 'target_inputs': [], 'target_returns': ['questions']}, {'source': 'write_questions_file', 'target': 'Path(output_dir).is_dir', 'target_inputs': []}, {'source': 'write_questions_file', 'target': 'Path', 'target_inputs': ['output_dir']}, {'source': 'write_questions_file', 'target': 'os.getcwd', 'target_inputs': []}, {'source': 'write_questions_file', 'target': 'open', 'target_inputs': ['os.path.join(output_dir, QUESTIONS_FILE)', \"'w'\"]}, {'source': 'write_questions_file', 'target': 'os.path.join', 'target_inputs': ['output_dir', 'QUESTIONS_FILE']}, {'source': 'write_questions_file', 'target': 'json.dump', 'target_inputs': ['questions', 'file']}, {'source': 'write_model_config_file', 'target': 'get_default_model_config', 'target_inputs': [], 'target_returns': ['model_config']}, {'source': 'write_model_config_file', 'target': 'Path(output_dir).is_dir', 'target_inputs': []}, {'source': 'write_model_config_file', 'target': 'Path', 'target_inputs': ['output_dir']}, {'source': 'write_model_config_file', 'target': 'os.getcwd', 'target_inputs': []}, {'source': 'write_model_config_file', 'target': 'open', 'target_inputs': ['os.path.join(output_dir, MODEL_CONFIG_FILE)', \"'w'\"]}, {'source': 'write_model_config_file', 'target': 'os.path.join', 'target_inputs': ['output_dir', 'MODEL_CONFIG_FILE']}, {'source': 'write_model_config_file', 'target': 'yaml.dump', 'target_inputs': ['model_config', 'file']}]}"
    },
    {
        "instruction": "Functions defined in Python file: `py2dataset.get_params.py`?",
        "input": "```python\n\"\"\"\nObtain data parameter and model from the py2dataset functions.\nRequirements:\n[req01] The `get_default_questions` function shall:\n        a. Return a list of default questions.\n        b. Ensure each question in the list is a dictionary.\n        c. Ensure each dictionary has the keys: id, text, and type.\n[req02] The `get_default_model_config` function shall:\n        a. Return a dictionary representing the default model configuration.\n[req03] The `get_output_dir` function shall:\n        a. Accept an optional output_dir argument.\n        b. Return the absolute path of the provided output_dir if it exists or can be created.\n        c. Return the default OUTPUT_DIR if the provided output_dir argument is not provided or invalid.\n[req04] The `get_questions` function shall:\n        a. Accept an optional questions_pathname argument.\n        b. Validate the question file provided by the questions_pathname.\n        c. Return the questions from the provided questions_pathname if valid.\n        d. Return default questions if the questions_pathname is not provided or invalid.\n[req05] The `instantiate_model` function shall:\n        a. Accept a model_config dictionary as an argument.\n        b. Import the specified module and class from the model_config.\n        c. Instantiate and return the model using the provided configuration.\n[req06] The `get_model` function shall:\n        a. Accept an optional model_config_pathname argument.\n        b. Validate the model config file provided by the model_config_pathname.\n        c. Return an instantiated model and a prompt template based on the provided configuration.\n        d. Return an instantiated model and a prompt template based on the default model configuration if the model_config_pathname is not provided or invalid.\n[req07] The `write_questions_file` function shall:\n        a. Accept an optional output_dir argument.\n        b. Write the default questions to the QUESTIONS_FILE in the specified directory.\n        c. Write the default questions to the QUESTIONS_FILE in the current working directory if the output_dir argument is not provided or invalid.\n[req08] The `write_model_config_file` function shall:\n        a. Accept an optional output_dir argument.\n        b. Write the default model configuration to the MODEL_CONFIG_FILE in the specified directory.\n        c. Write the default model configuration to the MODEL_CONFIG_FILE in the current working directory if the output_dir argument is not provided or invalid.\n\"\"\"\nimport os\nimport json\nimport logging\nimport importlib\nfrom typing import Dict, List\nfrom pathlib import Path\nimport yaml\n\n# Setting up a basic logger\nlogging.basicConfig(level=logging.INFO)\n\nQUESTIONS_FILE = \"py2dataset_questions.json\"\nMODEL_CONFIG_FILE = \"py2dataset_model_config.yaml\"\nOUTPUT_DIR = \"datasets\"\n\n\ndef get_default_questions() -> List[Dict]:\n    \"\"\"Return default question list\n    Args:\n        None\n    Returns:\n        List[Dict]: The default question list\n    \"\"\"\n    questions = [\n        {\n            \"id\": \"file_dependencies\",\n            \"text\": \"Dependencies of Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"entire_code_graph\",\n            \"text\": \"Call code graph of Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_functions\",\n            \"text\": \"Functions defined in Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_classes\",\n            \"text\": \"Classes defined in Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"function_inputs\",\n            \"text\": \"Inputs to function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_docstring\",\n            \"text\": \"Docstring of function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_calls\",\n            \"text\": \"Calls made in function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_variables\",\n            \"text\": \"Variables defined in function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_returns\",\n            \"text\": \"Returns from function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"class_methods\",\n            \"text\": \"Methods defined in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_docstring\",\n            \"text\": \"Docstring of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_attributes\",\n            \"text\": \"Attributes of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_variables\",\n            \"text\": \"Variables defined in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_inheritance\",\n            \"text\": \"Inheritance of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"method_inputs\",\n            \"text\": \"Inputs to method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_docstring\",\n            \"text\": \"Docstring of method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_calls\",\n            \"text\": \"Calls made in method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_returns\",\n            \"text\": \"Returns from method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"file_purpose\",\n            \"text\": \"1) Describe the Purpose and Processing Summary of Python file: '{filename}'; 2) Provide an itemized and detailed description of each applicable function, class, and method; 3) Explain what each input, output, and variable does in the code.\",\n            \"type\": \"file\",\n        },\n    ]\n    return questions\n\n\ndef get_default_model_config() -> Dict:\n    \"\"\"Return default model config dict\n    Args:\n        None\n    Returns:\n        Dict: The default model config dictionary\n    \"\"\"\n    model_config = {\n        \"prompt_template\": \"You are an AI coding instructor. Given this Context:\\n'{context}'\\n\\nPlease provide a very detailed, accurate, and insightful Response to this Instruction and include your reasoning step by step. \\n### Instruction:\\n{query}\\n### Response:\",\n        \"inference_model\": {\n            \"model_import_path\": \"ctransformers.AutoModelForCausalLM\",\n            \"model_inference_function\": \"from_pretrained\",\n            \"model_params\": {\n                \"model_path\": \"TheBloke/WizardCoder-Python-13B-V1.0-GGUF\",\n                \"model_type\": \"llama\",\n                \"local_files_only\": False,\n                ## MODEL CONFIGURATION PARAMETERS (set for current model with: GPU 4090-24GB VRAM, CPU 5950x-32 threads, 64GB RAM) \n                # avx2 and gpu_layers are not compatible\n                # \"lib\": \"avx2\",\n                \"threads\": 16,\n                \"batch_size\": 128,\n                \"context_length\": 12000,\n                \"max_new_tokens\": 12000,\n                \"gpu_layers\": 100,\n                \"reset\": True,\n            },\n        },\n    }\n    return model_config\n\n\ndef get_start_dir(start_dir: str = \"\") -> str:\n    \"\"\"\n    Returns the appropriate start directory.\n    Args:\n        start_dir (str): The directory to start the search from.\n    Returns:\n        str: The absolute path of the provided start_dir if it exists or can be created.\n    \"\"\"\n    if start_dir and not Path(start_dir).is_dir():\n        logging.info(f\"Setting Start Dir : {start_dir}\")\n        start_dir = os.getcwd()\n    else:\n        start_dir = os.path.abspath(start_dir)\n    return start_dir\n\n\ndef get_output_dir(output_dir: str = \"\") -> str:\n    \"\"\"Returns the appropriate output directory.\n    Args:\n        output_dir (str): The directory to write the output to.\n    Returns:\n        str: The absolute path of the provided output_dir if it exists or can be created.\n    \"\"\"\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f\"Using output directory: {output_dir}\")\n    return output_dir\n\n\ndef get_questions(questions_pathname: str) -> List[Dict]:\n    \"\"\"\n    Get questions from file or default\n    Args:\n        questions_pathname (str): The pathname of the questions file\n    Returns:\n        List[Dict]: The list of questions\n    \"\"\"\n    try:  # get questions from provided or default configuration file\n        if not questions_pathname:\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\n        with open(questions_pathname, \"r\") as f:\n            questions = json.load(f)\n        logging.info(f\"Using questions from file: {questions_pathname}\")\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\n        logging.info(\n            f\"Questions file not valid: {questions_pathname} Using default questions\"\n        )\n        questions = get_default_questions()\n    return questions\n\n\ndef instantiate_model(model_config: Dict) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): model configuration dictionary.\n    Returns:\n        object: An instance of the specified model class, or None if error.\n    \"\"\"\n    try:\n        module_name, class_name = model_config[\"model_import_path\"].rsplit(\".\", 1)\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\n        model_params = model_config[\"model_params\"]\n        inference_function_name = model_config[\"model_inference_function\"]\n        if inference_function_name != \"\":\n            inference_function = getattr(ModelClass, inference_function_name)\n            llm = inference_function(model_params.pop(\"model_path\"), **model_params)\n        else:\n            llm = ModelClass(model_params.pop(\"model_path\"), **model_params)\n        return llm\n    except (ImportError, AttributeError, Exception) as e:\n        logging.info(f\"Failed to instantiate the model. Error: {e}\")\n        return None\n\n\ndef get_model(model_config_pathname: str) -> tuple[object, str]:\n    \"\"\"\n    Returns an instantiated model and prompt template based on the model configuration.\n    Agrs:\n        model_config_pathname (str): The pathname of the model config file\n    Returns:\n        Tuple[object, str]: The instantiated model and prompt template\n    \"\"\"\n    try:\n        if not model_config_pathname:\n            model_config_pathname = os.path.join(os.getcwd(), MODEL_CONFIG_FILE)\n        with open(model_config_pathname, \"r\") as config_file:\n            model_config = yaml.safe_load(config_file)\n        logging.info(f\"Using model config from file: {model_config_pathname}\")\n    except (FileNotFoundError, yaml.YAMLError):\n        logging.info(\n            f\"Model config file not valid: {model_config_pathname} Using default model config\"\n        )\n        model_config = get_default_model_config()\n    model_config[\"model\"] = instantiate_model(model_config[\"inference_model\"])\n    return model_config\n\n\ndef write_questions_file(output_dir: str = \"\") -> None:\n    \"\"\"\n    Writes the default questions to a file in JSON format.\n    Args:\n        output_dir (str): The directory to write the questions file to.\n    Returns:\n        None\n    \"\"\"\n    questions = get_default_questions()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, QUESTIONS_FILE), \"w\") as file:\n        json.dump(questions, file, indent=4)\n\n\ndef write_model_config_file(output_dir: str = \"\") -> None:\n    \"\"\"\n    Writes the default model config to a file in YAML format.\n    Args:\n        output_dir (str): The directory to write the model config file to.\n    Returns:\n        None\n    \"\"\"\n    model_config = get_default_model_config()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, MODEL_CONFIG_FILE), \"w\") as file:\n        yaml.dump(model_config, file)\n\n```",
        "output": "Functions defined:get_default_questions, get_default_model_config, get_start_dir, get_output_dir, get_questions, instantiate_model, get_model, write_questions_file, write_model_config_file\n\nPurpose and Significance:\nThese objects define the following functionalities within the code:\r\n- `get_default_questions`: This function returns a list of default questions that are used to prompt the model for answers.\r\n- `get_default_model_config`: This function returns a dictionary containing the default configuration for the model used by the code.\r\n- `get_start_dir`: This function takes an optional start directory as input and returns the appropriate starting point of searching.\r\n- `get_output_dir`: This function validates or creates a given output directory to store files, if it doesn't exist already.\r\n- `get_questions`: This function checks for a provided questions file and returns default questions if no valid file is found.\r\n- `instantiate_model`: This function imports the specified model class from the provided configuration and instantiates an object of that class.\r\n- `get_model`: This function takes an optional model config pathname as input, reads it using YAML format, and returns an instantiated model based on the configuration.\r\n- `write_questions_file`: This function writes default questions to a JSON file in the specified directory or current working directory if no valid output directory is provided.\r\n- `write_model_config_file`: This function writes the default model config as YAML to the given directory, and to the current directory if not specified. "
    },
    {
        "instruction": "Inputs to `get_start_dir` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_start_dir(start_dir: str='') -> str:\n    \"\"\"\n    Returns the appropriate start directory.\n    Args:\n        start_dir (str): The directory to start the search from.\n    Returns:\n        str: The absolute path of the provided start_dir if it exists or can be created.\n    \"\"\"\n    if start_dir and (not Path(start_dir).is_dir()):\n        logging.info(f'Setting Start Dir : {start_dir}')\n        start_dir = os.getcwd()\n    else:\n        start_dir = os.path.abspath(start_dir)\n    return start_dir",
        "output": "Inputs to `get_start_dir`:start_dir\n\nPurpose and Significance:\nThe `get_start_dir()` function takes in an optional parameter called `start_dir` which defaults to `\"\"` (empty string) if not provided. If it is empty, the function returns the current working directory as the starting directory using `os.getcwd()`. Otherwise, it checks if the specified `start_dir` exists or can be created using `Path(start_dir).is_dir()` and raises an error if it doesn't exist.\r\n\nThe purpose of this function is to return a valid starting directory that will be used later on in the program. This function is called by other functions to get the starting directory for searching files or creating output directories, as well as ensuring that they are valid paths. "
    },
    {
        "instruction": "Inputs to `get_output_dir` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_output_dir(output_dir: str='') -> str:\n    \"\"\"Returns the appropriate output directory.\n    Args:\n        output_dir (str): The directory to write the output to.\n    Returns:\n        str: The absolute path of the provided output_dir if it exists or can be created.\n    \"\"\"\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f'Using output directory: {output_dir}')\n    return output_dir",
        "output": "Inputs to `get_output_dir`:output_dir\n\nPurpose and Significance:\nThe `get_output_dir` function is responsible for validating and returning an output directory path. It first checks if the provided argument is a valid directory or can be created, and then returns it as an absolute path. If no argument is provided or the provided argument is invalid, it returns the default OUTPUT_DIR value. "
    },
    {
        "instruction": "Inputs to `get_questions` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_questions(questions_pathname: str) -> List[Dict]:\n    \"\"\"\n    Get questions from file or default\n    Args:\n        questions_pathname (str): The pathname of the questions file\n    Returns:\n        List[Dict]: The list of questions\n    \"\"\"\n    try:\n        if not questions_pathname:\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\n        with open(questions_pathname, 'r') as f:\n            questions = json.load(f)\n        logging.info(f'Using questions from file: {questions_pathname}')\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\n        logging.info(f'Questions file not valid: {questions_pathname} Using default questions')\n        questions = get_default_questions()\n    return questions",
        "output": "Inputs to `get_questions`:questions_pathname\n\nPurpose and Significance:\nThe `get_default_questions` function returns a list of default questions, which is a dictionary containing 15 keys, where each key contains the details about each question, such as the id, text, and type. The `get_default_model_config` function returns a dictionary representing the default model configuration, which includes the prompt template and the parameters for instantiating the specified model. The `get_output_dir` function returns the absolute path of the provided output_dir if it exists or can be created; otherwise, it will return the default OUTPUT_DIR value, i.e., 'datasets'. The `instantiate_model` function imports and instantiates a model based on the provided configuration dictionary. The `get_questions` function accepts an optional questions_pathname argument, which is used to validate the question file provided by that pathname. If the provided questions_pathname is not valid or if it's not provided, then it returns default questions. The `write_questions_file` function writes the default questions to a JSON file named 'py2dataset_questions.json' in the specified directory (or current working directory if no output_dir argument is provided). The `write_model_config_file` function writes the default model configuration to a YAML file named 'py2dataset_model_config.yaml' in the specified directory (or current working directory if no output_dir argument is provided). "
    },
    {
        "instruction": "Inputs to `instantiate_model` in Python file: `py2dataset.get_params.py`?",
        "input": "def instantiate_model(model_config: Dict) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): model configuration dictionary.\n    Returns:\n        object: An instance of the specified model class, or None if error.\n    \"\"\"\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\n        model_params = model_config['model_params']\n        inference_function_name = model_config['model_inference_function']\n        if inference_function_name != '':\n            inference_function = getattr(ModelClass, inference_function_name)\n            llm = inference_function(model_params.pop('model_path'), **model_params)\n        else:\n            llm = ModelClass(model_params.pop('model_path'), **model_params)\n        return llm\n    except (ImportError, AttributeError, Exception) as e:\n        logging.info(f'Failed to instantiate the model. Error: {e}')\n        return None",
        "output": "Inputs to `instantiate_model`:model_config\n\nPurpose and Significance:\nThe `instantiate_model` function accepts a model configuration dictionary as an argument and imports the specified module and class from the model_config. It then instantiates and returns the model using the provided configuration.\r\n\n'{Output of `get_default_questions`}': The default question list is returned by the `get_default_questions` function, which contains a list of dictionaries with the keys 'id', 'text', and 'type'. Each dictionary represents a single question that can be asked to a language model for generating code. The 'id' key serves as an identifier for the type of query (e.g., \"file_dependencies\", \"function_calls\"), while the 'text' key contains the prompt template that is used to generate code using the model, and the 'type' key specifies whether the question pertains to a file, function, class, or method.\r\n\n```python\r\ndef get_default_questions() -> List[Dict]:\r\n    \"\"\"Return default question list\"\"\"\r\n    questions = [\r\n        {\r\n            \"id\": \"file_dependencies\",\r\n            \"text\": \"Dependencies of Python file: '{filename}'?\",\r\n            \"type\": \"file\",\r\n        },\r\n        ...,\r\n        {  # The last dictionary in the list is for a general purpose question about the code's purpose and processing steps.\r\n            \"id\": \"file_purpose\",\r\n            \"text\": \"1) Describe the Purpose and Processing Summary of Python file: '{filename}'; 2) Provide an itemized and detailed description of each applicable function, class, and method; 3) Explain what each input, output, and variable does in the code.\",\r\n            \"type\": \"file\",\r\n        },\r\n    ]\r\n    return questions\r\n```\r\n\n'{Output of `get_default_model_config`}': The default model configuration is returned by the `get_default_model_config` function, which contains a dictionary with two keys: 'prompt_template', and 'inference_model'. The 'prompt_template' key specifies the prompt that will be used to generate code responses from the language model, while the 'inference_model' key provides information about the model type and parameters for instantiation.\r\n\n```python\r\ndef get_default_model_config() -> Dict:\r\n    \"\"\"Return default model config dict\"\"\"\r\n    model_config = {\r\n        \"prompt_template\": \"You are an AI coding instructor. Given this Context:\\n'{context}'\\n\\nPlease provide a very detailed, accurate, and insightful Response to this Instruction and include your reasoning step by step. \\n### Instruction:\\n{query}\\n### Response:\",\r\n        \"inference_model\": {\r\n            \"model_import_path\": \"ctransformers.AutoModelForCausalLM\",\r\n            \"model_inference_function\": \"from_pretrained\",\r\n            \"model_params\": {\r\n                \"model_path\": \"TheBloke/WizardCoder-Python-13B-V1.0-GGUF\",\r\n                \"model_type\": \"llama\",\r\n                \"local_files_only\": False,\r\n                ## MODEL CONFIGURATION PARAMETERS (set for current model with: GPU 4090-24GB VRAM, CPU 5950x-32 threads, 64GB RAM)\r\n                # avx2 and gpu_layers are not compatible\r\n                # \"lib\": \"avx2\",\r\n                \"threads\": 16,\r\n                \"batch_size\": 128,\r\n                \"context_length\": 12000,\r\n                \"max_new_tokens\": 12000,\r\n                \"gpu_layers\": 100,\r\n                \"reset\": True,\r\n            },\r\n        },\r\n    }\r\n    return model_config\r\n```\r\n\n'{Output of `get_output_dir`}': The absolute path to the specified output directory is returned by the `get_output_dir` function, or a default output directory (current working directory if not provided).\r\n```python\r\ndef get_output_dir(output_dir: str = \"\") -> str:\r\n    \"\"\"Returns the appropriate output directory\"\"\"\r\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\r\n    os.makedirs(output_dir, exist_ok=True)\r\n    logging.info(f\"Using output directory: {output_dir}\")\r\n    return output_dir\r\n```\r\n\n'{Output of `get_questions`}': The function `get_questions` either reads the questions from a provided file or returns the default question list if no valid file is found.\r\n```python\r\ndef get_questions(questions_pathname: str) -> List[Dict]:\r\n    \"\"\"Get questions from file or default\"\"\"\r\n    try:  # get questions from provided or default configuration file\r\n        if not questions_pathname:\r\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\r\n        with open(questions_pathname, \"r\") as f:\r\n            questions = json.load(f)\r\n        logging.info(f\"Using questions from file: {questions_pathname}\")\r\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\r\n        logging.info(\r\n            f\"Questions file not valid: {questions_pathname} Using default questions\"\r\n        )\r\n        questions = get_default_questions()\r\n    return questions\r\n```\r\n\n'{Output of `instantiate_model`}': The function `instantiate_model` returns an instance of the specified model based on a provided configuration dictionary, after importing and accessing its class using its import path and inference function. This function is called by `get_model`, which takes a path to a YAML file as input and updates the default configuration with the custom settings if provided.\r\n```python\r\ndef instantiate_model(model_config: Dict) -> object:\r\n    \"\"\"\r\n    Imports and instantiates a model based on the provided configuration.\r\n    Args:\r\n        model_config (dict): model configuration dictionary.\r\n    Returns:\r\n        object: An instance of the specified model class, or None if error.\r\n    \"\"\"\r\n    try:\r\n        module_name, class_name = model_config[\"model_import_path\"].rsplit(\".\", 1)\r\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\r\n        model_params = model_config[\"model_params\"]\r\n        inference_function_name = model_config[\"model_inference_function\"]\r\n        if inference_function_name != \"\":\r\n            inference_function = getattr(ModelClass, inference_function_name)\r\n            llm = inference_function(model_params.pop(\"model_path\"), **model_params)\r\n        else:\r\n            llm = ModelClass(model_params.pop(\"model_path\"), **model_params)\r\n        return llm\r\n    except (ImportError, AttributeError, Exception) as e:\r\n        logging.info(f\"Failed to instantiate the model. Error: {e}\")\r\n        return None\r\n``` "
    },
    {
        "instruction": "Inputs to `get_model` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_model(model_config_pathname: str) -> tuple[object, str]:\n    \"\"\"\n    Returns an instantiated model and prompt template based on the model configuration.\n    Agrs:\n        model_config_pathname (str): The pathname of the model config file\n    Returns:\n        Tuple[object, str]: The instantiated model and prompt template\n    \"\"\"\n    try:\n        if not model_config_pathname:\n            model_config_pathname = os.path.join(os.getcwd(), MODEL_CONFIG_FILE)\n        with open(model_config_pathname, 'r') as config_file:\n            model_config = yaml.safe_load(config_file)\n        logging.info(f'Using model config from file: {model_config_pathname}')\n    except (FileNotFoundError, yaml.YAMLError):\n        logging.info(f'Model config file not valid: {model_config_pathname} Using default model config')\n        model_config = get_default_model_config()\n    model_config['model'] = instantiate_model(model_config['inference_model'])\n    return model_config",
        "output": "Inputs to `get_model`:model_config_pathname\n\nPurpose and Significance:\nThe purpose of `get_default_questions()` function is to return a list of default questions, ensuring each question in the list is a dictionary with keys \"id\", \"text\" and \"type\". It's used as a fallback if no valid questions file is provided. The significance of this function is that it provides a set of pre-defined questions that can be used to generate datasets for code analysis tasks.\r\n\nThe purpose of `get_default_model_config()` function is to return a dictionary representing the default model configuration, which includes the prompt template and parameters for instantiating the model. The significance of this function is that it provides a set of pre-defined values that can be used to instantiate a language model for code analysis tasks.\r\n\nThe purpose of `get_output_dir()` function is to return an absolute path for the specified output directory or the default one if not provided. The significance of this function is that it helps with setting up a standardized way to write data files to various paths while being platform independent by abstracting some directory/path specific information. For instance, even in Docker-based projects the OUTPUT_DIR constant ensures proper setup on multiple OS and avoids using environment variables.\r\n\nThe purpose of `get_questions()` function is to validate a provided questions file or use default ones if no valid question file is provided. It returns a list of dictionaries with \"id\", \"text\" and \"type\" keys for each question. The significance of this function is that it provides a way to customize the set of questions that will be used for code analysis tasks, and can help improve or reduce the dataset quality based on user preference.\r\n\nThe purpose of `instantiate_model()` function is to import and instantiate a model based on the provided configuration. The significance of this function is that it allows the user to use any language model from Hugging Face Transformers library for code analysis tasks, as long as they provide the correct import path and parameters.\r\n\nThe purpose of `get_model()` function is to validate a provided model config file or use default one if no valid config file is provided. It returns an instantiated model object based on the provided configuration, along with a prompt template string. The significance of this function is that it allows users to adjust prompt template or tune specific settings without manually building language models or custom classes, using parameters directly from config files.\r\n\nThe purpose of `write_questions_file()` and `write_model_config_file()` functions is to write default questions and model configuration to a file in JSON and YAML format respectively. The significance of these functions is that they provide a way for users to generate sample configuration files if no valid ones are provided, and helps with generating consistent data across multiple runs of the script.\r\n\nIn summary, this code provides a set of helper functions that can be used by other modules in the py2dataset library to obtain default questions, model configurations, and write them to files. "
    },
    {
        "instruction": "Inputs to `write_questions_file` in Python file: `py2dataset.get_params.py`?",
        "input": "def write_questions_file(output_dir: str='') -> None:\n    \"\"\"\n    Writes the default questions to a file in JSON format.\n    Args:\n        output_dir (str): The directory to write the questions file to.\n    Returns:\n        None\n    \"\"\"\n    questions = get_default_questions()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, QUESTIONS_FILE), 'w') as file:\n        json.dump(questions, file, indent=4)",
        "output": "Inputs to `write_questions_file`:output_dir\n\nPurpose and Significance:\nThe `write_questions_file` function is used to write the default questions to a JSON file named `py2dataset_questions.json`. The `output_dir` argument is optional and if not provided, it will write the file in the current working directory. "
    },
    {
        "instruction": "Inputs to `write_model_config_file` in Python file: `py2dataset.get_params.py`?",
        "input": "def write_model_config_file(output_dir: str='') -> None:\n    \"\"\"\n    Writes the default model config to a file in YAML format.\n    Args:\n        output_dir (str): The directory to write the model config file to.\n    Returns:\n        None\n    \"\"\"\n    model_config = get_default_model_config()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, MODEL_CONFIG_FILE), 'w') as file:\n        yaml.dump(model_config, file)",
        "output": "Inputs to `write_model_config_file`:output_dir\n\nPurpose and Significance:\nThe `write_model_config_file` function is responsible for writing the default model configuration to a file in YAML format. If an output directory is provided, it will write the default model configuration to the MODEL_CONFIG_FILE in that directory. Otherwise, it will write the default model configuration to the current working directory.\r\n\nThe `output_dir` argument is optional and if not provided or invalid, the function will use the default OUTPUT_DIR constant defined at the top of the code. "
    },
    {
        "instruction": "Docstring of `get_default_questions` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_default_questions() -> List[Dict]:\n    \"\"\"Return default question list\n    Args:\n        None\n    Returns:\n        List[Dict]: The default question list\n    \"\"\"\n    questions = [{'id': 'file_dependencies', 'text': \"Dependencies of Python file: '{filename}'?\", 'type': 'file'}, {'id': 'entire_code_graph', 'text': \"Call code graph of Python file: '{filename}'?\", 'type': 'file'}, {'id': 'file_functions', 'text': \"Functions defined in Python file: '{filename}'?\", 'type': 'file'}, {'id': 'file_classes', 'text': \"Classes defined in Python file: '{filename}'?\", 'type': 'file'}, {'id': 'function_inputs', 'text': \"Inputs to function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'function_docstring', 'text': \"Docstring of function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'function_calls', 'text': \"Calls made in function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'function_variables', 'text': \"Variables defined in function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'function_returns', 'text': \"Returns from function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'class_methods', 'text': \"Methods defined in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'class_docstring', 'text': \"Docstring of class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'class_attributes', 'text': \"Attributes of class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'class_variables', 'text': \"Variables defined in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'class_inheritance', 'text': \"Inheritance of class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'method_inputs', 'text': \"Inputs to method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'method'}, {'id': 'method_docstring', 'text': \"Docstring of method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'method'}, {'id': 'method_calls', 'text': \"Calls made in method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'method'}, {'id': 'method_returns', 'text': \"Returns from method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'method'}, {'id': 'file_purpose', 'text': \"1) Describe the Purpose and Processing Summary of Python file: '{filename}'; 2) Provide an itemized and detailed description of each applicable function, class, and method; 3) Explain what each input, output, and variable does in the code.\", 'type': 'file'}]\n    return questions",
        "output": "Return default question list\n    Args:\n        None\n    Returns:\n        List[Dict]: The default question list"
    },
    {
        "instruction": "Docstring of `get_default_model_config` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_default_model_config() -> Dict:\n    \"\"\"Return default model config dict\n    Args:\n        None\n    Returns:\n        Dict: The default model config dictionary\n    \"\"\"\n    model_config = {'prompt_template': \"You are an AI coding instructor. Given this Context:\\n'{context}'\\n\\nPlease provide a very detailed, accurate, and insightful Response to this Instruction and include your reasoning step by step. \\n### Instruction:\\n{query}\\n### Response:\", 'inference_model': {'model_import_path': 'ctransformers.AutoModelForCausalLM', 'model_inference_function': 'from_pretrained', 'model_params': {'model_path': 'TheBloke/WizardCoder-Python-13B-V1.0-GGUF', 'model_type': 'llama', 'local_files_only': False, 'threads': 16, 'batch_size': 128, 'context_length': 12000, 'max_new_tokens': 12000, 'gpu_layers': 100, 'reset': True}}}\n    return model_config",
        "output": "Return default model config dict\n    Args:\n        None\n    Returns:\n        Dict: The default model config dictionary"
    },
    {
        "instruction": "Docstring of `get_start_dir` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_start_dir(start_dir: str='') -> str:\n    \"\"\"\n    Returns the appropriate start directory.\n    Args:\n        start_dir (str): The directory to start the search from.\n    Returns:\n        str: The absolute path of the provided start_dir if it exists or can be created.\n    \"\"\"\n    if start_dir and (not Path(start_dir).is_dir()):\n        logging.info(f'Setting Start Dir : {start_dir}')\n        start_dir = os.getcwd()\n    else:\n        start_dir = os.path.abspath(start_dir)\n    return start_dir",
        "output": "Returns the appropriate start directory.\n    Args:\n        start_dir (str): The directory to start the search from.\n    Returns:\n        str: The absolute path of the provided start_dir if it exists or can be created."
    },
    {
        "instruction": "Docstring of `get_output_dir` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_output_dir(output_dir: str='') -> str:\n    \"\"\"Returns the appropriate output directory.\n    Args:\n        output_dir (str): The directory to write the output to.\n    Returns:\n        str: The absolute path of the provided output_dir if it exists or can be created.\n    \"\"\"\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f'Using output directory: {output_dir}')\n    return output_dir",
        "output": "Returns the appropriate output directory.\n    Args:\n        output_dir (str): The directory to write the output to.\n    Returns:\n        str: The absolute path of the provided output_dir if it exists or can be created."
    },
    {
        "instruction": "Docstring of `get_questions` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_questions(questions_pathname: str) -> List[Dict]:\n    \"\"\"\n    Get questions from file or default\n    Args:\n        questions_pathname (str): The pathname of the questions file\n    Returns:\n        List[Dict]: The list of questions\n    \"\"\"\n    try:\n        if not questions_pathname:\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\n        with open(questions_pathname, 'r') as f:\n            questions = json.load(f)\n        logging.info(f'Using questions from file: {questions_pathname}')\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\n        logging.info(f'Questions file not valid: {questions_pathname} Using default questions')\n        questions = get_default_questions()\n    return questions",
        "output": "Get questions from file or default\n    Args:\n        questions_pathname (str): The pathname of the questions file\n    Returns:\n        List[Dict]: The list of questions"
    },
    {
        "instruction": "Docstring of `instantiate_model` in Python file: `py2dataset.get_params.py`?",
        "input": "def instantiate_model(model_config: Dict) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): model configuration dictionary.\n    Returns:\n        object: An instance of the specified model class, or None if error.\n    \"\"\"\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\n        model_params = model_config['model_params']\n        inference_function_name = model_config['model_inference_function']\n        if inference_function_name != '':\n            inference_function = getattr(ModelClass, inference_function_name)\n            llm = inference_function(model_params.pop('model_path'), **model_params)\n        else:\n            llm = ModelClass(model_params.pop('model_path'), **model_params)\n        return llm\n    except (ImportError, AttributeError, Exception) as e:\n        logging.info(f'Failed to instantiate the model. Error: {e}')\n        return None",
        "output": "Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): model configuration dictionary.\n    Returns:\n        object: An instance of the specified model class, or None if error."
    },
    {
        "instruction": "Docstring of `get_model` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_model(model_config_pathname: str) -> tuple[object, str]:\n    \"\"\"\n    Returns an instantiated model and prompt template based on the model configuration.\n    Agrs:\n        model_config_pathname (str): The pathname of the model config file\n    Returns:\n        Tuple[object, str]: The instantiated model and prompt template\n    \"\"\"\n    try:\n        if not model_config_pathname:\n            model_config_pathname = os.path.join(os.getcwd(), MODEL_CONFIG_FILE)\n        with open(model_config_pathname, 'r') as config_file:\n            model_config = yaml.safe_load(config_file)\n        logging.info(f'Using model config from file: {model_config_pathname}')\n    except (FileNotFoundError, yaml.YAMLError):\n        logging.info(f'Model config file not valid: {model_config_pathname} Using default model config')\n        model_config = get_default_model_config()\n    model_config['model'] = instantiate_model(model_config['inference_model'])\n    return model_config",
        "output": "Returns an instantiated model and prompt template based on the model configuration.\n    Agrs:\n        model_config_pathname (str): The pathname of the model config file\n    Returns:\n        Tuple[object, str]: The instantiated model and prompt template"
    },
    {
        "instruction": "Docstring of `write_questions_file` in Python file: `py2dataset.get_params.py`?",
        "input": "def write_questions_file(output_dir: str='') -> None:\n    \"\"\"\n    Writes the default questions to a file in JSON format.\n    Args:\n        output_dir (str): The directory to write the questions file to.\n    Returns:\n        None\n    \"\"\"\n    questions = get_default_questions()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, QUESTIONS_FILE), 'w') as file:\n        json.dump(questions, file, indent=4)",
        "output": "Writes the default questions to a file in JSON format.\n    Args:\n        output_dir (str): The directory to write the questions file to.\n    Returns:\n        None"
    },
    {
        "instruction": "Docstring of `write_model_config_file` in Python file: `py2dataset.get_params.py`?",
        "input": "def write_model_config_file(output_dir: str='') -> None:\n    \"\"\"\n    Writes the default model config to a file in YAML format.\n    Args:\n        output_dir (str): The directory to write the model config file to.\n    Returns:\n        None\n    \"\"\"\n    model_config = get_default_model_config()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, MODEL_CONFIG_FILE), 'w') as file:\n        yaml.dump(model_config, file)",
        "output": "Writes the default model config to a file in YAML format.\n    Args:\n        output_dir (str): The directory to write the model config file to.\n    Returns:\n        None"
    },
    {
        "instruction": "Calls made in `get_start_dir` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_start_dir(start_dir: str='') -> str:\n    \"\"\"\n    Returns the appropriate start directory.\n    Args:\n        start_dir (str): The directory to start the search from.\n    Returns:\n        str: The absolute path of the provided start_dir if it exists or can be created.\n    \"\"\"\n    if start_dir and (not Path(start_dir).is_dir()):\n        logging.info(f'Setting Start Dir : {start_dir}')\n        start_dir = os.getcwd()\n    else:\n        start_dir = os.path.abspath(start_dir)\n    return start_dir",
        "output": "Calls made in `get_start_dir`:Path(start_dir).is_dir, Path, logging.info, os.getcwd, os.path.abspath\n\nPurpose and Significance:\nThe `get_start_dir` function is used to obtain the appropriate start directory based on the input argument `start_dir`. If it exists or can be created, it returns the absolute path of the provided directory; otherwise, it uses the current working directory. \r\n\n`'Calls made in `instantiate_model': 'importlib.import_module' loads the module at model_config[\"model_import_path\"]. `import_module` returns the loaded module. Getattr function is then used to load the specified class from that module, which will be the instantiated model.\r\n\n`'Calls made in `get_output_dir': 'os.makedirs' creates a directory if it doesn't exist and returns the absolute path of output_dir';\r\n'Prompt Template` refers to a custom text input format used to query for questions in our default_config (py2dataset/question/base/constants.py). This prompt template is then used by `get_model` function to generate prompts based on the user's inputs and question type.\r\n\n`'Calls made in get_questions': 'Open a file, read it using json.load(), return default questions if not found or unable to decode the content as JSON'\r\n\n`'Calls made in `get_model`: 'Open a YAML file, read its contents using yaml.safe_load() and returns the model configuration dictionary.'\r\n\n`'Output dir is where models will be written if a file was provided.` in get_output_dir "
    },
    {
        "instruction": "Calls made in `get_output_dir` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_output_dir(output_dir: str='') -> str:\n    \"\"\"Returns the appropriate output directory.\n    Args:\n        output_dir (str): The directory to write the output to.\n    Returns:\n        str: The absolute path of the provided output_dir if it exists or can be created.\n    \"\"\"\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f'Using output directory: {output_dir}')\n    return output_dir",
        "output": "Calls made in `get_output_dir`:os.path.abspath, os.makedirs, logging.info\n\nPurpose and Significance:\nIn get_default_model_config, 'inference_function' refers to the method name for the specific class used by a certain library, 'from_pretrained' in this case, that needs to be called on the model object to load it from a pre-trained checkpoint. The other parameters are specific to the GPT-J model being used (e.g., batch size, max_new_tokens) and depend on hardware/resources available.\r\n\nIn get_start_dir, 'start_dir' is used to determine the starting directory for searching files. If it exists or can be created, then os.getcwd() will be set as the start_dir, otherwise the current working directory will be used.\r\n\nIn get_output_dir, 'output_dir' is used to determine where the output should be written to. It first checks if the provided directory exists or can be created and returns an absolute path for it. If not, then OUTPUT_DIR is returned instead (which defaults to 'datasets').\r\n\nIn get_questions, a default question list is loaded from the QUESTIONS_FILE in the current working directory. If the provided questions_pathname file does not exist or cannot be parsed as JSON, then the default questions are used.\r\n\nIn instantiate_model, it takes in the 'inference_model' parameter (a dictionary of configuration settings for instantiating a model) and tries to import and call the specified class from the given module. If there is an error, None is returned.\r\n\nIn get_model, if no model_config_pathname is provided or it cannot be parsed as YAML, then the default model config is used instead. The 'inference_model' parameter is then loaded and instantiated using the specified class from the given module. If there is an error, None is returned.\r\n\nIn write_questions_file, if no output_dir is provided or it does not exist, then the default questions are written to a file named QUESTIONS_FILE in the current working directory.\r\n\nIn write_model_config_file, if no output_dir is provided or it does not exist, then the default model configuration (specifying the prompt template) is written to MODEL_CONFIG_FILE in the current working directory. "
    },
    {
        "instruction": "Calls made in `get_questions` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_questions(questions_pathname: str) -> List[Dict]:\n    \"\"\"\n    Get questions from file or default\n    Args:\n        questions_pathname (str): The pathname of the questions file\n    Returns:\n        List[Dict]: The list of questions\n    \"\"\"\n    try:\n        if not questions_pathname:\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\n        with open(questions_pathname, 'r') as f:\n            questions = json.load(f)\n        logging.info(f'Using questions from file: {questions_pathname}')\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\n        logging.info(f'Questions file not valid: {questions_pathname} Using default questions')\n        questions = get_default_questions()\n    return questions",
        "output": "Calls made in `get_questions`:os.path.join, os.getcwd, open, json.load, logging.info, get_default_questions\n\nPurpose and Significance:\nThe `get_questions` function is responsible for obtaining the questions from a provided file or default questions if no valid file is found. It first tries to load the questions from the provided pathname and falls back to the default questions if it fails. The `os.path.join`, `os.getcwd`, `open`, and `json.load` functions are used for this purpose, while `logging.info` is used to log messages about which file is being used.\r\n\nThe `instantiate_model` function is responsible for importing a model based on the provided configuration dictionary and returning an instance of that class if successful or None if unsuccessful. It first tries to import the specified module using `importlib.import_module` and then instantiates the model with `inference_function_name` if it exists, otherwise uses `__init__`.\r\n\nThe `get_model` function is responsible for obtaining a model based on the provided configuration file or default configuration. It first tries to load the specified configuration from the pathname and falls back to the default config if loading fails. Then it calls the `instantiate_model` function to create an instance of that model and returns it along with the prompt template.\r\n\nThe `write_questions_file` function writes the default questions to a file in JSON format at the specified output directory or the current working directory if no valid output directory is provided. It uses `os.path.join`, `open`, and `json.dump` for this purpose.\r\n\nThe `write_model_config_file` function writes the default model configuration to a file in YAML format at the specified output directory or the current working directory if no valid output directory is provided. It uses `os.path.join` and `yaml.dump` for this purpose. "
    },
    {
        "instruction": "Calls made in `instantiate_model` in Python file: `py2dataset.get_params.py`?",
        "input": "def instantiate_model(model_config: Dict) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): model configuration dictionary.\n    Returns:\n        object: An instance of the specified model class, or None if error.\n    \"\"\"\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\n        model_params = model_config['model_params']\n        inference_function_name = model_config['model_inference_function']\n        if inference_function_name != '':\n            inference_function = getattr(ModelClass, inference_function_name)\n            llm = inference_function(model_params.pop('model_path'), **model_params)\n        else:\n            llm = ModelClass(model_params.pop('model_path'), **model_params)\n        return llm\n    except (ImportError, AttributeError, Exception) as e:\n        logging.info(f'Failed to instantiate the model. Error: {e}')\n        return None",
        "output": "Calls made in `instantiate_model`:model_config['model_import_path'].rsplit, getattr, importlib.import_module, inference_function, model_params.pop, ModelClass, logging.info\n\nPurpose and Significance:\nThe purpose of the `get_default_questions()` function is to return a list of default questions in dictionary format that are used by py2dataset to ask prompts for each code file. Each question has an id, text and type attribute. The `id` attribute is a unique identifier for the question, while the `text` attribute is the actual prompt that will be asked to the model, and the `type` attribute specifies whether it's asking about a file, function or class.\r\n\nThe purpose of the `get_default_model_config()` function is to return a dictionary representing the default configuration for the model. The `prompt_template` key contains the prompt that will be used by py2dataset to generate the prompts, while the `inference_model` key contains the import path for the class in `ctransforms` which should be used to generate predictions. In addition to the inference_function attribute which is used to instantiate the model, there are other parameters such as `model_path`, `model_type`, and `local_files_only`. These parameters are specific to the `AutoModelForCausalLM` class from `ctransforms`.\r\n\nThe purpose of the `get_output_dir()` function is to get an appropriate output directory based on a provided argument or default one. If no argument is provided, it will return the current working directory.\r\n\nThe `get_questions()` function is used to obtain questions from a file if provided, otherwise it returns the default questions obtained from `get_default_questions()`. It also validates the provided question file by trying to load it as JSON and raising an error if it fails. If the file is not found or invalid, it will return the default questions.\r\n\nThe `instantiate_model()` function takes a model configuration dictionary and imports the specified module and class from the `ctransforms` library using the `importlib` library. It then instantiates the model using the provided parameters in the `model_params` key of the dictionary, and returns it.\r\n\nThe `get_model()` function is used to obtain a model configuration from a file if provided, otherwise it uses the default one obtained from `get_default_model_config()`. It also imports the specified class and instantiates it using the provided parameters in the configuration. The `inference_function` key specifies which method of the imported class should be used to instantiate the model.\r\n\nThe `write_questions_file()` function is used to write the default questions to a JSON file with a default filename.\r\nThe `write_model_config_file()` function does the same as above for a YAML format, but writes the default configuration to a file named `py2dataset_model_config.yaml`\r\n\nIn terms of significance, `get_default_questions()` and `get_default_model_config()` are essential functions that provide py2dataset with the necessary information for generating prompts and predictions. The `instantiate_model()` function is used to create an instance of a model from the specified configuration, which will be used by py2dataset to generate predictions.\r\n\nThe `get_output_dir()` function determines where to write output files such as the generated dataset or questions file. It either takes an argument or defaults to the current working directory if none is provided.\r\n\nThe `write_questions_file()` and `write_model_config_file()` functions are used to provide convenience to the user, by providing them with default values they can customize before generating their dataset or use later for different experiments. If an argument isn't specified for either of these functions, they will write their output files in the current working directory. "
    },
    {
        "instruction": "Calls made in `get_model` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_model(model_config_pathname: str) -> tuple[object, str]:\n    \"\"\"\n    Returns an instantiated model and prompt template based on the model configuration.\n    Agrs:\n        model_config_pathname (str): The pathname of the model config file\n    Returns:\n        Tuple[object, str]: The instantiated model and prompt template\n    \"\"\"\n    try:\n        if not model_config_pathname:\n            model_config_pathname = os.path.join(os.getcwd(), MODEL_CONFIG_FILE)\n        with open(model_config_pathname, 'r') as config_file:\n            model_config = yaml.safe_load(config_file)\n        logging.info(f'Using model config from file: {model_config_pathname}')\n    except (FileNotFoundError, yaml.YAMLError):\n        logging.info(f'Model config file not valid: {model_config_pathname} Using default model config')\n        model_config = get_default_model_config()\n    model_config['model'] = instantiate_model(model_config['inference_model'])\n    return model_config",
        "output": "Calls made in `get_model`:os.path.join, os.getcwd, open, yaml.safe_load, logging.info, get_default_model_config, instantiate_model\n\nPurpose and Significance:\nThe `get_model` function calls the `yaml.safe_load()` function to read the configuration file, and also handles errors related to file loading and instantiation of a model based on the given configurations.\r\n\nThe `write_questions_file` function writes the default questions to a JSON file in the specified directory or current working directory if no output directory is provided. \r\n\nThe `write_model_config_file` function writes the default model configuration to a YAML file in the specified directory or current working directory if no output directory is provided.\r\n\nThe `get_default_questions` function returns a list of default questions that are used by the py2dataset library for generating datasets. \r\n\nThe `get_default_model_config` function returns a dictionary representing the default model configuration, which includes the prompt template and parameters to be used while generating datasets using the specified model.\r\n\nThe `instantiate_model` function imports the specified module and class from the provided model_config dictionary and instantiates it. \r\n\nThe `get_output_dir` function checks if a valid output directory is provided or creates a default one. If an invalid pathname is provided, it sets the current working directory as the output directory.\r\n\nThe `get_questions` function either reads questions from a specified file or uses the default ones provided by `get_default_questions`. \r\n\nOverall, these functions are used to obtain data parameters and model configuration for generating datasets using py2dataset. "
    },
    {
        "instruction": "Calls made in `write_questions_file` in Python file: `py2dataset.get_params.py`?",
        "input": "def write_questions_file(output_dir: str='') -> None:\n    \"\"\"\n    Writes the default questions to a file in JSON format.\n    Args:\n        output_dir (str): The directory to write the questions file to.\n    Returns:\n        None\n    \"\"\"\n    questions = get_default_questions()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, QUESTIONS_FILE), 'w') as file:\n        json.dump(questions, file, indent=4)",
        "output": "Calls made in `write_questions_file`:get_default_questions, Path(output_dir).is_dir, Path, os.getcwd, open, os.path.join, json.dump\n\nPurpose and Significance:\nThe purpose of this context is to provide a basic implementation for obtaining data parameter and model from the py2dataset functions. It provides the following objects with their corresponding roles in the code:\n- `get_default_questions()` returns the default list of questions. \n- `get_default_model_config()` returns the default dictionary representing the model configuration.\n- `get_output_dir(output_dir)` accepts an optional output_dir argument and returns the absolute path of the provided output_dir if it exists or can be created. If not, it will return the default OUTPUT_DIR (defined in this context).\n- `get_questions()` accepts an optional questions_pathname argument and validates the question file provided by the questions_pathname. It returns the questions from the provided questions_pathname if valid or default questions if the questions_pathname is not provided or invalid. \n- `instantiate_model(model_config)` imports the specified module and class from the model_config and instantiates a model using the provided configuration.\n- `get_model()` accepts an optional model_config_pathname argument, validates the model config file provided by the model_config_pathname, returns an instantiated model and a prompt template based on the provided configuration or default model configuration if the model_config_pathname is not provided or invalid.\n- `write_questions_file(output_dir)` accepts an optional output_dir argument and writes the default questions to the QUESTIONS_FILE in the specified directory. If no output_dir is provided, it will write the default questions to the QUESTIONS_FILE in the current working directory.\n- `write_model_config_file(output_dir)` accepts an optional output_dir argument and writes the default model configuration to the MODEL_CONFIG_FILE in the specified directory. If no output_dir is provided, it will write the default model configuration to the MODEL_CONFIG_FILE in the current working directory. "
    },
    {
        "instruction": "Calls made in `write_model_config_file` in Python file: `py2dataset.get_params.py`?",
        "input": "def write_model_config_file(output_dir: str='') -> None:\n    \"\"\"\n    Writes the default model config to a file in YAML format.\n    Args:\n        output_dir (str): The directory to write the model config file to.\n    Returns:\n        None\n    \"\"\"\n    model_config = get_default_model_config()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, MODEL_CONFIG_FILE), 'w') as file:\n        yaml.dump(model_config, file)",
        "output": "Calls made in `write_model_config_file`:get_default_model_config, Path(output_dir).is_dir, Path, os.getcwd, open, os.path.join, yaml.dump\n\nPurpose and Significance:\nThe purpose of this code is to provide the default configuration and model instantiation functions for the `py2dataset` library. \n\nThese objects within the code are used as follows:\n- The `get_default_model_config()` function returns a dictionary containing the default prompt template and model configuration parameters, which are then used by the `instantiate_model()` function to instantiate an instance of the specified model class using the provided configuration. \n- The `write_model_config_file()` function is used to write the default model config (a YAML file) to a given directory if the path is valid or to the current working directory if it is not provided or invalid. This is done by first checking if the output directory exists using `Path(output_dir).is_dir()`, then creating an instance of the `yaml` module to load and dump the configuration to file in YAML format using the `open()` function and the `yaml.dump()` method.\n- The `write_questions_file()` function is used to write the default questions (in JSON format) to a given directory if it exists or to the current working directory if it is not provided or invalid. This is done by first checking if the output directory exists using `Path(output_dir).is_dir()`, then creating an instance of the `json` module to load and dump the questions to file in JSON format using the `open()` function and the `json.dump()` method. "
    },
    {
        "instruction": "Variables defined in `get_default_questions` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_default_questions() -> List[Dict]:\n    \"\"\"Return default question list\n    Args:\n        None\n    Returns:\n        List[Dict]: The default question list\n    \"\"\"\n    questions = [{'id': 'file_dependencies', 'text': \"Dependencies of Python file: '{filename}'?\", 'type': 'file'}, {'id': 'entire_code_graph', 'text': \"Call code graph of Python file: '{filename}'?\", 'type': 'file'}, {'id': 'file_functions', 'text': \"Functions defined in Python file: '{filename}'?\", 'type': 'file'}, {'id': 'file_classes', 'text': \"Classes defined in Python file: '{filename}'?\", 'type': 'file'}, {'id': 'function_inputs', 'text': \"Inputs to function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'function_docstring', 'text': \"Docstring of function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'function_calls', 'text': \"Calls made in function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'function_variables', 'text': \"Variables defined in function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'function_returns', 'text': \"Returns from function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'class_methods', 'text': \"Methods defined in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'class_docstring', 'text': \"Docstring of class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'class_attributes', 'text': \"Attributes of class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'class_variables', 'text': \"Variables defined in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'class_inheritance', 'text': \"Inheritance of class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'method_inputs', 'text': \"Inputs to method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'method'}, {'id': 'method_docstring', 'text': \"Docstring of method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'method'}, {'id': 'method_calls', 'text': \"Calls made in method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'method'}, {'id': 'method_returns', 'text': \"Returns from method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'method'}, {'id': 'file_purpose', 'text': \"1) Describe the Purpose and Processing Summary of Python file: '{filename}'; 2) Provide an itemized and detailed description of each applicable function, class, and method; 3) Explain what each input, output, and variable does in the code.\", 'type': 'file'}]\n    return questions",
        "output": "Variables defined in `get_default_questions`:questions\n\nPurpose and Significance:\nThe `get_default_questions` function returns a list of default questions that are used to prompt the model for answers. Each question is represented as a dictionary with three keys: \"id\", \"text\" and \"type\". The \"id\" key represents a unique identifier for each question, \"text\" contains the actual text of the question, and \"type\" specifies whether it's asking about a file, function, class or method. "
    },
    {
        "instruction": "Variables defined in `get_default_model_config` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_default_model_config() -> Dict:\n    \"\"\"Return default model config dict\n    Args:\n        None\n    Returns:\n        Dict: The default model config dictionary\n    \"\"\"\n    model_config = {'prompt_template': \"You are an AI coding instructor. Given this Context:\\n'{context}'\\n\\nPlease provide a very detailed, accurate, and insightful Response to this Instruction and include your reasoning step by step. \\n### Instruction:\\n{query}\\n### Response:\", 'inference_model': {'model_import_path': 'ctransformers.AutoModelForCausalLM', 'model_inference_function': 'from_pretrained', 'model_params': {'model_path': 'TheBloke/WizardCoder-Python-13B-V1.0-GGUF', 'model_type': 'llama', 'local_files_only': False, 'threads': 16, 'batch_size': 128, 'context_length': 12000, 'max_new_tokens': 12000, 'gpu_layers': 100, 'reset': True}}}\n    return model_config",
        "output": "Variables defined in `get_default_model_config`:model_config\n\nPurpose and Significance:\nThe `get_default_model_config` function returns a dictionary representing the default model configuration. The `model_config` variable is used to store this dictionary, which contains the prompt template and the parameters for instantiating an inference model from the `ctransformers` library using the specified `inference_function`. This model is used later on in the `get_model` function to instantiate a GPT-2 model. "
    },
    {
        "instruction": "Variables defined in `get_start_dir` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_start_dir(start_dir: str='') -> str:\n    \"\"\"\n    Returns the appropriate start directory.\n    Args:\n        start_dir (str): The directory to start the search from.\n    Returns:\n        str: The absolute path of the provided start_dir if it exists or can be created.\n    \"\"\"\n    if start_dir and (not Path(start_dir).is_dir()):\n        logging.info(f'Setting Start Dir : {start_dir}')\n        start_dir = os.getcwd()\n    else:\n        start_dir = os.path.abspath(start_dir)\n    return start_dir",
        "output": "Variables defined in `get_start_dir`:start_dir\n\nPurpose and Significance:\nThe `get_start_dir` function is used to obtain the appropriate start directory for the project. It checks if the provided argument, `start_dir`, is a valid directory or not and returns it as an absolute path if it exists, otherwise returns the current working directory. \r\n\nSignificance: It allows users to set a starting directory to look for Python files.\r\n\n```python\r\ndef get_output_dir(output_dir: str) -> str:\r\n    \"\"\"Returns the appropriate output directory.\r\n    Args:\r\n        output_dir (str): The directory to write the output to.\r\n    Returns:\r\n        str: The absolute path of the provided output_dir if it exists or can be created.\r\n    \"\"\"\r\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\r\n    os.makedirs(output_dir, exist_ok=True)\r\n    logging.info(f\"Using output directory: {output_dir}\")\r\n    return output_dir\r\n```\r\nThis function takes an optional `output_dir` argument and returns the absolute path of that directory if it exists or creates a new directory with the default name `OUTPUT_DIR`. It also logs the output directory being used.\r\n\nSignificance: This function is used to set the output directory for the generated datasets. If no output directory is provided, it will create one named `datasets` in the current working directory.\r\n```python\r\ndef get_questions(questions_pathname: str) -> List[Dict]:\r\n    \"\"\"\r\n    Get questions from file or default\r\n    Args:\r\n        questions_pathname (str): The pathname of the questions file\r\n    Returns:\r\n        List[Dict]: The list of questions\r\n    \"\"\"\r\n    try:  # get questions from provided or default configuration file\r\n        if not questions_pathname:\r\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\r\n        with open(questions_pathname, \"r\") as f:\r\n            questions = json.load(f)\r\n        logging.info(f\"Using questions from file: {questions_pathname}\")\r\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\r\n        logging.info(\r\n            f\"Questions file not valid: {questions_pathname} Using default questions\"\r\n        )\r\n        questions = get_default_questions()\r\n    return questions\r\n```\r\nThis function takes an optional `questions_pathname` argument, reads it using JSON, and returns a list of questions from that file if it is valid. Otherwise, it uses the default `QUESTIONS_FILE` located in the current working directory.\r\n\nSignificance: This function allows users to provide their own customized question set or use the default one provided by the library.\r\n```python\r\ndef instantiate_model(model_config: Dict) -> object:\r\n    \"\"\"\r\n    Imports and instantiates a model based on the provided configuration.\r\n    Args:\r\n        model_config (dict): model configuration dictionary.\r\n    Returns:\r\n        object: An instance of the specified model class, or None if error.\r\n    \"\"\"\r\n    try:\r\n        module_name, class_name = model_config[\"model_import_path\"].rsplit(\".\", 1)\r\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\r\n        model_params = model_config[\"model_params\"]\r\n        inference_function_name = model_config[\"model_inference_function\"]\r\n        if inference_function_name != \"\":\r\n            inference_function = getattr(ModelClass, inference_function_name)\r\n            llm = inference_function(model_params.pop(\"model_path\"), **model_params)\r\n        else:\r\n            llm = ModelClass(model_params.pop(\"model_path\"), **model_params)\r\n        return llm\r\n    except (ImportError, AttributeError, Exception) as e:\r\n        logging.info(f\"Failed to instantiate the model. Error: {e}\")\r\n        return None\r\n```\r\nThis function takes a `model_config` dictionary and imports the specified module and class from it. It then creates an instance of the class with the given parameters and returns it.\r\n\nSignificance: This is the main function to load models using Hugging Face's Transformers library. The `inference_function` parameter allows users to specify a specific inference function for their model, such as `from_pretrained`, `from_pt`, or `from_tf`. If no `inference_function` is provided, it will use the default constructor.\r\n```python\r\ndef get_model(model_config_pathname: str) -> tuple[object, str]:\r\n    \"\"\"\r\n    Returns an instantiated model and prompt template based on the model configuration.\r\n    Agrs:\r\n        model_config_pathname (str): The pathname of the model config file\r\n    Returns:\r\n        Tuple[object, str]: The instantiated model and prompt template\r\n    \"\"\"\r\n    try:\r\n        if not model_config_pathname:\r\n            model_config_pathname = os.path.join(os.getcwd(), MODEL_CONFIG_FILE)\r\n        with open(model_config_pathname, \"r\") as config_file:\r\n            model_config = yaml.safe_load(config_file)\r\n        logging.info(f\"Using model config from file: {model_config_pathname}\")\r\n    except (FileNotFoundError, yaml.YAMLError):\r\n        logging.info(\r\n            f\"Model config file not valid: {model_config_pathname} Using default model config\"\r\n        )\r\n        model_config = get_default_model_config()\r\n    model_config[\"model\"] = instantiate_model(model_config[\"inference_model\"])\r\n    return model_config\r\n```\r\nThis function takes an optional `model_config_pathname` argument, reads it using YAML, and returns a tuple containing the instantiated model and prompt template based on that configuration. If no `model_config_pathname` is provided or if the file is invalid, it will use the default `MODEL_CONFIG_FILE` located in the current working directory.\r\n\nSignificance: This function loads a Hugging Face's transformer model from a specific pre-trained checkpoint and sets up the prompt template for generating answers to questions. Users can provide their own configuration file or use the default one provided by the library. If the provided pathname is not found or the yaml package raises an exception, it uses the `get_default_model_config` function to create a new instance of the model and prompt template.\r\n```python\r\ndef write_questions_file(output_dir: str) -> None:\r\n    \"\"\"\r\n    Writes the default questions to a file in JSON format.\r\n    Args:\r\n        output_dir (str): The directory to write the questions file to.\r\n    Returns:\r\n        None\r\n    \"\"\"\r\n    questions = get_default_questions()\r\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\r\n    with open(os.path.join(output_dir, QUESTIONS_FILE), \"w\") as file:\r\n        json.dump(questions, file, indent=4)\r\n```\r\nThis function creates a default question list and writes it to the `QUESTIONS_FILE` in the specified directory or current working directory if no output directory is provided.\r\n\nSignificance: This function allows users to create their own customized questions file with their desired format. If no output directory is provided, it will write the default questions to a JSON file named `py2dataset_questions.json` in the current working directory.\r\n```python\r\ndef write_model_config_file(output_dir: str) -> None:\r\n    \"\"\"\r\n    Writes the default model config to a file in YAML format.\r\n    Args:\r\n        output_dir (str): The directory to write the model config file to.\r\n    Returns:\r\n        None\r\n    \"\"\"\r\n    model_config = get_default_model_config()\r\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\r\n    with open(os.path.join(output_dir, MODEL_CONFIG_FILE), \"w\") as file:\r\n        yaml.dump(model_config, file)\r\n```\r\nThis function creates a default model configuration dictionary and writes it to the `MODEL_CONFIG_FILE` in the specified directory or current working directory if no output directory is provided.\r\n\nSignificance: This function allows users to create their own customized model configuration file with their desired parameters. If no output directory is provided, it will write the default model configuration to a YAML file named `py2dataset_model_config.yaml` in the current working directory. "
    },
    {
        "instruction": "Variables defined in `get_output_dir` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_output_dir(output_dir: str='') -> str:\n    \"\"\"Returns the appropriate output directory.\n    Args:\n        output_dir (str): The directory to write the output to.\n    Returns:\n        str: The absolute path of the provided output_dir if it exists or can be created.\n    \"\"\"\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f'Using output directory: {output_dir}')\n    return output_dir",
        "output": "Variables defined in `get_output_dir`:output_dir\n\nPurpose and Significance:\nThe `get_output_dir` function is used to obtain an output directory. If a valid output directory is provided, it returns its absolute path; otherwise, it creates a default directory called \"datasets\" and returns the absolute path of that directory. "
    },
    {
        "instruction": "Variables defined in `get_questions` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_questions(questions_pathname: str) -> List[Dict]:\n    \"\"\"\n    Get questions from file or default\n    Args:\n        questions_pathname (str): The pathname of the questions file\n    Returns:\n        List[Dict]: The list of questions\n    \"\"\"\n    try:\n        if not questions_pathname:\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\n        with open(questions_pathname, 'r') as f:\n            questions = json.load(f)\n        logging.info(f'Using questions from file: {questions_pathname}')\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\n        logging.info(f'Questions file not valid: {questions_pathname} Using default questions')\n        questions = get_default_questions()\n    return questions",
        "output": "Variables defined in `get_questions`:questions_pathname, questions\n\nPurpose and Significance:\nThe function `get_questions()` takes an optional `questions_pathname` argument and validates the question file provided by that pathname. If the provided `questions_pathname` is not provided or invalid, it returns the default questions instead.\r\n\n'Variables defined in `instantiate_model`: 'model_config', 'ModelClass', 'inference_function_name', and 'llm'. \r\nThe function imports a module based on the specified model path and class name from the provided `model_config` dictionary, instantiates the model using the specified parameters, and returns it. If there is an error during import or instantiation, it returns None.\r\n\n'Variables defined in `get_model`: 'model_config', 'model_pathname', and 'prompt_template'. \r\nThe function takes an optional `model_config_pathname` argument and validates the model config file provided by that pathname. If the provided `model_config_pathname` is not provided or invalid, it returns the default configuration and a prompt template.\r\n\n'Variables defined in `write_questions_file`: 'output_dir'\r\nThe function writes the default questions to the `QUESTIONS_FILE` file in the specified directory. If the output directory argument is not provided or invalid, it writes to the current working directory.\r\n\n'Variables defined in `write_model_config_file`: 'output_dir' \r\nThe function writes the default model configuration to the `MODEL_CONFIG_FILE` file in the specified directory. If the output directory argument is not provided or invalid, it writes to the current working directory.\r\n\nOverall, these functions are used to obtain data parameters and model from the py2dataset library. "
    },
    {
        "instruction": "Variables defined in `instantiate_model` in Python file: `py2dataset.get_params.py`?",
        "input": "def instantiate_model(model_config: Dict) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): model configuration dictionary.\n    Returns:\n        object: An instance of the specified model class, or None if error.\n    \"\"\"\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\n        model_params = model_config['model_params']\n        inference_function_name = model_config['model_inference_function']\n        if inference_function_name != '':\n            inference_function = getattr(ModelClass, inference_function_name)\n            llm = inference_function(model_params.pop('model_path'), **model_params)\n        else:\n            llm = ModelClass(model_params.pop('model_path'), **model_params)\n        return llm\n    except (ImportError, AttributeError, Exception) as e:\n        logging.info(f'Failed to instantiate the model. Error: {e}')\n        return None",
        "output": "Variables defined in `instantiate_model`:inference_function_name, inference_function, model_params, llm, ModelClass\n\nPurpose and Significance:\nThe `instantiate_model` function is responsible for importing the specified module and class from the provided model configuration dictionary. It accepts a model_config dictionary as an argument which contains information about the module path, class name, inference function name, and model parameters to be used to instantiate the model. The following objects are returned by this function:\n- `inference_function_name` - the name of the inference function specified in the model configuration file (e.g., \"from_pretrained\")\n- `inference_function` - the actual inference function imported from the module and class\n- `model_params` - a dictionary containing all the parameters required to instantiate the model, except for the `model_path` key which is used separately by the `llm` object.\n- `ModelClass` - the class object that will be used to create an instance of the specified model. "
    },
    {
        "instruction": "Variables defined in `get_model` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_model(model_config_pathname: str) -> tuple[object, str]:\n    \"\"\"\n    Returns an instantiated model and prompt template based on the model configuration.\n    Agrs:\n        model_config_pathname (str): The pathname of the model config file\n    Returns:\n        Tuple[object, str]: The instantiated model and prompt template\n    \"\"\"\n    try:\n        if not model_config_pathname:\n            model_config_pathname = os.path.join(os.getcwd(), MODEL_CONFIG_FILE)\n        with open(model_config_pathname, 'r') as config_file:\n            model_config = yaml.safe_load(config_file)\n        logging.info(f'Using model config from file: {model_config_pathname}')\n    except (FileNotFoundError, yaml.YAMLError):\n        logging.info(f'Model config file not valid: {model_config_pathname} Using default model config')\n        model_config = get_default_model_config()\n    model_config['model'] = instantiate_model(model_config['inference_model'])\n    return model_config",
        "output": "Variables defined in `get_model`:model_config_pathname, model_config\n\nPurpose and Significance:\nThe `get_model` function accepts an optional `model_config_pathname` argument. If it is provided, it validates the model config file provided by the `model_config_pathname`. If the `model_config_pathname` is not provided or invalid, it returns an instantiated model and a prompt template based on the default model configuration.\r\n\nThe `model_config_pathname` variable is used to specify the path of the YAML file containing the model configuration. The function then imports the specified module and class from the model_config dictionary using the `inference_model` key, which contains the import path for the model and the inference function name. It instantiates the model using the provided configuration by calling the specified inference function on the imported class with the corresponding parameters.\r\n\nThe default model configuration is obtained using the `get_default_model_config()` function, which returns a dictionary representing the default model configuration. The `inference_model` key of this dictionary specifies the module and class name for importing the required model while `model_params` contains additional parameters required by that model.\r\n\nFinally, it returns an instantiated model along with a prompt template, which is stored as a string in the default model configuration under the `prompt_template` key. "
    },
    {
        "instruction": "Variables defined in `write_questions_file` in Python file: `py2dataset.get_params.py`?",
        "input": "def write_questions_file(output_dir: str='') -> None:\n    \"\"\"\n    Writes the default questions to a file in JSON format.\n    Args:\n        output_dir (str): The directory to write the questions file to.\n    Returns:\n        None\n    \"\"\"\n    questions = get_default_questions()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, QUESTIONS_FILE), 'w') as file:\n        json.dump(questions, file, indent=4)",
        "output": "Variables defined in `write_questions_file`:output_dir, questions\n\nPurpose and Significance:\nThe `write_questions_file` function takes an optional output directory argument. If provided, it writes the default questions to a file named `py2dataset_questions.json` in that directory. Otherwise, it writes the default questions to the current working directory.\r\n\nThe `output_dir` variable is used to determine where to write the file. It first checks if the output directory exists or can be created and then returns its absolute path using `os.path.abspath`. If no output directory is provided, it defaults to the current working directory. "
    },
    {
        "instruction": "Variables defined in `write_model_config_file` in Python file: `py2dataset.get_params.py`?",
        "input": "def write_model_config_file(output_dir: str='') -> None:\n    \"\"\"\n    Writes the default model config to a file in YAML format.\n    Args:\n        output_dir (str): The directory to write the model config file to.\n    Returns:\n        None\n    \"\"\"\n    model_config = get_default_model_config()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, MODEL_CONFIG_FILE), 'w') as file:\n        yaml.dump(model_config, file)",
        "output": "Variables defined in `write_model_config_file`:output_dir, model_config\n\nPurpose and Significance:\nThe `write_model_config_file` function writes the default model configuration to a YAML file named `py2dataset_model_config.yaml`. The `output_dir` argument is used to specify where to write this file if it is provided, otherwise, the file will be written to the current working directory.\r\n\nThe `model_config` dictionary contains the default model configuration that can be modified by the user and passed as an optional argument to the function. If no `model_config` is provided, then the function writes the default configuration to the YAML file. "
    },
    {
        "instruction": "Returns from `get_default_questions` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_default_questions() -> List[Dict]:\n    \"\"\"Return default question list\n    Args:\n        None\n    Returns:\n        List[Dict]: The default question list\n    \"\"\"\n    questions = [{'id': 'file_dependencies', 'text': \"Dependencies of Python file: '{filename}'?\", 'type': 'file'}, {'id': 'entire_code_graph', 'text': \"Call code graph of Python file: '{filename}'?\", 'type': 'file'}, {'id': 'file_functions', 'text': \"Functions defined in Python file: '{filename}'?\", 'type': 'file'}, {'id': 'file_classes', 'text': \"Classes defined in Python file: '{filename}'?\", 'type': 'file'}, {'id': 'function_inputs', 'text': \"Inputs to function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'function_docstring', 'text': \"Docstring of function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'function_calls', 'text': \"Calls made in function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'function_variables', 'text': \"Variables defined in function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'function_returns', 'text': \"Returns from function: '{function_name}' in Python file: '{filename}'?\", 'type': 'function'}, {'id': 'class_methods', 'text': \"Methods defined in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'class_docstring', 'text': \"Docstring of class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'class_attributes', 'text': \"Attributes of class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'class_variables', 'text': \"Variables defined in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'class_inheritance', 'text': \"Inheritance of class: '{class_name}' in Python file: '{filename}'?\", 'type': 'class'}, {'id': 'method_inputs', 'text': \"Inputs to method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'method'}, {'id': 'method_docstring', 'text': \"Docstring of method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'method'}, {'id': 'method_calls', 'text': \"Calls made in method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'method'}, {'id': 'method_returns', 'text': \"Returns from method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\", 'type': 'method'}, {'id': 'file_purpose', 'text': \"1) Describe the Purpose and Processing Summary of Python file: '{filename}'; 2) Provide an itemized and detailed description of each applicable function, class, and method; 3) Explain what each input, output, and variable does in the code.\", 'type': 'file'}]\n    return questions",
        "output": "Returns from `get_default_questions`:questions\n\nPurpose and Significance:\nThe `get_default_questions` function returns a list of default questions in dictionary format. Each question has an id, text, and type key.\r\n\n'Returns from `get_default_model_config`: 'model_config' is a dictionary that contains the prompt template and model configuration parameters.'\r\n\nThe `get_output_dir` function accepts an optional output directory argument and returns the absolute path of the provided output directory if it exists or can be created. If no output directory is provided, it will return the default OUTPUT_DIR.\r\n\n'Returns from `get_questions`: 'If a valid questions file is provided, it will read in the questions and return them as a list of dictionaries. Otherwise, it returns default questions.' The output includes validation, parsing for path, ensuring files can be used in an automated workflow and provides functionality when an output directory isn't passed.\r\n\nThe `instantiate_model` function accepts a model configuration dictionary and imports the specified module and class from the model_config. It then instantiates the model using the provided configuration and returns it.\r\n\n'Returns from `get_model`: 'If a valid model config file is provided, it will read in the model configuration and instantiate the model based on that file. Otherwise, it uses default parameters.' The output includes validation, parsing for path, ensuring files can be used in an automated workflow and provides functionality when no model config file is passed.\r\n\nThe `write_questions_file` function accepts an optional output directory argument and writes the default questions to a JSON file named QUESTIONS_FILE. If no output directory is provided, it will write the file to the current working directory.\r\n\nThe `write_model_config_file` function also accepts an optional output directory argument and writes the default model configuration in YAML format to MODEL_CONFIG_FILE. If no output directory is provided, it will write the file to the current working directory.\r\n\nNote: this context provides the structure for creating a Python data parameter that can be used with `py2dataset` functions. "
    },
    {
        "instruction": "Returns from `get_default_model_config` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_default_model_config() -> Dict:\n    \"\"\"Return default model config dict\n    Args:\n        None\n    Returns:\n        Dict: The default model config dictionary\n    \"\"\"\n    model_config = {'prompt_template': \"You are an AI coding instructor. Given this Context:\\n'{context}'\\n\\nPlease provide a very detailed, accurate, and insightful Response to this Instruction and include your reasoning step by step. \\n### Instruction:\\n{query}\\n### Response:\", 'inference_model': {'model_import_path': 'ctransformers.AutoModelForCausalLM', 'model_inference_function': 'from_pretrained', 'model_params': {'model_path': 'TheBloke/WizardCoder-Python-13B-V1.0-GGUF', 'model_type': 'llama', 'local_files_only': False, 'threads': 16, 'batch_size': 128, 'context_length': 12000, 'max_new_tokens': 12000, 'gpu_layers': 100, 'reset': True}}}\n    return model_config",
        "output": "Returns from `get_default_model_config`:model_config\n\nPurpose and Significance:\nThe function `get_default_model_config()` returns a dictionary representing the default model configuration. The dictionary contains two keys, `prompt_template` and `inference_model`. The `prompt_template` key is used to generate prompts for the language model, while the `inference_model` key specifies the model class and parameters to be used for inference.\r\n\nThe `inference_model` dictionary contains two keys: `model_import_path`, which specifies the path of the model class to be imported, and `model_params`, which are the arguments required by the model class constructor. The `prompt_template` is a string that is used to generate prompts for the language model.\r\n\nExample usage: \r\n```python\r\n>>> default_config = get_default_model_config()\r\n>>> default_config['inference_model']['model_params']\r\n{'model_path': 'TheBloke/WizardCoder-Python-13B-V1.0-GGUF', 'model_type': 'llama', 'local_files_only': False, 'threads': 16, 'batch_size': 128, 'context_length': 12000, 'max_new_tokens': 12000, 'gpu_layers': 100, 'reset': True}\r\n>>> default_config['prompt_template']\r\n'You are an AI coding instructor. Given this Context:\\n{context}\\n\\nPlease provide a very detailed, accurate, and insightful Response to the following Instruction and include your reasoning step by step. \\n### Instruction: {query}  \\n### Response:'\r\n``` "
    },
    {
        "instruction": "Returns from `get_start_dir` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_start_dir(start_dir: str='') -> str:\n    \"\"\"\n    Returns the appropriate start directory.\n    Args:\n        start_dir (str): The directory to start the search from.\n    Returns:\n        str: The absolute path of the provided start_dir if it exists or can be created.\n    \"\"\"\n    if start_dir and (not Path(start_dir).is_dir()):\n        logging.info(f'Setting Start Dir : {start_dir}')\n        start_dir = os.getcwd()\n    else:\n        start_dir = os.path.abspath(start_dir)\n    return start_dir",
        "output": "Returns from `get_start_dir`:start_dir\n\nPurpose and Significance:\nThe `get_start_dir` function returns the appropriate start directory based on the input parameter. If the input parameter is not a valid directory, it will return the current working directory instead.\r\n\nThe purpose of this function is to get the starting directory for searching files recursively. The significance of this function is that it helps in defining where the search should start from when looking for Python files to analyze. "
    },
    {
        "instruction": "Returns from `get_output_dir` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_output_dir(output_dir: str='') -> str:\n    \"\"\"Returns the appropriate output directory.\n    Args:\n        output_dir (str): The directory to write the output to.\n    Returns:\n        str: The absolute path of the provided output_dir if it exists or can be created.\n    \"\"\"\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f'Using output directory: {output_dir}')\n    return output_dir",
        "output": "Returns from `get_output_dir`:output_dir\n\nPurpose and Significance:\nThe `get_output_dir` function is used to obtain an output directory path that can be used for writing files. If no valid output directory is provided, it returns a default one (named \"datasets\"). This default behavior can be modified by passing a different name or location as an argument. The output_dir is also validated before being returned, ensuring that the specified directory exists and creating it if necessary.\r\n\nThe `get_default_questions` function returns a list of dictionaries representing default questions that will be used in py2dataset for data generation. Each dictionary contains an id, text, and type field. \r\n\nThe `get_default_model_config` function returns a dictionary representing the default model configuration. This includes a prompt template and information about how to import and instantiate a specific model from the ctransformers library.\r\n\nThe `instantiate_model` function takes in a model_config dictionary, imports the specified module and class, and instantiates the model using the provided configuration. The parameters given ensure optimal performance when utilizing hardware for certain GPU cards. However, changing the specific configurations of `avx2` may depend on the user's hardware.\r\n\nThe `get_model` function takes in an optional model_config_pathname argument to obtain a dictionary representing a specified configuration from that file. If no path is provided or the configuration file is not valid, it defaults to using the default configuration.\r\n\nThe `write_questions_file` function writes the default questions to a JSON file in the output directory specified (or the current working directory if none is provided). This will be used by py2dataset to generate datasets for each question type. \r\n\nThe `write_model_config_file` function also takes an optional output_dir argument and writes the default model configuration to a YAML file in that directory, which can then be modified or customized by the user as needed. "
    },
    {
        "instruction": "Returns from `get_questions` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_questions(questions_pathname: str) -> List[Dict]:\n    \"\"\"\n    Get questions from file or default\n    Args:\n        questions_pathname (str): The pathname of the questions file\n    Returns:\n        List[Dict]: The list of questions\n    \"\"\"\n    try:\n        if not questions_pathname:\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\n        with open(questions_pathname, 'r') as f:\n            questions = json.load(f)\n        logging.info(f'Using questions from file: {questions_pathname}')\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\n        logging.info(f'Questions file not valid: {questions_pathname} Using default questions')\n        questions = get_default_questions()\n    return questions",
        "output": "Returns from `get_questions`:questions\n\nPurpose and Significance:\nThe `get_questions` function returns a list of default questions in JSON format. Each question is represented as a dictionary with three keys: \"id\", \"text\" and \"type\". The \"id\" key represents the unique identifier for each question, while \"text\" contains the prompt text that will be used by the model to generate answers, and \"type\" specifies whether the question is related to a file, function or class. "
    },
    {
        "instruction": "Returns from `instantiate_model` in Python file: `py2dataset.get_params.py`?",
        "input": "def instantiate_model(model_config: Dict) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): model configuration dictionary.\n    Returns:\n        object: An instance of the specified model class, or None if error.\n    \"\"\"\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\n        model_params = model_config['model_params']\n        inference_function_name = model_config['model_inference_function']\n        if inference_function_name != '':\n            inference_function = getattr(ModelClass, inference_function_name)\n            llm = inference_function(model_params.pop('model_path'), **model_params)\n        else:\n            llm = ModelClass(model_params.pop('model_path'), **model_params)\n        return llm\n    except (ImportError, AttributeError, Exception) as e:\n        logging.info(f'Failed to instantiate the model. Error: {e}')\n        return None",
        "output": "Returns from `instantiate_model`:llm, None\n\nPurpose and Significance:\nThe function `instantiate_model()` takes in a dictionary called `model_config`, imports the specified model using its class name and path from that configuration dictionary. The instantiated object of this imported model is returned along with the `None` value to signify an error in case any issue occurs while importing the model.\r\n\n'Returns from `get_output_dir`: 'str': Returns the absolute path of the provided output directory or creates a new one if it doesn't exist. If no output directory is provided, it returns the default OUTPUT_DIR.' "
    },
    {
        "instruction": "Returns from `get_model` in Python file: `py2dataset.get_params.py`?",
        "input": "def get_model(model_config_pathname: str) -> tuple[object, str]:\n    \"\"\"\n    Returns an instantiated model and prompt template based on the model configuration.\n    Agrs:\n        model_config_pathname (str): The pathname of the model config file\n    Returns:\n        Tuple[object, str]: The instantiated model and prompt template\n    \"\"\"\n    try:\n        if not model_config_pathname:\n            model_config_pathname = os.path.join(os.getcwd(), MODEL_CONFIG_FILE)\n        with open(model_config_pathname, 'r') as config_file:\n            model_config = yaml.safe_load(config_file)\n        logging.info(f'Using model config from file: {model_config_pathname}')\n    except (FileNotFoundError, yaml.YAMLError):\n        logging.info(f'Model config file not valid: {model_config_pathname} Using default model config')\n        model_config = get_default_model_config()\n    model_config['model'] = instantiate_model(model_config['inference_model'])\n    return model_config",
        "output": "Returns from `get_model`:model_config\n\nPurpose and Significance:\nThe `get_model` function returns a dictionary that includes the default model configuration along with an instantiated model object based on the provided arguments or defaults. The returned dictionary contains two keys: 'model', which stores an instance of the specified AI language model class from CTransformers library, and 'prompt_template', which represents a template used for creating queries. If the user doesn't provide an `model_config_pathname`, the function uses the default model configuration from the `get_default_model_config` function.\r\n\nExample:\r\n```python\r\nmodel, prompt_template = get_model()\r\nprint(type(model)) # <class 'ctransformers.models.gpt2.GPT2LMHeadModel'>\r\nprint(prompt_template) # \"You are an AI coding instructor. Given this Context:\\n{context}\\nPlease provide a very detailed, accurate, and insightful Response to this Instruction and include your reasoning step by step. \\n### Instruction:\\n{query}\\n### Response:\"\r\n``` "
    },
    {
        "instruction": "1) Describe the Purpose and Processing summary of Python file: `py2dataset.get_params.py`; 2) Summarize the Significance of applicable Function, Class, and Method; 3) Explain what each Input, Output, and Variable does in the code.",
        "input": "```python\n\"\"\"\nObtain data parameter and model from the py2dataset functions.\nRequirements:\n[req01] The `get_default_questions` function shall:\n        a. Return a list of default questions.\n        b. Ensure each question in the list is a dictionary.\n        c. Ensure each dictionary has the keys: id, text, and type.\n[req02] The `get_default_model_config` function shall:\n        a. Return a dictionary representing the default model configuration.\n[req03] The `get_output_dir` function shall:\n        a. Accept an optional output_dir argument.\n        b. Return the absolute path of the provided output_dir if it exists or can be created.\n        c. Return the default OUTPUT_DIR if the provided output_dir argument is not provided or invalid.\n[req04] The `get_questions` function shall:\n        a. Accept an optional questions_pathname argument.\n        b. Validate the question file provided by the questions_pathname.\n        c. Return the questions from the provided questions_pathname if valid.\n        d. Return default questions if the questions_pathname is not provided or invalid.\n[req05] The `instantiate_model` function shall:\n        a. Accept a model_config dictionary as an argument.\n        b. Import the specified module and class from the model_config.\n        c. Instantiate and return the model using the provided configuration.\n[req06] The `get_model` function shall:\n        a. Accept an optional model_config_pathname argument.\n        b. Validate the model config file provided by the model_config_pathname.\n        c. Return an instantiated model and a prompt template based on the provided configuration.\n        d. Return an instantiated model and a prompt template based on the default model configuration if the model_config_pathname is not provided or invalid.\n[req07] The `write_questions_file` function shall:\n        a. Accept an optional output_dir argument.\n        b. Write the default questions to the QUESTIONS_FILE in the specified directory.\n        c. Write the default questions to the QUESTIONS_FILE in the current working directory if the output_dir argument is not provided or invalid.\n[req08] The `write_model_config_file` function shall:\n        a. Accept an optional output_dir argument.\n        b. Write the default model configuration to the MODEL_CONFIG_FILE in the specified directory.\n        c. Write the default model configuration to the MODEL_CONFIG_FILE in the current working directory if the output_dir argument is not provided or invalid.\n\"\"\"\nimport os\nimport json\nimport logging\nimport importlib\nfrom typing import Dict, List\nfrom pathlib import Path\nimport yaml\n\n# Setting up a basic logger\nlogging.basicConfig(level=logging.INFO)\n\nQUESTIONS_FILE = \"py2dataset_questions.json\"\nMODEL_CONFIG_FILE = \"py2dataset_model_config.yaml\"\nOUTPUT_DIR = \"datasets\"\n\n\ndef get_default_questions() -> List[Dict]:\n    \"\"\"Return default question list\n    Args:\n        None\n    Returns:\n        List[Dict]: The default question list\n    \"\"\"\n    questions = [\n        {\n            \"id\": \"file_dependencies\",\n            \"text\": \"Dependencies of Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"entire_code_graph\",\n            \"text\": \"Call code graph of Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_functions\",\n            \"text\": \"Functions defined in Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_classes\",\n            \"text\": \"Classes defined in Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"function_inputs\",\n            \"text\": \"Inputs to function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_docstring\",\n            \"text\": \"Docstring of function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_calls\",\n            \"text\": \"Calls made in function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_variables\",\n            \"text\": \"Variables defined in function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_returns\",\n            \"text\": \"Returns from function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"class_methods\",\n            \"text\": \"Methods defined in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_docstring\",\n            \"text\": \"Docstring of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_attributes\",\n            \"text\": \"Attributes of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_variables\",\n            \"text\": \"Variables defined in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_inheritance\",\n            \"text\": \"Inheritance of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"method_inputs\",\n            \"text\": \"Inputs to method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_docstring\",\n            \"text\": \"Docstring of method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_calls\",\n            \"text\": \"Calls made in method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_returns\",\n            \"text\": \"Returns from method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"file_purpose\",\n            \"text\": \"1) Describe the Purpose and Processing Summary of Python file: '{filename}'; 2) Provide an itemized and detailed description of each applicable function, class, and method; 3) Explain what each input, output, and variable does in the code.\",\n            \"type\": \"file\",\n        },\n    ]\n    return questions\n\n\ndef get_default_model_config() -> Dict:\n    \"\"\"Return default model config dict\n    Args:\n        None\n    Returns:\n        Dict: The default model config dictionary\n    \"\"\"\n    model_config = {\n        \"prompt_template\": \"You are an AI coding instructor. Given this Context:\\n'{context}'\\n\\nPlease provide a very detailed, accurate, and insightful Response to this Instruction and include your reasoning step by step. \\n### Instruction:\\n{query}\\n### Response:\",\n        \"inference_model\": {\n            \"model_import_path\": \"ctransformers.AutoModelForCausalLM\",\n            \"model_inference_function\": \"from_pretrained\",\n            \"model_params\": {\n                \"model_path\": \"TheBloke/WizardCoder-Python-13B-V1.0-GGUF\",\n                \"model_type\": \"llama\",\n                \"local_files_only\": False,\n                ## MODEL CONFIGURATION PARAMETERS (set for current model with: GPU 4090-24GB VRAM, CPU 5950x-32 threads, 64GB RAM) \n                # avx2 and gpu_layers are not compatible\n                # \"lib\": \"avx2\",\n                \"threads\": 16,\n                \"batch_size\": 128,\n                \"context_length\": 12000,\n                \"max_new_tokens\": 12000,\n                \"gpu_layers\": 100,\n                \"reset\": True,\n            },\n        },\n    }\n    return model_config\n\n\ndef get_start_dir(start_dir: str = \"\") -> str:\n    \"\"\"\n    Returns the appropriate start directory.\n    Args:\n        start_dir (str): The directory to start the search from.\n    Returns:\n        str: The absolute path of the provided start_dir if it exists or can be created.\n    \"\"\"\n    if start_dir and not Path(start_dir).is_dir():\n        logging.info(f\"Setting Start Dir : {start_dir}\")\n        start_dir = os.getcwd()\n    else:\n        start_dir = os.path.abspath(start_dir)\n    return start_dir\n\n\ndef get_output_dir(output_dir: str = \"\") -> str:\n    \"\"\"Returns the appropriate output directory.\n    Args:\n        output_dir (str): The directory to write the output to.\n    Returns:\n        str: The absolute path of the provided output_dir if it exists or can be created.\n    \"\"\"\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f\"Using output directory: {output_dir}\")\n    return output_dir\n\n\ndef get_questions(questions_pathname: str) -> List[Dict]:\n    \"\"\"\n    Get questions from file or default\n    Args:\n        questions_pathname (str): The pathname of the questions file\n    Returns:\n        List[Dict]: The list of questions\n    \"\"\"\n    try:  # get questions from provided or default configuration file\n        if not questions_pathname:\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\n        with open(questions_pathname, \"r\") as f:\n            questions = json.load(f)\n        logging.info(f\"Using questions from file: {questions_pathname}\")\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\n        logging.info(\n            f\"Questions file not valid: {questions_pathname} Using default questions\"\n        )\n        questions = get_default_questions()\n    return questions\n\n\ndef instantiate_model(model_config: Dict) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): model configuration dictionary.\n    Returns:\n        object: An instance of the specified model class, or None if error.\n    \"\"\"\n    try:\n        module_name, class_name = model_config[\"model_import_path\"].rsplit(\".\", 1)\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\n        model_params = model_config[\"model_params\"]\n        inference_function_name = model_config[\"model_inference_function\"]\n        if inference_function_name != \"\":\n            inference_function = getattr(ModelClass, inference_function_name)\n            llm = inference_function(model_params.pop(\"model_path\"), **model_params)\n        else:\n            llm = ModelClass(model_params.pop(\"model_path\"), **model_params)\n        return llm\n    except (ImportError, AttributeError, Exception) as e:\n        logging.info(f\"Failed to instantiate the model. Error: {e}\")\n        return None\n\n\ndef get_model(model_config_pathname: str) -> tuple[object, str]:\n    \"\"\"\n    Returns an instantiated model and prompt template based on the model configuration.\n    Agrs:\n        model_config_pathname (str): The pathname of the model config file\n    Returns:\n        Tuple[object, str]: The instantiated model and prompt template\n    \"\"\"\n    try:\n        if not model_config_pathname:\n            model_config_pathname = os.path.join(os.getcwd(), MODEL_CONFIG_FILE)\n        with open(model_config_pathname, \"r\") as config_file:\n            model_config = yaml.safe_load(config_file)\n        logging.info(f\"Using model config from file: {model_config_pathname}\")\n    except (FileNotFoundError, yaml.YAMLError):\n        logging.info(\n            f\"Model config file not valid: {model_config_pathname} Using default model config\"\n        )\n        model_config = get_default_model_config()\n    model_config[\"model\"] = instantiate_model(model_config[\"inference_model\"])\n    return model_config\n\n\ndef write_questions_file(output_dir: str = \"\") -> None:\n    \"\"\"\n    Writes the default questions to a file in JSON format.\n    Args:\n        output_dir (str): The directory to write the questions file to.\n    Returns:\n        None\n    \"\"\"\n    questions = get_default_questions()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, QUESTIONS_FILE), \"w\") as file:\n        json.dump(questions, file, indent=4)\n\n\ndef write_model_config_file(output_dir: str = \"\") -> None:\n    \"\"\"\n    Writes the default model config to a file in YAML format.\n    Args:\n        output_dir (str): The directory to write the model config file to.\n    Returns:\n        None\n    \"\"\"\n    model_config = get_default_model_config()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, MODEL_CONFIG_FILE), \"w\") as file:\n        yaml.dump(model_config, file)\n\n```",
        "output": "The purpose of this Python file is to provide a structure for handling the questions, model configuration, and output directory. It also provides functions to write default questions and model configurations to files if they are not provided by the user. The `get_default_questions` function returns a list of default questions that can be used as prompts for an AI language model. The `get_default_model_config` function returns a dictionary representing the default configuration for the GPT-3 model, which is used to instantiate a model using the ctransformers library. The `get_output_dir` function sets up and validates the output directory for writing data generated by py2dataset. Finally, `write_questions_file` writes the default questions to a file called \"py2dataset_questions.json\" in the specified or current working directory, and `write_model_config_file` writes the default model configuration to a file called \"py2dataset_model_config.yaml\". \n{\n    \"Code Elements\": {\n        \"Dependencies\": \"importlib, typing, yaml, os, logging, pathlib, json\",\n        \"Functions defined\": \"get_default_questions, get_default_model_config, get_start_dir, get_output_dir, get_questions, instantiate_model, get_model, write_questions_file, write_model_config_file\",\n        \"`get_default_model_config` Returns\": \"model_config\",\n        \"`get_default_model_config` Variables\": \"model_config\",\n        \"`get_default_questions` Returns\": \"questions\",\n        \"`get_default_questions` Variables\": \"questions\",\n        \"`get_model` Calls\": \"os.path.join, os.getcwd, open, yaml.safe_load, logging.info, get_default_model_config, instantiate_model\",\n        \"`get_model` Inputs\": \"model_config_pathname\",\n        \"`get_model` Returns\": \"model_config\",\n        \"`get_model` Variables\": \"model_config_pathname, model_config\",\n        \"`get_output_dir` Calls\": \"os.path.abspath, os.makedirs, logging.info\",\n        \"`get_output_dir` Inputs\": \"output_dir\",\n        \"`get_output_dir` Returns\": \"output_dir\",\n        \"`get_output_dir` Variables\": \"output_dir\",\n        \"`get_questions` Calls\": \"os.path.join, os.getcwd, open, json.load, logging.info, get_default_questions\",\n        \"`get_questions` Inputs\": \"questions_pathname\",\n        \"`get_questions` Returns\": \"questions\",\n        \"`get_questions` Variables\": \"questions_pathname, questions\",\n        \"`get_start_dir` Calls\": \"Path(start_dir).is_dir, Path, logging.info, os.getcwd, os.path.abspath\",\n        \"`get_start_dir` Inputs\": \"start_dir\",\n        \"`get_start_dir` Returns\": \"start_dir\",\n        \"`get_start_dir` Variables\": \"start_dir\",\n        \"`instantiate_model` Calls\": \"model_config['model_import_path'].rsplit, getattr, importlib.import_module, inference_function, model_params.pop, ModelClass, logging.info\",\n        \"`instantiate_model` Inputs\": \"model_config\",\n        \"`instantiate_model` Returns\": \"llm, None\",\n        \"`instantiate_model` Variables\": \"inference_function_name, inference_function, model_params, llm, ModelClass\",\n        \"`write_model_config_file` Calls\": \"get_default_model_config, Path(output_dir).is_dir, Path, os.getcwd, open, os.path.join, yaml.dump\",\n        \"`write_model_config_file` Inputs\": \"output_dir\",\n        \"`write_model_config_file` Variables\": \"output_dir, model_config\",\n        \"`write_questions_file` Calls\": \"get_default_questions, Path(output_dir).is_dir, Path, os.getcwd, open, os.path.join, json.dump\",\n        \"`write_questions_file` Inputs\": \"output_dir\",\n        \"`write_questions_file` Variables\": \"output_dir, questions\"\n    }\n}"
    }
]