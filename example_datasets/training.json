[
    {
        "instruction": "Define a Python code file that is described as follows:\nPurpose of Python File: \nThis python file provides functions that extract code details and graph, create control flow trees for python files and convert it to plant UML representation for the visualization purposes.\r\n\nSignificance of Applicable Functions/Classes:\r\n\nFunction `code_graph()`: \r\n    - Takes a dictionary object representing the file summary as input. It creates nodes and edges for functions, class methods and their relationships based on how they call each other and returns them.\r\n    - Each function call is represented as an edge with appropriate label. If it is calling another method within a class or function, it will have additional data in the edge that includes input/output details.\r\n\nFunction `extract_control_flow_tree()`: \r\n    - Extracts control flow tree from AST. It traverses through the AST nodes and creates a list of nested dictionaries to represent the control flow structure.\r\n    - Each dictionary contains either a string or another dictionary with key-value pairs representing different control structures like if, while, for, try, etc. If it is calling other functions or methods within a class or function, those will be added as a key in the respective dictionaries.\r\n\nFunction `reorganize_control_flow()`: \r\n    - It re-arranges the structure to start with functions or class that don't have any parent or ancestors in the control flow tree. This is done recursively by checking if the current element has any parents and adding it to the organized list.\r\n\nFunction `get_plantUML_element()`: \r\n    - It converts each element of the control flow tree into a Plant UML code for better visualization and readability.\r\n\nFunction `get_plantUML()`: \r\n    - It takes the reorganized control flow structure and creates a plant UML representation of the file.\r\n\nFunction `get_code_graph()`: \r\n    - This function adds the code graph and control flow to the file details dictionary. It also removes any nodes that are not either a source or target of an edge in the code graph.\r\n\nInputs to `code_graph`:\r\n- `file_summary` : A dictionary containing various attributes extracted from python file.\r\n    - keys: name, encoding, error_lineno, class_defs, function_defs\r\n\nOutputs of `code_graph`:\r\n- Returns a dictionary with nodes and edges representing the relationships in the code.\r\n\nInputs to `extract_control_flow_tree()`:\r\n- `nodes` : A list of ast.AST objects that represent different parts of the file.\r\n\nOutputs of `extract_control_flow_tree`:\r\n- Returns a control flow tree as a nested dictionary.\r\n\nInputs to `reorganize_control_flow()`:\r\n- `code_graph` : A dictionary containing nodes and edges representing the relationships in the code.\r\n- `control_flow_structure` : Control flow structure extracted from the AST.\r\n\nOutputs of `reorganize_control_flow`:\r\n- Returns a reorganized control flow structure that matches the code graph.\r\n\nInputs to `get_plantUML_element()`:\r\n- `element` : An element in the control flow tree.\r\n- `indentation` : The current level of indentation.\r\nOutputs of `get_plantUML_element`: \r\n- A Plant UML string that can be used to visualize the code structure.\r\n\nInputs to `get_plantUML()`:\r\n- `file_details` : Dictionary containing file details and control flow structure.\r\nOutputs of `get_plantUML`:\r\n- Returns a plantUML string for the entire file.\r\n\nInputs to `get_code_graph()`:\r\n- `file_details` : A dictionary containing various attributes extracted from python file.\r\nOutputs of `get_code_graph`:\r\n- Adds code graph and control flow to file details dictionary. It also removes any nodes that are not either a source or target of an edge in the code graph.\r\n``` \n{\n    \"Code Elements\": {\n        \"Dependencies\": \"json, networkx, typing, ast\",\n        \"Functions defined\": \"code_graph, get_edge_data_from_details, add_edge_with_data, add_edges_for_calls, extract_control_flow_tree, reorganize_control_flow, reorganize_structure, get_plantUML_element, get_plantUML, get_code_graph\",\n        \"`add_edge_with_data` Calls\": \"class_method_details_lookup.get, function_details_lookup.get, G.add_edge, get_edge_data_from_details\",\n        \"`add_edge_with_data` Inputs\": \"source, target, init_method\",\n        \"`add_edge_with_data` Variables\": \"source_details, target_details\",\n        \"`add_edges_for_calls` Calls\": \"list, class_def.keys, called.split, called.startswith, called.replace, source_name.split, add_edge_with_data, G.add_node\",\n        \"`add_edges_for_calls` Inputs\": \"source_name, calls\",\n        \"`add_edges_for_calls` Variables\": \"class_names, fully_qualified_name, init_method, method_name, init_method_name, called_class_name\",\n        \"`code_graph` Calls\": \"nx.DiGraph, function_details_lookup.update, class_def.items, G.add_node, class_details['method_defs'].items, G.add_edge, target_details.get, list, set, class_method_details_lookup.get, function_details_lookup.get, get_edge_data_from_details, class_def.keys, called.split, called.startswith, called.replace, source_name.split, add_edge_with_data, function_details_lookup.items, add_edges_for_calls, class_method_details_lookup.items, G[source][target].update, G.edges.data, nodes_to_remove.append\",\n        \"`code_graph` Inputs\": \"file_summary\",\n        \"`code_graph` Returns\": \"{'nodes': nodes, 'edges': edges}, edge_data\",\n        \"`code_graph` Variables\": \"class_names, called_class_name, fully_qualified_name, G, edges, edge_data, method_name, init_method, source_details, class_method_details_lookup, init_method_name, nodes, qualified_method_name, nodes_to_remove, function_details_lookup, target_details\",\n        \"`extract_control_flow_tree` Calls\": \"isinstance, , .join, ast.unparse, control_flow_tree.append, extract_control_flow_tree, if_block.update, except_block.append, any, getattr, node_keywords_map.keys\",\n        \"`extract_control_flow_tree` Inputs\": \"nodes\",\n        \"`extract_control_flow_tree` Returns\": \"control_flow_tree\",\n        \"`extract_control_flow_tree` Variables\": \"try_block, control_flow_tree, handler_name, else_block, args_str, if_block, handler_type, orelse, node_keywords_map, control_flow_dict, except_block, finally_block\",\n        \"`get_code_graph` Calls\": \"code_graph, json.dumps(file_details['file_info']['file_summary']).replace, json.dumps, reorganize_control_flow, extract_control_flow_tree, get_plantUML, str\",\n        \"`get_code_graph` Inputs\": \"file_details\",\n        \"`get_code_graph` Returns\": \"file_details\",\n        \"`get_code_graph` Variables\": \"file_ast\",\n        \"`get_edge_data_from_details` Calls\": \"target_details.get, list, set\",\n        \"`get_edge_data_from_details` Inputs\": \"target_details, source_details, target\",\n        \"`get_edge_data_from_details` Returns\": \"edge_data\",\n        \"`get_edge_data_from_details` Variables\": \"edge_data\",\n        \"`get_plantUML_element` Calls\": \"isinstance, next, iter, key.startswith, get_plantUML_element\",\n        \"`get_plantUML_element` Inputs\": \"element, indentation\",\n        \"`get_plantUML_element` Returns\": \"plantuml_str\",\n        \"`get_plantUML_element` Variables\": \"inner_indentation, value, key, plantuml_str\",\n        \"`get_plantUML` Calls\": \"get_plantUML_element\",\n        \"`get_plantUML` Inputs\": \"file_details\",\n        \"`get_plantUML` Returns\": \"plantuml_str\",\n        \"`get_plantUML` Variables\": \"plantuml_str\",\n        \"`reorganize_control_flow` Calls\": \"set, isinstance, next, iter, json.dumps, organized.append, seen.add, organized.extend, reorganize_structure\",\n        \"`reorganize_control_flow` Inputs\": \"code_graph, control_flow_structure\",\n        \"`reorganize_control_flow` Returns\": \"reorganize_structure(control_flow_structure, starting_points), organized\",\n        \"`reorganize_control_flow` Variables\": \"targets, element_id, key, remaining, starting_points\",\n        \"`reorganize_structure` Calls\": \"set, isinstance, next, iter, json.dumps, organized.append, seen.add, organized.extend\",\n        \"`reorganize_structure` Inputs\": \"structure, start_points\",\n        \"`reorganize_structure` Returns\": \"organized\",\n        \"`reorganize_structure` Variables\": \"remaining, key, element_id\"\n    }\n}",
        "output": "```python\n\"\"\"\nRequirements\n[req01] The code_graph function performs the following:\n        a. Extracts the function and class method details from the file summary.\n        b. Creates a lookup dictionary for function and class method details.\n        c. Creates a directed graph with nodes and edges representing the relationships in the code.\n        d. Adds edges for function and class method calls.\n        e. Adds edge data to edges.\n        f. Returns a dictionary with nodes and edges representing the relationships in the code.\n[req02] The extract_control_flow_tree function performs the following:\n        a. Extracts control flow tree from AST.\n        b. Returns control flow tree.\n[req03] The reorganize_control_flow function performs the following:\n        a. Gets starting points from the code graph.\n        b. Reorganizes the control flow structure recursively.\n        c. Returns reorganized control flow structure.\n[req04] The get_plantUML_element function performs the following:\n        a. Gets plantUML code for each element.\n        b. Returns plantUML code for each element.\n[req05] The get_plantUML function performs the following: \n        a. Gets plantUML code for entire file.\n        b. Returns plantUML code for entire file.\n[req06] The get_code_graph function performs the following:\n        a. Adds code graph and control flow to file details.\n        b. Returns file details.\n\"\"\"\nimport ast\nimport json\nfrom typing import Dict, List, Optional, Union\nimport networkx as nx\n\n\ndef code_graph(\n    file_summary: Dict[str, Union[Dict, str]]\n) -> Dict[str, Union[List[str], Dict[str, List[str]]]]:\n    \"\"\"\n    Create a dictionary representation of file details.\n    Args:\n        file_summary: Dict[str, Union[Dict, str]]: The details extracted from the file.\n    Returns:\n        dict: A dictionary with nodes and edges representing the relationships in the code.\n    \"\"\"\n    G = nx.DiGraph()\n\n    # Create lookup dictionaries for function and class method details\n    function_details_lookup = {}\n    for function_def in file_summary[\"function_defs\"]:\n        function_details_lookup.update(function_def)\n    class_method_details_lookup = {}\n    for class_def in file_summary[\"class_defs\"]:\n        for (\n            class_name,\n            class_details,\n        ) in class_def.items():  # Extract class name and details\n            G.add_node(class_name)  # Add class as a graph node\n            for method_name, method_details in class_details[\"method_defs\"].items():\n                qualified_method_name = (\n                    f\"{class_name}.{method_name}\"  # Create method fully qualified name\n                )\n                G.add_node(qualified_method_name)  # Add method as a graph node\n                class_method_details_lookup[\n                    qualified_method_name\n                ] = method_details  # Store method details\n                G.add_edge(\n                    class_name, qualified_method_name\n                )  # Add edge from class to method\n\n    # Helper function to extract edge data from target details\n    def get_edge_data_from_details(\n        target_details: dict, source_details: dict, target: str\n    ) -> dict:\n        edge_data = {}\n        if target_details:\n            edge_data[\"target_inputs\"] = target_details.get(\"inputs\")\n            edge_data[\"target_returns\"] = list(set(target_details.get(\"returns\", [])))\n        if (\n            source_details\n            and \"call_inputs\" in source_details\n            and target in source_details[\"call_inputs\"]\n        ):\n            edge_data[\"target_inputs\"] = source_details[\"call_inputs\"][target]\n        return edge_data\n\n    # Helper function to add edge with data\n    def add_edge_with_data(\n        source: str, target: str, init_method: Optional[str] = None\n    ) -> None:\n        target_details = class_method_details_lookup.get(\n            init_method or target\n        ) or function_details_lookup.get(target)\n        source_details = function_details_lookup.get(\n            source\n        ) or class_method_details_lookup.get(source)\n        G.add_edge(\n            source,\n            target,\n            **get_edge_data_from_details(target_details, source_details, target),\n        )\n\n    # Helper function to add edges for function or class method calls\n    def add_edges_for_calls(source_name, calls):\n        class_names = [\n            list(class_def.keys())[0] for class_def in file_summary[\"class_defs\"]\n        ]\n        for called in calls:\n            called_class_name = called.split(\".\")[0]\n            if called.startswith(\"self.\"):\n                method_name = called.replace(\"self.\", \"\")\n                fully_qualified_name = f\"{source_name.split('.')[0]}.{method_name}\"\n                if fully_qualified_name in class_method_details_lookup:\n                    add_edge_with_data(source_name, fully_qualified_name)\n                    continue\n            if (\n                called in function_details_lookup\n                or called in class_method_details_lookup\n                or f\"{source_name.split('.')[0]}.{called}\"\n                in class_method_details_lookup\n            ):\n                add_edge_with_data(source_name, called)\n            elif called_class_name in class_names:\n                init_method = None\n                init_method_name = f\"{called}.__init__\"\n                if init_method_name in class_method_details_lookup:\n                    init_method = init_method_name\n                add_edge_with_data(source_name, called, init_method)\n            else:\n                G.add_node(called)\n                add_edge_with_data(source_name, called)\n\n    # Add function nodes to graph and edges for function calls\n    for function_name in function_details_lookup:\n        G.add_node(function_name)\n    for func_name, details in function_details_lookup.items():\n        add_edges_for_calls(func_name, details[\"calls\"])\n\n    # Add edges for method calls\n    for qualified_method_name, details in class_method_details_lookup.items():\n        add_edges_for_calls(qualified_method_name, details[\"calls\"])\n\n    # Add edge data to edges and create node and edges to return\n    for edge in G.edges:\n        source, target = edge\n        target_details = function_details_lookup.get(\n            target\n        ) or class_method_details_lookup.get(target)\n        source_details = function_details_lookup.get(\n            source\n        ) or class_method_details_lookup.get(source)\n        edge_data = get_edge_data_from_details(target_details, source_details, target)\n        G[source][target].update(edge_data)\n    nodes = list(G.nodes)\n    edges = [\n        {\"source\": edge[0], \"target\": edge[1], **edge[2]} for edge in G.edges.data()\n    ]\n\n    # remove any nodes that are not either a source of an edge or a target of an edge\n    nodes_to_remove = []\n    for node in nodes:\n        if node not in [edge[\"source\"] for edge in edges] and node not in [\n            edge[\"target\"] for edge in edges\n        ]:\n            nodes_to_remove.append(node)\n\n    return {\"nodes\": nodes, \"edges\": edges}\n\n\ndef extract_control_flow_tree(nodes: List[ast.AST]) -> List[Union[str, dict]]:\n    \"\"\"\n    Extract control flow tree from AST.\n    Args:\n        nodes: AST nodes\n    Returns:\n        control_flow_tree: control flow tree\n    \"\"\"\n    control_flow_tree = []\n    node_keywords_map = {\n        \"if\": \"If\",\n        \"while\": \"While\",\n        \"for\": \"For\",\n        \"asyncfor\": \"AsyncFor\",\n        \"with\": \"With\",\n        \"asyncwith\": \"AsyncWith\",\n        \"try\": \"Try\",\n        \"except\": \"ExceptHandler\",\n        \"def\": \"FunctionDef\",\n        \"asyncdef\": \"AsyncFunctionDef\",\n        \"class\": \"ClassDef\",\n        \"return\": \"Return\",\n    }\n    for node in nodes:\n        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n            args_str = \", \".join([ast.unparse(arg) for arg in node.args.args])\n            control_flow_tree.append(\n                {f\"def {node.name}({args_str})\": extract_control_flow_tree(node.body)}\n            )\n        elif isinstance(node, ast.If):\n            if_block = {\n                f\"if {ast.unparse(node.test)}\": extract_control_flow_tree(node.body)\n            }\n            orelse = node.orelse\n            while orelse and isinstance(orelse[0], ast.If):\n                if_block.update(\n                    {\n                        f\"elif {ast.unparse(orelse[0].test)}\": extract_control_flow_tree(\n                            orelse[0].body\n                        )\n                    }\n                )\n                orelse = orelse[0].orelse\n            if orelse:\n                if_block.update({\"else\": extract_control_flow_tree(orelse)})\n            control_flow_tree.append(if_block)\n        elif isinstance(node, ast.Return):\n            if node.value is not None:\n                control_flow_tree.append({\"return\": [ast.unparse(node.value)]})\n            else:\n                control_flow_tree.append(\n                    {\"return\": []}\n                )  # Handle cases with no return value\n        elif isinstance(node, ast.Try):\n            try_block = extract_control_flow_tree(node.body)\n            except_block = []\n            for handler in node.handlers:\n                handler_type = (\n                    ast.unparse(handler.type) if handler.type is not None else \"\"\n                )\n                handler_name = (\n                    ast.unparse(handler.name)\n                    if isinstance(handler.name, ast.Name)\n                    else \"\"\n                )\n                except_block.append(\n                    {\n                        f\"except {handler_type} as {handler_name}:\": extract_control_flow_tree(\n                            handler.body\n                        )\n                    }\n                )\n            control_flow_dict = {\"try\": try_block, \"except\": except_block}\n            if node.orelse:\n                else_block = extract_control_flow_tree(node.orelse)\n                control_flow_dict[\"else\"] = else_block\n            if node.finalbody:\n                finally_block = extract_control_flow_tree(node.finalbody)\n                control_flow_dict[\"finally\"] = finally_block\n            control_flow_tree.append(control_flow_dict)\n        elif isinstance(node, ast.While):\n            control_flow_tree.append(\n                {\n                    f\"while {ast.unparse(node.test)}\": extract_control_flow_tree(\n                        node.body\n                    )\n                }\n            )\n        elif isinstance(node, ast.For):\n            control_flow_tree.append(\n                {\n                    f\"for {ast.unparse(node.target)} in {ast.unparse(node.iter)}\": extract_control_flow_tree(\n                        node.body\n                    )\n                }\n            )\n        elif isinstance(node, ast.With):\n            control_flow_tree.append(\n                {\n                    f\"with {', '.join([ast.unparse(item) for item in node.items])}\": extract_control_flow_tree(\n                        node.body\n                    )\n                }\n            )\n        elif isinstance(node, ast.ClassDef):\n            control_flow_tree.append(\n                {f\"class {node.name}\": extract_control_flow_tree(node.body)}\n            )\n        elif any(\n            isinstance(node, getattr(ast, node_keywords_map[keyword]))\n            for keyword in node_keywords_map.keys()\n        ):\n            control_flow_tree.append({ast.unparse(node): []})\n        else:\n            control_flow_tree.append(ast.unparse(node))\n    return control_flow_tree\n\n\ndef reorganize_control_flow(code_graph, control_flow_structure):\n    \"\"\"\n    Reorganize control flow sturcture to match the code graph.\n    Args:\n        file_details: file details\n        control_flow_structure: control flow structure\n    Returns:\n        reorganized_control_flow_structure: reorganized control flow structure\n    \"\"\"\n    # Get starting points from the code graph\n    targets = [edge[\"target\"] for edge in code_graph[\"edges\"]]\n    starting_points = [node for node in code_graph[\"nodes\"] if node not in targets]\n\n    # Define a function to reorganize the structure recursively\n    def reorganize_structure(structure, start_points):\n        organized, seen = [], set()\n\n        # Iterate through each start point and find matching elements in structure\n        for start in start_points:\n            for element in structure:\n                if isinstance(element, dict):\n                    key = next(iter(element))  # Get the first key of the dictionary\n                    if (\n                        start in key\n                    ):  # Add element if it matches the start point and hasn't been seen\n                        element_id = json.dumps(element)\n                        if element_id not in seen:\n                            organized.append(element)\n                            seen.add(element_id)\n                elif (\n                    isinstance(element, str) and start in element\n                ):  # Handle string elements\n                    organized.append(element)\n\n        # Append elements not included in the organized list\n        remaining = [elem for elem in structure if json.dumps(elem) not in seen]\n        organized.extend(remaining)\n        return organized\n\n    # Reorganize the control flow structure recursively\n    return reorganize_structure(control_flow_structure, starting_points)\n\n\ndef get_plantUML_element(element, indentation=\"\"):\n    \"\"\"\n    Get plantUML code for each element.\n    Args:\n        element: element\n        indentation: current indentation level\n    Returns:\n        plantuml_str: plantUML code for each element\n    \"\"\"\n    plantuml_str = \"\"\n    if isinstance(element, dict):\n        key = next(iter(element))\n        value = element[key]\n        if key.startswith(\"def \") or key.startswith(\"class \"):\n            plantuml_str += f\"{indentation}class {value} {{\\n\"\n            inner_indentation = indentation + \"  \"\n        elif key.startswith(\"if \"):\n            plantuml_str += f\"{indentation}if ({value}) {{\\n\"\n            inner_indentation = indentation + \"  \"\n        elif (\n            key.startswith(\"for \")\n            or key.startswith(\"while \")\n            or key.startswith(\"asyncfor \")\n        ):\n            plantuml_str += f\"{indentation}while ({value}) {{\\n\"\n            inner_indentation = indentation + \"  \"\n        elif key.startswith(\"try \"):\n            plantuml_str += f\"{indentation}try {{\\n\"\n            inner_indentation = indentation + \"  \"\n        elif key.startswith(\"except \"):\n            plantuml_str += f\"{indentation}catch ({value}) {{\\n\"\n            inner_indentation = indentation + \"  \"\n        else:\n            plantuml_str += f\"{indentation}:{key};\\n\"\n            inner_indentation = indentation\n\n        if isinstance(value, list):\n            for child in value:\n                plantuml_str += get_plantUML_element(child, inner_indentation)\n\n        if key.startswith(\"def \") or key.startswith(\"class \"):\n            plantuml_str += f\"{indentation}}}\\n\"\n        elif (\n            key.startswith(\"if \")\n            or key.startswith(\"for \")\n            or key.startswith(\"while \")\n            or key.startswith(\"asyncfor \")\n            or key.startswith(\"try \")\n            or key.startswith(\"except \")\n        ):\n            plantuml_str += f\"{indentation}}}\\n\"\n\n    elif isinstance(element, str):\n        plantuml_str += f\"{indentation}:{element};\\n\"\n\n    return plantuml_str\n\n\ndef get_plantUML(file_details):\n    \"\"\"\n    Get plantUML code for entire file.\n    Args:\n        file_details: file details\n    Returns:\n        plantuml_str: plantUML code for entire file\n    \"\"\"\n    plantuml_str = \"@startuml\\n\"\n    for element in file_details[\"file_info\"][\"control_flow_structure\"]:\n        plantuml_str += get_plantUML_element(element, \"  \")\n    plantuml_str += \"end\\n@enduml\"\n    return plantuml_str\n\n\ndef get_code_graph(file_details):\n    \"\"\"\n    Add code graph and control flow to file details.\n    Args:\n        file_details: file details\n    Returns:\n        file_details: file details\n    \"\"\"\n    file_details[\"file_info\"][\"entire_code_graph\"] = code_graph(\n        file_details[\"file_info\"][\"file_summary\"]\n    )\n    file_details[\"file_info\"][\"file_summary\"] = json.dumps(\n        file_details[\"file_info\"][\"file_summary\"]\n    ).replace('\"', \"\")\n\n    # file_code_simplified is the code without comments and docstrings\n    try:\n        file_ast = file_details[\"file_info\"][\"file_ast\"]  # Accessing the AST\n        file_details[\"file_info\"][\"control_flow_structure\"] = reorganize_control_flow(\n            file_details[\"file_info\"][\"entire_code_graph\"],\n            extract_control_flow_tree(file_ast.body)  # Using the AST for control flow extraction\n        )\n        file_details[\"file_info\"][\"plantUML\"] = get_plantUML(file_details)\n    except Exception as e:\n        file_details[\"file_info\"][\"control_flow_structure\"] = [str(e)]\n        file_details[\"file_info\"][\"plantUML\"] = str(e)  \n\n    # remove the AST from the file_details\n    del file_details[\"file_info\"][\"file_ast\"]\n\n    return file_details\n\n```"
    },
    {
        "instruction": "Define a Python code file that is described as follows:\nThe purpose of this Python file is to provide a structure for handling the questions, model configuration, and output directory. It also provides functions to write default questions and model configurations to files if they are not provided by the user. The `get_default_questions` function returns a list of default questions that can be used as prompts for an AI language model. The `get_default_model_config` function returns a dictionary representing the default configuration for the GPT-3 model, which is used to instantiate a model using the ctransformers library. The `get_output_dir` function sets up and validates the output directory for writing data generated by py2dataset. Finally, `write_questions_file` writes the default questions to a file called \"py2dataset_questions.json\" in the specified or current working directory, and `write_model_config_file` writes the default model configuration to a file called \"py2dataset_model_config.yaml\". \n{\n    \"Code Elements\": {\n        \"Dependencies\": \"importlib, typing, yaml, os, logging, pathlib, json\",\n        \"Functions defined\": \"get_default_questions, get_default_model_config, get_start_dir, get_output_dir, get_questions, instantiate_model, get_model, write_questions_file, write_model_config_file\",\n        \"`get_default_model_config` Returns\": \"model_config\",\n        \"`get_default_model_config` Variables\": \"model_config\",\n        \"`get_default_questions` Returns\": \"questions\",\n        \"`get_default_questions` Variables\": \"questions\",\n        \"`get_model` Calls\": \"os.path.join, os.getcwd, open, yaml.safe_load, logging.info, get_default_model_config, instantiate_model\",\n        \"`get_model` Inputs\": \"model_config_pathname\",\n        \"`get_model` Returns\": \"model_config\",\n        \"`get_model` Variables\": \"model_config_pathname, model_config\",\n        \"`get_output_dir` Calls\": \"os.path.abspath, os.makedirs, logging.info\",\n        \"`get_output_dir` Inputs\": \"output_dir\",\n        \"`get_output_dir` Returns\": \"output_dir\",\n        \"`get_output_dir` Variables\": \"output_dir\",\n        \"`get_questions` Calls\": \"os.path.join, os.getcwd, open, json.load, logging.info, get_default_questions\",\n        \"`get_questions` Inputs\": \"questions_pathname\",\n        \"`get_questions` Returns\": \"questions\",\n        \"`get_questions` Variables\": \"questions_pathname, questions\",\n        \"`get_start_dir` Calls\": \"Path(start_dir).is_dir, Path, logging.info, os.getcwd, os.path.abspath\",\n        \"`get_start_dir` Inputs\": \"start_dir\",\n        \"`get_start_dir` Returns\": \"start_dir\",\n        \"`get_start_dir` Variables\": \"start_dir\",\n        \"`instantiate_model` Calls\": \"model_config['model_import_path'].rsplit, getattr, importlib.import_module, inference_function, model_params.pop, ModelClass, logging.info\",\n        \"`instantiate_model` Inputs\": \"model_config\",\n        \"`instantiate_model` Returns\": \"llm, None\",\n        \"`instantiate_model` Variables\": \"inference_function_name, inference_function, model_params, llm, ModelClass\",\n        \"`write_model_config_file` Calls\": \"get_default_model_config, Path(output_dir).is_dir, Path, os.getcwd, open, os.path.join, yaml.dump\",\n        \"`write_model_config_file` Inputs\": \"output_dir\",\n        \"`write_model_config_file` Variables\": \"output_dir, model_config\",\n        \"`write_questions_file` Calls\": \"get_default_questions, Path(output_dir).is_dir, Path, os.getcwd, open, os.path.join, json.dump\",\n        \"`write_questions_file` Inputs\": \"output_dir\",\n        \"`write_questions_file` Variables\": \"output_dir, questions\"\n    }\n}",
        "output": "```python\n\"\"\"\nObtain data parameter and model from the py2dataset functions.\nRequirements:\n[req01] The `get_default_questions` function shall:\n        a. Return a list of default questions.\n        b. Ensure each question in the list is a dictionary.\n        c. Ensure each dictionary has the keys: id, text, and type.\n[req02] The `get_default_model_config` function shall:\n        a. Return a dictionary representing the default model configuration.\n[req03] The `get_output_dir` function shall:\n        a. Accept an optional output_dir argument.\n        b. Return the absolute path of the provided output_dir if it exists or can be created.\n        c. Return the default OUTPUT_DIR if the provided output_dir argument is not provided or invalid.\n[req04] The `get_questions` function shall:\n        a. Accept an optional questions_pathname argument.\n        b. Validate the question file provided by the questions_pathname.\n        c. Return the questions from the provided questions_pathname if valid.\n        d. Return default questions if the questions_pathname is not provided or invalid.\n[req05] The `instantiate_model` function shall:\n        a. Accept a model_config dictionary as an argument.\n        b. Import the specified module and class from the model_config.\n        c. Instantiate and return the model using the provided configuration.\n[req06] The `get_model` function shall:\n        a. Accept an optional model_config_pathname argument.\n        b. Validate the model config file provided by the model_config_pathname.\n        c. Return an instantiated model and a prompt template based on the provided configuration.\n        d. Return an instantiated model and a prompt template based on the default model configuration if the model_config_pathname is not provided or invalid.\n[req07] The `write_questions_file` function shall:\n        a. Accept an optional output_dir argument.\n        b. Write the default questions to the QUESTIONS_FILE in the specified directory.\n        c. Write the default questions to the QUESTIONS_FILE in the current working directory if the output_dir argument is not provided or invalid.\n[req08] The `write_model_config_file` function shall:\n        a. Accept an optional output_dir argument.\n        b. Write the default model configuration to the MODEL_CONFIG_FILE in the specified directory.\n        c. Write the default model configuration to the MODEL_CONFIG_FILE in the current working directory if the output_dir argument is not provided or invalid.\n\"\"\"\nimport os\nimport json\nimport logging\nimport importlib\nfrom typing import Dict, List\nfrom pathlib import Path\nimport yaml\n\n# Setting up a basic logger\nlogging.basicConfig(level=logging.INFO)\n\nQUESTIONS_FILE = \"py2dataset_questions.json\"\nMODEL_CONFIG_FILE = \"py2dataset_model_config.yaml\"\nOUTPUT_DIR = \"datasets\"\n\n\ndef get_default_questions() -> List[Dict]:\n    \"\"\"Return default question list\n    Args:\n        None\n    Returns:\n        List[Dict]: The default question list\n    \"\"\"\n    questions = [\n        {\n            \"id\": \"file_dependencies\",\n            \"text\": \"Dependencies of Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"entire_code_graph\",\n            \"text\": \"Call code graph of Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_functions\",\n            \"text\": \"Functions defined in Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"file_classes\",\n            \"text\": \"Classes defined in Python file: '{filename}'?\",\n            \"type\": \"file\",\n        },\n        {\n            \"id\": \"function_inputs\",\n            \"text\": \"Inputs to function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_docstring\",\n            \"text\": \"Docstring of function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_calls\",\n            \"text\": \"Calls made in function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_variables\",\n            \"text\": \"Variables defined in function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"function_returns\",\n            \"text\": \"Returns from function: '{function_name}' in Python file: '{filename}'?\",\n            \"type\": \"function\",\n        },\n        {\n            \"id\": \"class_methods\",\n            \"text\": \"Methods defined in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_docstring\",\n            \"text\": \"Docstring of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_attributes\",\n            \"text\": \"Attributes of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_variables\",\n            \"text\": \"Variables defined in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"class_inheritance\",\n            \"text\": \"Inheritance of class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"class\",\n        },\n        {\n            \"id\": \"method_inputs\",\n            \"text\": \"Inputs to method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_docstring\",\n            \"text\": \"Docstring of method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_calls\",\n            \"text\": \"Calls made in method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"method_returns\",\n            \"text\": \"Returns from method: '{method_name}' in class: '{class_name}' in Python file: '{filename}'?\",\n            \"type\": \"method\",\n        },\n        {\n            \"id\": \"file_purpose\",\n            \"text\": \"1) Describe the Purpose and Processing Summary of Python file: '{filename}'; 2) Provide an itemized and detailed description of each applicable function, class, and method; 3) Explain what each input, output, and variable does in the code.\",\n            \"type\": \"file\",\n        },\n    ]\n    return questions\n\n\ndef get_default_model_config() -> Dict:\n    \"\"\"Return default model config dict\n    Args:\n        None\n    Returns:\n        Dict: The default model config dictionary\n    \"\"\"\n    model_config = {\n        \"prompt_template\": \"You are an AI coding instructor. Given this Context:\\n'{context}'\\n\\nPlease provide a very detailed, accurate, and insightful Response to this Instruction and include your reasoning step by step. \\n### Instruction:\\n{query}\\n### Response:\",\n        \"inference_model\": {\n            \"model_import_path\": \"ctransformers.AutoModelForCausalLM\",\n            \"model_inference_function\": \"from_pretrained\",\n            \"model_params\": {\n                \"model_path\": \"TheBloke/WizardCoder-Python-13B-V1.0-GGUF\",\n                \"model_type\": \"llama\",\n                \"local_files_only\": False,\n                ## MODEL CONFIGURATION PARAMETERS (set for current model with: GPU 4090-24GB VRAM, CPU 5950x-32 threads, 64GB RAM) \n                # avx2 and gpu_layers are not compatible\n                # \"lib\": \"avx2\",\n                \"threads\": 16,\n                \"batch_size\": 128,\n                \"context_length\": 12000,\n                \"max_new_tokens\": 12000,\n                \"gpu_layers\": 100,\n                \"reset\": True,\n            },\n        },\n    }\n    return model_config\n\n\ndef get_start_dir(start_dir: str = \"\") -> str:\n    \"\"\"\n    Returns the appropriate start directory.\n    Args:\n        start_dir (str): The directory to start the search from.\n    Returns:\n        str: The absolute path of the provided start_dir if it exists or can be created.\n    \"\"\"\n    if start_dir and not Path(start_dir).is_dir():\n        logging.info(f\"Setting Start Dir : {start_dir}\")\n        start_dir = os.getcwd()\n    else:\n        start_dir = os.path.abspath(start_dir)\n    return start_dir\n\n\ndef get_output_dir(output_dir: str = \"\") -> str:\n    \"\"\"Returns the appropriate output directory.\n    Args:\n        output_dir (str): The directory to write the output to.\n    Returns:\n        str: The absolute path of the provided output_dir if it exists or can be created.\n    \"\"\"\n    output_dir = os.path.abspath(output_dir or OUTPUT_DIR)\n    os.makedirs(output_dir, exist_ok=True)\n    logging.info(f\"Using output directory: {output_dir}\")\n    return output_dir\n\n\ndef get_questions(questions_pathname: str) -> List[Dict]:\n    \"\"\"\n    Get questions from file or default\n    Args:\n        questions_pathname (str): The pathname of the questions file\n    Returns:\n        List[Dict]: The list of questions\n    \"\"\"\n    try:  # get questions from provided or default configuration file\n        if not questions_pathname:\n            questions_pathname = os.path.join(os.getcwd(), QUESTIONS_FILE)\n        with open(questions_pathname, \"r\") as f:\n            questions = json.load(f)\n        logging.info(f\"Using questions from file: {questions_pathname}\")\n    except (FileNotFoundError, json.decoder.JSONDecodeError):\n        logging.info(\n            f\"Questions file not valid: {questions_pathname} Using default questions\"\n        )\n        questions = get_default_questions()\n    return questions\n\n\ndef instantiate_model(model_config: Dict) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): model configuration dictionary.\n    Returns:\n        object: An instance of the specified model class, or None if error.\n    \"\"\"\n    try:\n        module_name, class_name = model_config[\"model_import_path\"].rsplit(\".\", 1)\n        ModelClass = getattr(importlib.import_module(module_name), class_name)\n        model_params = model_config[\"model_params\"]\n        inference_function_name = model_config[\"model_inference_function\"]\n        if inference_function_name != \"\":\n            inference_function = getattr(ModelClass, inference_function_name)\n            llm = inference_function(model_params.pop(\"model_path\"), **model_params)\n        else:\n            llm = ModelClass(model_params.pop(\"model_path\"), **model_params)\n        return llm\n    except (ImportError, AttributeError, Exception) as e:\n        logging.info(f\"Failed to instantiate the model. Error: {e}\")\n        return None\n\n\ndef get_model(model_config_pathname: str) -> tuple[object, str]:\n    \"\"\"\n    Returns an instantiated model and prompt template based on the model configuration.\n    Agrs:\n        model_config_pathname (str): The pathname of the model config file\n    Returns:\n        Tuple[object, str]: The instantiated model and prompt template\n    \"\"\"\n    try:\n        if not model_config_pathname:\n            model_config_pathname = os.path.join(os.getcwd(), MODEL_CONFIG_FILE)\n        with open(model_config_pathname, \"r\") as config_file:\n            model_config = yaml.safe_load(config_file)\n        logging.info(f\"Using model config from file: {model_config_pathname}\")\n    except (FileNotFoundError, yaml.YAMLError):\n        logging.info(\n            f\"Model config file not valid: {model_config_pathname} Using default model config\"\n        )\n        model_config = get_default_model_config()\n    model_config[\"model\"] = instantiate_model(model_config[\"inference_model\"])\n    return model_config\n\n\ndef write_questions_file(output_dir: str = \"\") -> None:\n    \"\"\"\n    Writes the default questions to a file in JSON format.\n    Args:\n        output_dir (str): The directory to write the questions file to.\n    Returns:\n        None\n    \"\"\"\n    questions = get_default_questions()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, QUESTIONS_FILE), \"w\") as file:\n        json.dump(questions, file, indent=4)\n\n\ndef write_model_config_file(output_dir: str = \"\") -> None:\n    \"\"\"\n    Writes the default model config to a file in YAML format.\n    Args:\n        output_dir (str): The directory to write the model config file to.\n    Returns:\n        None\n    \"\"\"\n    model_config = get_default_model_config()\n    output_dir = output_dir if output_dir and Path(output_dir).is_dir() else os.getcwd()\n    with open(os.path.join(output_dir, MODEL_CONFIG_FILE), \"w\") as file:\n        yaml.dump(model_config, file)\n\n```"
    },
    {
        "instruction": "Define a Python code file that is described as follows:\n```\nFunctions defined:\n- get_response_from_llm: takes query as input, returns a response based on that query with an external Language Model. \n    Calls made in `get_response_from_llm`: tokenize(prompt), model(prompt)\n- add_to_list: adds the response to the instruct list and returns it. \n    Inputs: list_to_update, query, response, additional_field (optional). \n        Calls made: append(), input\n- clean_and_get_unique_elements: Takes in an input string, removes unwanted characters and returns a unique elements as a string. \n    Inputs: input_str\n        Calls made: strip('[]\"'), strip('{}' chars), regex splitting with .split(',\\s*', flags=re.DOTALL), filtering for non-empty items\n- process_question_type: Processes questions related to a file, function, or method. \n    Inputs: question_type, question_id, query, context, info (optional).\n        Calls made in `process_file` type question: add_to_list() and self.file_details[self.question_mapping[question_type]].get(question_id)\n    Calls made in `process_function` type question: get_string_from_info(), clean_and_get_unique_elements()\n        Inputs: info['functions'], 'inputs', 'variables'\n    Calls made in `process_class` and `process_method`: same as for 'file', get_response_from_llm, process_string().\n    If llm used for question type 'purpose': \n        Calls: self.get_response_from_llm(query, context), enumerate(), re.sub() to remove newlines and multiple spaces, .join() to combine elements of code_qa_list\n- generate: Generates responses for all the questions and returns the instruct_list. \n    Inputs: None\n        Calls made: process_question_type() for each question in self.questions\n```\nOverall Processing Summary:\nThe `get_python_datasets` function initializes a `DatasetGenerator` class with input parameters file path, file details, base name, questions list, and model configuration. It then generates the instruct_list using the `generate()` method of `DatasetGenerator`. Within that, for each question it sets the corresponding question_type ('file', 'class' or 'method'), prompt, context and retrieves information from self.file_details to populate the response based on the given input. The `get_response_from_llm` method is called with a query and file content if required (self.use_llm==True), then a summary of that object within the code is extracted from each call and formatted in a JSON string for output. \n{\n    \"Code Elements\": {\n        \"Classes defined\": \"DatasetGenerator\",\n        \"Dependencies\": \"re, math, json, logging, typing\",\n        \"Functions defined\": \"group_json, clean_and_get_unique_elements, element_generator, get_python_datasets\",\n        \"`DatasetGenerator` Attributes\": \"file_path, file_details, base_name, questions, model_config, instruct_list, question_mapping, llm, use_llm, detailed, llm, use_llm, detailed\",\n        \"`DatasetGenerator` Methods\": \"add_to_list, get_response_from_llm, process_question, get_string_from_info, process_question_type, generate\",\n        \"`DatasetGenerator` Variables\": \"max_context_length, context, info, response_str, code_qa_list, excluded_instructions, context_strategies, context_size, combined_string, query, mapping, instruction, inputs_string, code_elements_combined, method_name, prompt, full_context, code_elements_json, variables_string, methods_string, item_prompt, response, item_response, items\",\n        \"`__init__` Inputs\": \"self, file_path, file_details, base_name, questions, model_config, detailed\",\n        \"`add_to_list` Calls\": \"list_to_update.append\",\n        \"`add_to_list` Inputs\": \"self, list_to_update, query, response, additional_field\",\n        \"`add_to_list` Returns\": \"list_to_update\",\n        \"`clean_and_get_unique_elements` Calls\": \"enumerate, input_str.strip(\\\\'[]\\\\\\\\\\\\'\\\"\\\\').strip, input_str.strip, element.strip(\\\\'\\\\\\\\\\\\'\\\" \\\\').strip, element.strip, element_generator, , .join\",\n        \"`clean_and_get_unique_elements` Inputs\": \"input_str\",\n        \"`clean_and_get_unique_elements` Returns\": \".join(cleaned_elements)\",\n        \"`clean_and_get_unique_elements` Variables\": \"cleaned_elements, input_str, start\",\n        \"`element_generator` Calls\": \"enumerate\",\n        \"`element_generator` Inputs\": \"input_str\",\n        \"`element_generator` Variables\": \"start\",\n        \"`generate` Calls\": \"self.process_question_type\",\n        \"`generate` Inputs\": \"self\",\n        \"`generate` Returns\": \"self.instruct_list\",\n        \"`get_context_and_prompt` Calls\": \"self.model_config['prompt_template'].format, len, self.llm.tokenize\",\n        \"`get_context_and_prompt` Inputs\": \"query, context, code_qa_list\",\n        \"`get_context_and_prompt` Returns\": \"(prompt, context_size)\",\n        \"`get_python_datasets` Calls\": \"DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate, DatasetGenerator\",\n        \"`get_python_datasets` Inputs\": \"file_path, file_details, base_name, questions, model_config, detailed\",\n        \"`get_python_datasets` Returns\": \"DatasetGenerator(file_path, file_details, base_name, questions, model_config, detailed).generate()\",\n        \"`get_response_from_llm` Calls\": \"self.model_config['prompt_template'].format, len, self.llm.tokenize, item['instruction'].split, any, item['instruction'].startswith, str, self.get_string_from_info, strategy, get_context_and_prompt, logging.error, math.ceil, re.sub, self.llm, code_elements_combined.update, json.dumps, group_json, json.loads, logging.info, enumerate, instruct_item['instruction'].startswith, list, item.keys, item.values\",\n        \"`get_response_from_llm` Inputs\": \"self, query, context\",\n        \"`get_response_from_llm` Returns\": \"response, (prompt, context_size)\",\n        \"`get_string_from_info` Calls\": \"item.strip, str(info[item_type]).split, str, , .join\",\n        \"`get_string_from_info` Inputs\": \"info, item_type\",\n        \"`get_string_from_info` Returns\": \", .join(items)\",\n        \"`group_json` Calls\": \"input_json['Code Elements'].items, key.split, output_json['Code Elements'].setdefault(new_key, []).append, output_json['Code Elements'].setdefault, output_json['Code Elements'].setdefault(key, []).append, , .join, output_json['Code Elements'].items, dict, sorted\",\n        \"`group_json` Inputs\": \"input_json\",\n        \"`group_json` Returns\": \"output_json\",\n        \"`group_json` Variables\": \"new_key, output_json\",\n        \"`process_question_type` Calls\": \"question_text.format, self.process_question, self.file_details['classes'].items, class_info.items, key.startswith, len, self.file_details[self.question_mapping[question_type]].items, self.get_string_from_info, , .join, clean_and_get_unique_elements\",\n        \"`process_question_type` Inputs\": \"self, question_type, question_id, question_text\",\n        \"`process_question` Calls\": \"question_id.endswith, info.get, self.get_response_from_llm, clean_and_get_unique_elements, str, str(response).strip, self.instruct_list.append\",\n        \"`process_question` Inputs\": \"self, question_type, question_id, query, context, info\"\n    }\n}",
        "output": "```python\n\"\"\"\nGenerates JSON format question-answer pairs and instructions for a Python file\nRequirements:\n[req01] The `DatasetGenerator` class shall:\n        a. Accept Python file path, file details, base name, list of questions,\n        and model configuration as input during instantiation.\n        b. Initialize and store Python file path, file details, base name,\n        question list, llm, and use_llm flag as class attributes.\n        d. Provide `get_response_from_llm` method to retrieve llm response.\n        e. Provide `process_question` method to process each question, generate\n        corresponding responses, and add to the instruct_list.\n        f. Provide `process_question_type` method to process questions\n        related to the file, functions, classes, and methods.\n        g. Provide `generate` method to generate responses for all questions\n        and return the instruct_list.\n        h. Internally manage question mapping to file details.\n[req02] The `get_python_datasets` function shall:\n        a. Accept a Python file path, file details, base name, questions list,\n        and model config as input.\n        b. Instantiate `DatasetGenerator` class using the provided input.\n        c. Generate instruct_list using `DatasetGenerator` `generate` method.\n        d. Return the generated `instruct_list`.\n[req03] The `clean_and_get_unique_elements` function shall:\n        a. Clean an input string (str) and return a string of unique elements.\n\"\"\"\nimport logging\nimport re\nimport math\nimport json\nfrom typing import Dict, List, Tuple\n\n\ndef group_json(input_json):\n    \"\"\"\n    Group JSON formatted dictionary by key.\n    Args:\n        input_json (Dict): The input JSON formatted dictionary.\n    Returns:\n        Dict: The grouped JSON formatted dictionary.\n    \"\"\"\n    output_json = {\"Code Elements\": {}}   \n    for key, value in input_json[\"Code Elements\"].items():\n        if \"`\" in key:\n            new_key = f\"{key.split()[-1]} {key.split()[0]}\"\n            output_json[\"Code Elements\"].setdefault(new_key, []).append(value)\n        else:\n            output_json[\"Code Elements\"].setdefault(key, []).append(value)\n    output_json[\"Code Elements\"] = {k: \", \".join(v) for k, v in output_json[\"Code Elements\"].items()}\n    output_json[\"Code Elements\"] = dict(sorted(output_json[\"Code Elements\"].items()))\n    return output_json\n\n\ndef clean_and_get_unique_elements(input_str: str) -> str:\n    \"\"\"\n    Clean an input string (str) and return a string of unique elements.\n    Args:\n        input_str (str): The input string to be cleaned.\n    Returns:\n        str: The cleaned string.\n    \"\"\"\n    def element_generator(input_str):\n        start, brace_level = 0, 0\n        for i, char in enumerate(input_str):\n            if char in \"{}\":\n                brace_level += 1 if char == \"{\" else -1\n            elif char == \",\" and brace_level == 0:\n                yield input_str[start:i]\n                start = i + 1\n        yield input_str[start:]\n\n    input_str = input_str.strip(\"[]'\\\"\").strip()\n    cleaned_elements = [\n        element.strip(\"'\\\" \").strip()\n        for element in element_generator(input_str)\n        if element.strip()\n    ]\n    return \", \".join(cleaned_elements)\n\n\nclass DatasetGenerator:\n    \"\"\"\n    Generate JSON formatted dictionary outputs for a Python file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict[str, Any]): Details of the Python file.\n        base_name (str): The base name of the Python file.\n        questions (List[Dict[str, str]]): Questions for generating responses.\n        instruct_list (List[Dict[str, str]]): Storage for generated instructions.\n        question_mapping (Dict[str, str]): Mapping of question types to keys in file details.\n        use_llm (bool): Flag indicating if a language model should be used.\n        llm (object): The language model for generating responses.\n        prompt (str): The prompt format for querying the language model.\n    Methods:\n        add_to_list(list_to_update: List[Dict], query: str, response: str,\n        additional_field=None) -> List[Dict]:\n            Add response to the instruct list.\n        get_response_from_llm(query: str, context: str) -> str:\n            Get language model response to query for given context.\n        process_question(question_type: str, question_id: str, query: str,\n        context: str, info: Dict) -> None:\n            Process question and add generated response to the instruct_list.\n        process_question_type(question_type: str, question_id: str,\n        question_text: str) -> None:\n            Process question related to file, function, class, or method.\n        generate() -> Tuple[List[Dict], List[Dict]]:\n            Generate responses for all the questions and return the instruct_list.\n    \"\"\"\n\n    def __init__(\n        self,\n        file_path: str,\n        file_details: Dict,\n        base_name: str,\n        questions: List[Dict],\n        model_config: Dict,\n        detailed: bool,\n    ) -> None:\n        \"\"\"\n        Initialize the DatasetGenerator class.\n        Args:\n            file_path (str): The path to the Python file.\n            file_details (Dict[str, Any]): Details of the Python file.\n            base_name (str): The base name of the Python file.\n            questions (List[Dict[str, str]]): Questions for generating responses.\n            model_config (Dict): Configuration for the language model.\n        Returns:\n            None\n        \"\"\"\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.model_config = model_config\n        if model_config is not None:\n            self.llm = model_config[\"model\"]\n            self.use_llm = True\n            self.detailed = detailed\n        else:\n            self.llm = None\n            self.use_llm = False\n            self.detailed = False\n        self.instruct_list = []\n        self.question_mapping = {\n            \"file\": \"file\",\n            \"function\": \"functions\",\n            \"class\": \"classes\",\n            \"method\": \"classes\",\n        }\n\n    def add_to_list(\n        self,\n        list_to_update: List[Dict],\n        query: str,\n        response: str,\n        additional_field=None,\n    ) -> List[Dict]:\n        \"\"\"\n        Add response to the instruct list.\n        Args:\n            list_to_update (List[Dict]): The list to update.\n            query (str): The query to be added.\n            response (str): The response to be added.\n            additional_field (Any): Additional field to be added.\n        Returns:\n            List[Dict]: The updated list.\n        \"\"\"\n        list_to_update.append(\n            {\"instruction\": query, \"input\": additional_field, \"output\": response}\n        )\n        return list_to_update\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        \"\"\"\n        Get language model response to query for given context.\n        Args:\n            query (str): The query to be used for generating the response.\n            context (str): The context to be used for generating the response.\n        Returns:\n            str: The generated response.\n        \"\"\"\n\n        def get_context_and_prompt(query, context, code_qa_list):\n            full_context = f\"{context}\\nCODE Q and A:\\n{code_qa_list}\"\n            prompt = self.model_config[\"prompt_template\"].format(\n                context=full_context, query=query\n            )\n            context_size = len(self.llm.tokenize(prompt))\n            return prompt, context_size\n\n        # Creating a list of dictionaries for Q and A pairs to be used as additional LLM context\n        excluded_instructions = [\"Call code graph\", \"Docstring\"]\n        code_qa_list = [\n            {item[\"instruction\"].split(\" in Python file:\")[0]: item[\"output\"]}\n            for item in self.instruct_list\n            if not any(\n                item[\"instruction\"].startswith(prefix)\n                for prefix in excluded_instructions\n            )\n        ]\n\n        # Manage context length for LLM starting with the longest and most comprehensive\n        context_strategies = [\n            lambda: \"```python\\n\" + str(context) + \"\\n```\",\n            lambda: \"```python\\n\"\n            + str(self.file_details[\"file_info\"][\"file_code_simplified\"])\n            + \"\\n```\",\n            lambda: self.get_string_from_info(\n                self.file_details[\"file_info\"], \"file_summary\"\n            ),\n            lambda: \"\",\n        ]\n        max_context_length = self.model_config[\"inference_model\"][\"model_params\"][\n            \"context_length\"\n        ]\n        for strategy in context_strategies:\n            context = strategy()\n            prompt, context_size = get_context_and_prompt(query, context, code_qa_list)\n            if context_size <= 0.70 * max_context_length:\n                break\n            else:\n                logging.error(\n                    f\"Model response failed, increase py2dataset_model_config.yaml context_length > {math.ceil(context_size/0.70)}\"\n                )\n                return \"\"\n\n        response = \"\"\n        try:  # get response from LLM\n            response = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(prompt))\n            code_elements_combined = {}\n            for item in code_qa_list:\n                code_elements_combined.update(item)\n            code_elements_json = json.dumps(\n                {\"Code Elements\": code_elements_combined}, indent=4\n            )\n            code_elements_json = json.dumps(group_json(json.loads(code_elements_json)), indent = 4)\n            response += \"\\n\" + code_elements_json  # Appending the JSON formatted string\n            logging.info(f\"***Overall Response: {response}\")\n\n        except Exception as error:\n            logging.error(f\"Failed to generate model response: {error}\")\n\n        if self.detailed:  # Get llm response for each code_qa_list item\n            for item in code_qa_list:\n                instruction = (\n                    f\"Describe the purpose and significance of these objects within the code: '{item}'\"\n                )\n                item_prompt = f\"\\n### Instruction:\\nUsing this context:\\n{context}\\n\\n{instruction}.\\n### Response:\"\n                try:\n                    item_response = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", self.llm(item_prompt))\n                    logging.info(\n                        f\"\\n***Itemized Response: {instruction}\\n{item_response}\"\n                    )\n                except Exception as error:\n                    logging.error(f\"Failed to generate model response: {error}\")\n\n                # replace the output value in self.instruct_list with the item.key + this_response\n                for i, instruct_item in enumerate(self.instruct_list):\n                    if instruct_item[\"instruction\"].startswith(list(item.keys())[0]):\n                        instruct_item[\"output\"] = (\n                            list(item.keys())[0]\n                            + \":\"\n                            + list(item.values())[0]\n                            + \"\\n\\n\"\n                            + \"Purpose and Significance:\\n\"\n                            + item_response\n                        )\n                        break\n\n        return response\n\n    def process_question(\n        self, question_type: str, question_id: str, query: str, context: str, info: Dict\n    ) -> None:\n        \"\"\"\n        Process question and add the generated response to the instruct_list.\n        Args:\n            question_type (str): The type of question to be processed.\n            question_id (str): The ID of the question to be processed.\n            query (str): The query to be processed.\n            context (str): The context to be used for generating the response.`\n            info (Dict): The information of the Python file.\n        Returns:\n            None\n        \"\"\"\n        if question_id.endswith(\"code_graph\") or question_id.endswith(\"docstring\"):\n            response = info.get(question_id, {})\n        elif self.use_llm and question_id.endswith(\"purpose\"):\n            response = self.get_response_from_llm(query, context)\n        else:\n            response = clean_and_get_unique_elements(str(info.get(question_id, \"\")))\n\n        if question_type == \"file\":\n            context = (\n                \"```python\\n\"\n                + str(self.file_details[\"file_info\"][\"file_code\"])\n                + \"\\n```\"\n            )\n        if response and response != \"None\":\n            response_str = str(response).strip()\n            if response_str:\n                self.instruct_list.append(\n                    {\"instruction\": query, \"input\": context, \"output\": response_str}\n                )\n\n    @staticmethod\n    def get_string_from_info(info, item_type):\n        \"\"\"\n        Get string from info dictionary.\n        Args:\n            info (Dict): The information of the Python file.\n            item_type (str): The type of item to get the string from.\n        Returns:\n            str: The string from the info.\n        \"\"\"\n        if info[item_type]:\n            items = [item.strip() for item in str(info[item_type]).split(\",\") if item]\n            return \", \".join(items)\n        return \"\"\n\n    def process_question_type(\n        self, question_type: str, question_id: str, question_text: str\n    ) -> None:\n        \"\"\"\n        Process questions related to a file, function, class, or method.\n        Args:\n            question_type (str): The type of question to be processed.\n            question_id (str): The ID of the question to be processed.\n            question_text (str): The text of the question to be processed.\n        Returns:\n            None\n        \"\"\"\n        if question_type == \"file\":\n            query = question_text.format(filename=self.base_name)\n            info = self.file_details[\"file_info\"]\n            context = self.file_details[\"file_info\"][\"file_code\"]\n            self.process_question(question_type, question_id, query, context, info)\n        elif question_type == \"method\":\n            for class_name, class_info in self.file_details[\"classes\"].items():\n                for key, method_info in class_info.items():\n                    if key.startswith(\"class_method_\"):\n                        method_name = key[len(\"class_method_\") :]\n                        context = method_info[\"method_code\"]\n                        mapping = {\"class_name\": class_name, \"method_name\": method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(\n                            question_type, question_id, query, context, method_info\n                        )\n        else:  # if question_type == 'function' or question_type == 'class'\n            for name, info in self.file_details[\n                self.question_mapping[question_type]\n            ].items():\n                context = info[f\"{question_type}_code\"]\n                mapping = {f\"{question_type}_name\": name}\n                if question_id == f\"{question_type}_purpose\" and self.use_llm:\n                    variables_string = self.get_string_from_info(\n                        info, f\"{question_type}_variables\"\n                    )\n                    inputs_string = self.get_string_from_info(\n                        info, f\"{question_type}_inputs\"\n                    )\n                    combined_string = \", \".join(\n                        [s for s in [variables_string, inputs_string] if s]\n                    )\n                    mapping[\n                        f\"{question_type}_variables\"\n                    ] = clean_and_get_unique_elements(combined_string)\n                    \n                    if question_type == \"class\":\n                        methods_string = self.get_string_from_info(\n                            info, f\"{question_type}_methods\"\n                        )\n                        mapping[f\"{question_type}_methods\"] = methods_string\n\n                query = question_text.format(filename=self.base_name, **mapping)\n                self.process_question(question_type, question_id, query, context, info)\n\n    def generate(self) -> Tuple[List[Dict], List[Dict]]:\n        \"\"\"\n        Generate responses for all the questions and returns the instruct_list.\n        Args:\n            None\n        Returns:\n            Tuple[List[Dict], List[Dict]]: The generated question-answer pairs and instructions.\n        \"\"\"\n        for question in self.questions:\n            self.process_question_type(\n                question[\"type\"], question[\"id\"], question[\"text\"]\n            )\n        return self.instruct_list\n\n\ndef get_python_datasets(\n    file_path: str,\n    file_details: Dict,\n    base_name: str,\n    questions: List[Dict],\n    model_config: Dict,\n    detailed: bool,\n) -> Tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base Python code filename.\n        questions (List[Dict]): The list of questions.\n        llm (object): The language model to be used for generating responses.\n        prompt (str): The prompt to be used for generating responses.\n        detailed (bool): Flag indicating if detailed information should be extracted.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    return DatasetGenerator(\n        file_path, file_details, base_name, questions, model_config, detailed\n    ).generate()\n\n```"
    },
    {
        "instruction": "Define a Python code file that is described as follows:\nPurpose of `get_python_file_details`:\r\n- Provides a detailed view of the source Python file through extracting information such as function definitions, class definitions, constants, dependencies, and call graph.\r\n- The `get_python_file_details` function is responsible for extracting all the necessary details from a given Python file using the AST (Abstract Syntax Tree) and the `CodeVisitor` class. It returns a dictionary encompassing the extracted file details, including the entire code call graph.\r\n\nProcessing Summary:\r\n- The function `get_python_file_details` takes in a Python file path as an argument and uses the AST to extract various information about the file.\r\n- It then uses the `CodeVisitor` class to traverse the AST, which inherits from `ast.NodeVisitor`, for obtaining required information on the different components like function and classes within it and calls between these nodes, with attributes defined, parameters accepted etc., based on certain specific rules implemented as requirements.\r\n- The extracted details are then used to construct a directed graph using networkx library which represents the code call graph. Finally, the dictionary representation of the code call graph is returned along with the file information in a single object for further analysis.\r\n\nSignificance of Functions:\r\n- `remove_docstring` function removes docstrings and comments from the provided Python code.\r\n- `get_all_calls` function recursively finds all function calls in the subtree rooted at the node, including those in class attributes, list comprehensions, and lambda functions.\r\n- `CodeVisitor` class inherits from `ast.NodeVisitor` to traverse the AST and extract information about classes and functions present within a file, handling `__init__` method and updating context with regard to classes defined during its execution.\r\n\nFunction Significance:\r\n- `remove_docstring`: Removes docstrings from Python code. It accepts a string as input and returns the sanitized code without docstrings or comments.\r\n- `get_all_calls`: Finds all function calls in the subtree rooted at a given node, including those in class attributes, list comprehensions, and lambda functions.\r\n- `CodeVisitor` is a subclass of ast.NodeVisitor that provides information about the code like classes, functions, constants, dependencies, etc., by traversing the AST and inheriting from it.\r\n\nInputs:\r\n- `file_path`: The path to the Python file being analyzed.\r\nOutputs:\r\n- `file_details`: Dictionary containing information about the file including the entire code call graph.\r\nVariables:\r\n- `file_dependencies`: List of dependencies extracted from the file.\r\n- `function_defs`: List of function definitions found in the file.\r\n- `class_defs`: List of class definitions found in the file.\r\n- `constant_assignment`: String containing constant assignments within a Python file.\r\n- `details`: Dictionary containing information about each function and class.\r\n- `call_data`: Dictionary mapping function calls to their arguments.\r\n- `node_walk`: List of nodes in the AST.\r\n\nSignificance of Classes:\r\n- `CodeVisitor` is a subclass of ast.NodeVisitor that provides information about the code like classes, functions, constants, dependencies, etc., by traversing the AST and inheriting from it.\r\nMethods Significance:\r\n- `visit_FunctionDef`: Extracts details about a function such as name, code, docstring, inputs, defaults, returns, calls, arguments, call inputs, variables, decorators, and annotations. If it's within a class context, the method updates the current class dictionary with the extracted information.\r\n- `visit_ClassDef`: Extracts details about a class such as name, code, docstring, attributes, methods, inheritance, static methods, and call inputs.\r\n- `generic_visit`: Traverses the AST to extract various information from nodes like assignments, imports, etc., and updates the current context with regard to classes defined during its execution.\r\n- `extract_details`: Extracts details about a node based on the provided type such as function, class or module.\r\n- `analyze`: Traverses the AST, populates the file information dictionary, and constructs a directed graph using networkx library. It also maintains a current class context to keep track of classes while traversing the AST.\r\n\nSignificance of Variables:\r\n- `current_class`: Indicates whether we are inside a class definition or not.\r\n- `functions`: Dictionary containing details about functions found in the file.\r\n- `classes`: Dictionary containing details about classes found in the file.\r\n- `file_info`: Final dictionary with information on file contents.\r\n- `file_details`: Returns both file content and graph representation as a dictionary object.\r\n\nThe purpose of this code is to provide an in-depth view of a Python file by extracting relevant information like function definitions, class definitions, dependencies, and call graph. The input parameter for the function is a Python file path and the output returns a dictionary encompassing all the required details about the file including the entire code call graph.\r\n\nThe `remove_docstring` function removes docstrings from the provided Python code by parsing it using AST, while `get_all_calls` recursively finds all function calls in the subtree rooted at a given node. The `CodeVisitor` class inherits from `ast.NodeVisitor` to traverse the AST and extract information about classes and functions present within a file.\r\n\nThe `CodeVisitor` class has methods like `visit_FunctionDef`, `visit_ClassDef`, `extract_details`, and `analyze` that are responsible for populating the `file_info` dictionary with details such as function name, code, docstring, inputs, defaults, returns, calls, arguments, call inputs, variables, decorators, annotations, etc. It also maintains a current class context using the attribute 'current_class' to keep track of classes while traversing the AST.\r\n\nFinally, `get_python_file_details` uses these methods to construct a directed graph with nodes and edges representing functions, classes, and method calls. The function returns this dictionary representation of the code call graph along with the file information. \n{\n    \"Code Elements\": {\n        \"Classes defined\": \"CodeVisitor\",\n        \"Dependencies\": \"get_code_graph, typing, logging, ast\",\n        \"Functions defined\": \"remove_docstring, get_all_calls, get_python_file_details\",\n        \"`CodeVisitor` Attributes\": \"current_class, current_class, file_info\",\n        \"`CodeVisitor` Inheritance\": \"ast.NodeVisitor\",\n        \"`CodeVisitor` Methods\": \"visit_FunctionDef, visit_ClassDef, generic_visit, visit_Assign, extract_details, analyze\",\n        \"`CodeVisitor` Variables\": \"file_dependencies, attributes, function_defs, class_defs, constant_assignment, details, call_data, value, value_repr, node_walk, method_defs\",\n        \"`__init__` Inputs\": \"self, code\",\n        \"`analyze` Calls\": \"self.visit, list, ast.walk, isinstance, self.functions.items, self.classes.items, class_details.items, method_name.startswith, len, class_defs.append, self.functions.keys, self.classes.keys, remove_docstring, ast.unparse\",\n        \"`analyze` Inputs\": \"self, node\",\n        \"`extract_details` Calls\": \"list, ast.walk, get_all_calls, ast.unparse, next, isinstance, call_data.keys, set, self.classes[self.current_class].setdefault('class_attributes, []).extend, self.classes[self.current_class].setdefault, details.update, any\",\n        \"`extract_details` Inputs\": \"self, node, node_type\",\n        \"`extract_details` Returns\": \"details\",\n        \"`generic_visit` Calls\": \"ast.iter_child_nodes, self.visit\",\n        \"`generic_visit` Inputs\": \"self, node\",\n        \"`get_all_calls` Calls\": \"isinstance, calls.append, ast.unparse, get_all_calls, ast.iter_child_nodes, calls_dict[func].extend\",\n        \"`get_all_calls` Inputs\": \"node, calls\",\n        \"`get_all_calls` Returns\": \"calls_dict\",\n        \"`get_all_calls` Variables\": \"calls, calls_dict\",\n        \"`get_python_file_details` Calls\": \"open, f.read, ast.parse, logging.warning, CodeVisitor, visitor.analyze, get_code_graph\",\n        \"`get_python_file_details` Inputs\": \"file_path\",\n        \"`get_python_file_details` Returns\": \"file_details, None\",\n        \"`get_python_file_details` Variables\": \"file_details, code, tree, visitor\",\n        \"`remove_docstring` Calls\": \"ast.parse, isinstance, tree.body.pop, ast.walk, node.body.pop, ast.unparse\",\n        \"`remove_docstring` Inputs\": \"code\",\n        \"`remove_docstring` Returns\": \"ast.unparse(tree)\",\n        \"`remove_docstring` Variables\": \"first_body_node, tree\",\n        \"`visit_Assign` Calls\": \"isinstance, ast.unparse, self.constants.append, self.generic_visit\",\n        \"`visit_Assign` Inputs\": \"self, node\",\n        \"`visit_ClassDef` Calls\": \"self.extract_details, self.generic_visit\",\n        \"`visit_ClassDef` Inputs\": \"self, node\",\n        \"`visit_FunctionDef` Calls\": \"self.extract_details, self.generic_visit\",\n        \"`visit_FunctionDef` Inputs\": \"self, node\"\n    }\n}",
        "output": "```python\n\"\"\"\nReturn file_details dictionary for python source code.\nRequirements:\n[req00] The `remove_docstring` function shall:\n        a. Accept a Python code string as an argument.\n        b. Remove docstrings and comments from the provided code.\n        c. Return the sanitized code string.\n[req01] The get_all_calls function shall:\n        a. Accept a node of type ast.AST as an argument.\n        b. Recursively find all function calls in the subtree rooted at the node.\n        c. Return a dictionary of all function calls in the subtree rooted at the node.\n[req02] The `CodeVisitor` class shall:\n        a. Accept the source code as input when instantiated.\n        b. Use the AST to extract details about the code.\n        c. Inherit from `ast.NodeVisitor`.\n        d. Implement `visit_FunctionDef` method to gather details about functions.\n        e. Implement `visit_ClassDef` method to gather details about classes.\n        f. Implement `extract_details` method to parse information about a given node.\n        g. Implement `analyze` method to traverse the AST and 'file_info'. \n        h. Maintain a current class context using the attribute 'current_class'.\n[req03] The `code_graph` function shall:\n        a. Accept the file summary as input.\n        b. Construct a directed graph with nodes and edges using networkx library.\n        c. Define elements such as function nodes, class nodes, and method nodes.\n        d. Specify edges to represent relationships. \n        e. Return a dictionary representation of the code call graph. \n[req04] The `get_python_file_details` function shall:\n        a. Accept a file path as an argument.\n        b. Extract info from Python file using the AST and the `CodeVisitor` class.\n        c. Include the entire code graph in the returned details.\n        d. Return a dictionary encompassing the extracted file details. \n\"\"\"\nimport ast\nimport logging\nfrom typing import Dict, List, Union\nfrom get_code_graph import get_code_graph\n\n\ndef remove_docstring(code: str) -> str:\n    \"\"\"\n    Remove docstrings from the provided Python code.\n    This includes top-level module docstrings and docstrings in\n    functions, classes, and async functions.\n\n    Args:\n        code (str): The source code from which to remove docstrings.\n\n    Returns:\n        str: The source code with docstrings removed.\n    \"\"\"\n    tree = ast.parse(code)\n\n    # Remove top-level docstring if present\n    if (\n        tree.body\n        and isinstance(tree.body[0], ast.Expr)\n        and isinstance(tree.body[0].value, ast.Str)\n    ):\n        tree.body.pop(0)\n\n    for node in ast.walk(tree):\n        if (\n            isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef))\n            and node.body\n        ):\n            first_body_node = node.body[0]\n            if isinstance(first_body_node, ast.Expr) and isinstance(\n                first_body_node.value, ast.Str\n            ):\n                node.body.pop(0)\n\n    return ast.unparse(tree)\n\n\ndef get_all_calls(node: ast.AST, calls=None) -> Dict[str, List[str]]:\n    \"\"\"\n    Recursively find all function calls in the subtree rooted at `node`,\n    including those in class attributes, list comprehensions, and lambda functions.\n\n    Args:\n        node (ast.AST): The root node to start the search from.\n        calls (List[Tuple[str, List[str]]], optional): Accumulator for function calls found.\n\n    Returns:\n        Dict[str, List[str]]: A dictionary mapping function calls (as strings)\n                              to lists of their arguments.\n    \"\"\"\n    if calls is None:\n        calls = [] \n    if isinstance(node, ast.Call):\n        calls.append((ast.unparse(node.func), [ast.unparse(arg) for arg in node.args]))\n    elif isinstance(node, ast.ClassDef):\n        for body_item in node.body:\n            if isinstance(body_item, ast.Assign) and isinstance(\n                body_item.targets[0], ast.Name\n            ):\n                get_all_calls(body_item.value, calls)\n\n    for child in ast.iter_child_nodes(node):\n        get_all_calls(child, calls)\n\n    # Build the dictionary after traversal\n    calls_dict = {}\n    for func, args in calls:\n        if func not in calls_dict:\n            calls_dict[func] = []\n        calls_dict[func].extend(args)\n\n    return calls_dict\n\n\nclass CodeVisitor(ast.NodeVisitor):\n    \"\"\"\n    Visitor class for traversing an AST (Abstract Syntax Tree) and extracting details about the code.\n    Attributes:\n        code (str): The source code.\n        functions(Dict): details about functions in the code.\n        classes (Dict): details about classes in the code.\n        file_info (Dict): details about the file.\n    Methods:\n        visit_FunctionDef(node: ast.FunctionDef) -> None:\n            Extract details about a function.\n        visit_ClassDef(node: ast.ClassDef) -> None:\n            Extract details about a class.\n        extract_details(node: ast.AST, node_type: str) -> Dict[str, Union[str, List[str]]]:\n            Extract details about a node.\n        analyze(node: ast.AST) -> None:\n            Populate file_info with details about the file.\n    \"\"\"\n\n    def __init__(self, code: str):\n        \"\"\"\n        Initialize a new instance of the class.\n        Args:\n            code: str: The source code.\n        \"\"\"\n        self.code: str = code\n        self.functions: Dict[str, Dict[str, Union[str, List[str]]]] = {}\n        self.classes: Dict[str, Dict[str, Union[str, List[str]]]] = {}\n        self.file_info: Dict[str, Union[str, List[str]]] = {}\n        self.current_class: str = None\n        self.constants: List[str] = []\n\n    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:\n        \"\"\"\n        Extract details about a function.\n        Args:\n            node: ast.FunctionDef: The node to visit.\n        \"\"\"\n        details = self.extract_details(\n            node, \"method\" if self.current_class else \"function\"\n        )\n        if self.current_class:\n            self.classes[self.current_class][f\"class_method_{node.name}\"] = details\n        else:\n            self.functions[node.name] = details\n        self.generic_visit(node)\n\n    def visit_ClassDef(self, node: ast.ClassDef) -> None:\n        \"\"\"\n        Extract details about a class.\n        Args:\n            node: ast.ClassDef: The node to visit.\n        \"\"\"\n        self.classes[node.name] = self.extract_details(\n            node, \"class\"\n        )  # populate class dictionary when class definition found in AST\n        self.current_class = node.name  # set current_class to indicate inside a class\n        self.generic_visit(node)  # continue AST traversal to the next node\n        self.current_class = None  # reset current_class when finished with this class\n\n    def generic_visit(self, node):\n        \"\"\"\n        Called if no explicit visitor function exists for a node.\n        Args:\n            node: ast.AST: The node to visit.\n        \"\"\"\n        for child in ast.iter_child_nodes(node):\n            child.parent = node  # Set the parent attribute\n            self.visit(child)\n\n    def visit_Assign(self, node: ast.Assign):\n        \"\"\"\n        Get self.constants \n        Args:\n            node: ast.Assign: The node to visit.\n        \"\"\"\n        if isinstance(node.parent, ast.Module):\n            for target in node.targets:\n                if isinstance(target, ast.Name):\n                    value = node.value\n                    if isinstance(value, ast.Constant) and isinstance(value.value, str):\n                        value_repr = f\"'{value.value}'\"\n                    else:\n                        value_repr = ast.unparse(value)\n                    constant_assignment = f\"{target.id}={value_repr}\"\n                    self.constants.append(constant_assignment)\n        self.generic_visit(node)  \n\n    def extract_details(\n        self, node: ast.AST, node_type: str\n    ) -> Dict[str, Union[str, List[str]]]:\n        \"\"\"\n        Extract details about a node.\n        Args:\n            node: ast.AST: The node to extract details from.\n            node_type: str: The type of node.\n        Returns:\n            Dict[str, Union[str, List[str]]]: The details extracted from the node.\n        \"\"\"\n        node_walk = list(ast.walk(node))\n        call_data = get_all_calls(node)\n        details = {\n            f\"{node_type}_name\": node.name,\n            f\"{node_type}_code\": ast.unparse(node),\n            f\"{node_type}_docstring\": next(\n                (\n                    n.value.s\n                    for n in node_walk\n                    if isinstance(n, ast.Expr) and isinstance(n.value, ast.Str)\n                ),\n                None,\n            ),\n            f\"{node_type}_inputs\": [arg.arg for arg in node.args.args]\n            if node_type in [\"function\", \"method\"]\n            else None,\n            f\"{node_type}_defaults\": [ast.unparse(d) for d in node.args.defaults]\n            if node_type in [\"function\", \"method\"]\n            else None,\n            f\"{node_type}_returns\": [\n                ast.unparse(subnode.value) if subnode.value is not None else \"None\"\n                for subnode in node_walk\n                if isinstance(subnode, ast.Return)\n            ],\n            f\"{node_type}_calls\": list(call_data.keys()),\n            f\"{node_type}_call_inputs\": call_data,\n            f\"{node_type}_variables\": list(\n                {\n                    ast.unparse(target)\n                    for subnode in node_walk\n                    if isinstance(subnode, ast.Assign)\n                    for target in subnode.targets\n                    if isinstance(target, ast.Name)\n                }\n            ),\n            f\"{node_type}_decorators\": list(\n                {ast.unparse(decorator) for decorator in node.decorator_list}\n                if node.decorator_list\n                else set()\n            ),\n            f\"{node_type}_annotations\": list(\n                {\n                    ast.unparse(subnode.annotation)\n                    for subnode in node_walk\n                    if isinstance(subnode, ast.AnnAssign)\n                    and subnode.annotation is not None\n                }\n            ),\n            f\"{node_type}_properties\": list(\n                {\n                    ast.unparse(subnode)\n                    for subnode in node_walk\n                    if isinstance(subnode, ast.Attribute)\n                    and isinstance(subnode.ctx, ast.Store)\n                }\n            ),\n        }\n        if node_type in [\"class\", \"method\"]:\n            if (\n                node_type == \"method\" and self.current_class\n            ):  # find attributes defined as self.attribute\n                attributes = [\n                    target.attr\n                    for subnode in node_walk\n                    if isinstance(subnode, ast.Assign)\n                    for target in subnode.targets\n                    if isinstance(target, ast.Attribute)\n                    and isinstance(target.value, ast.Name)\n                    and target.value.id == \"self\"\n                ]\n                self.classes[self.current_class].setdefault(\n                    \"class_attributes\", []\n                ).extend(attributes)\n            if node_type == \"class\":\n                details.update(\n                    {\n                        \"class_attributes\": [\n                            target.attr\n                            for subnode in node.body\n                            if isinstance(subnode, ast.Assign)\n                            for target in subnode.targets\n                            if isinstance(target, ast.Attribute)\n                        ],\n                        \"class_methods\": [\n                            subnode.name\n                            for subnode in node.body\n                            if isinstance(subnode, ast.FunctionDef)\n                            and subnode.name != \"__init__\"\n                        ],\n                        \"class_inheritance\": [ast.unparse(base) for base in node.bases]\n                        if node.bases\n                        else [],\n                        \"class_static_methods\": [\n                            subnode.name\n                            for subnode in node.body\n                            if isinstance(subnode, ast.FunctionDef)\n                            and subnode.name != \"__init__\"\n                            and any(\n                                isinstance(decorator, ast.Name)\n                                and decorator.id == \"staticmethod\"\n                                for decorator in subnode.decorator_list\n                            )\n                        ],\n                    }\n                )\n        return details\n\n    def analyze(self, node: ast.AST) -> None:\n        \"\"\"\n        Traverse the AST, list all nodes, and populate 'file_info'\n        Args:\n            node: ast.AST: The node to analyze.\n        \"\"\"\n        # Traverse the AST to capture various code details\n        self.visit(node)\n        node_walk = list(ast.walk(node))\n\n        file_dependencies = {\n            alias.name\n            for subnode in node_walk\n            if isinstance(subnode, ast.Import)\n            for alias in subnode.names\n        } | {\n            subnode.module\n            for subnode in node_walk\n            if isinstance(subnode, ast.ImportFrom)\n        }\n\n        function_defs = [\n            {\n                func_name: {\n                    \"inputs\": details[\"function_inputs\"],\n                    \"calls\": details[\"function_calls\"],\n                    \"call_inputs\": details[\"function_call_inputs\"],\n                    \"returns\": details[\"function_returns\"],\n                }\n            }\n            for func_name, details in self.functions.items()\n        ]\n        \n        class_defs = []\n        for class_name, class_details in self.classes.items():\n            method_defs = {}\n            for method_name, details in class_details.items():\n                if method_name.startswith(\"class_method_\"):\n                    method_defs[method_name[len(\"class_method_\") :]] = {\n                        \"inputs\": details[\"method_inputs\"],\n                        \"calls\": details[\"method_calls\"],\n                        \"call_inputs\": details[\"method_call_inputs\"],\n                        \"returns\": details[\"method_returns\"],\n                    }\n            class_defs.append({class_name: {\"method_defs\": method_defs}})\n\n        self.file_info = {\n            \"file_code\": self.code,\n            \"file_ast\" : node,\n            \"file_dependencies\": list(file_dependencies),\n            \"file_functions\": list(self.functions.keys()),\n            \"file_classes\": list(self.classes.keys()),\n            \"file_constants\": self.constants,\n            \"file_summary\": {\n                \"dependencies\": list(file_dependencies),\n                \"function_defs\": function_defs,\n                \"class_defs\": class_defs,\n            },\n            \"file_code_simplified\": remove_docstring(ast.unparse(node)),\n        }\n\n\ndef get_python_file_details(file_path: str) -> Dict[str, Union[Dict, str]]:\n    \"\"\"\n    Extract details from a Python file.\n    Args:\n        file_path: str: The path to the Python file.\n    Returns:\n        Dict[str, Union[Dict, str]]: The details extracted from the file.\n    \"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            code = f.read()\n            tree = ast.parse(code)\n    except (PermissionError, SyntaxError, IOError) as e:\n        logging.warning(f\"{e} error in file: {file_path}\")\n        return None\n    visitor = CodeVisitor(code)\n    visitor.analyze(tree)\n    file_details = {\n        \"file_info\": visitor.file_info,\n        \"functions\": visitor.functions,\n        \"classes\": visitor.classes,\n    }\n    file_details = get_code_graph(file_details)\n    return file_details\n\n```"
    },
    {
        "instruction": "Define a Python code file that is described as follows:\nThe purpose of this module is to generate, save, and return datasets that include responses to questions about Python files. The processing summary includes the following functions: `process_single_python_file`, `py2dataset`, `clone_github_repo`, and `main`. \n\n1. `process_single_python_file` function: This function is responsible for processing a single Python file to generate question-answer pairs and instructions. It accepts parameters for Python file path, start directory, model config pathname, questions dict, use of LLM, and output dir. If 'use_llm' is True, it uses the `get_model` function to instantiate an LLM model configuration. It then retrieves the file details using the `get_python_file_details` function and acquires instruct.json datasets using the `get_python_datasets` function. Finally, it saves the file details and instruct.json data using the `save_python_data` function. \n2. `py2dataset` function: This function is responsible for generating datasets by processing Python files within a specified directory. It accepts parameters for start directory, output dir, questions path, model config path, use of LLM, and quiet mode. Based on the command-line arguments provided, it adjusts logging level based on 'quiet', determines an output dir with `get_output_dir`, retrieves a question dictionary with `get_questions`. It then uses the `Path(start_dir).rglob` method to search for Python files excluding those starting with \"_\". For each Python file, it spawns a child process using `process_single_python_file` to get file details and instruct.json data if 'single_process' is False. If 'use_llm' is True and single_process is False, it loads the LLM model configuration in the main process. After processing all Python files, it combines the individual datasets using `combine_json_files` function and returns a dictionary of generated datasets. \n3. `clone_github_repo` function: This function clones a GitHub repository or pulls the latest changes based on the provided URL. It first checks if the URL is valid by running `git ls-remote` command, then proceeds to clone or fetch depending on whether it's a new repository or an existing one. \n4. `main` function: This function processes command-line arguments and determines py2dataset parameters based on processed arguments. It calls the `py2dataset` with the derived parameters including logging, search parameters (path/repository) based on `args.interactive` (switched True for interactive mode). Additionally, it checks if 'use_llm' is set to False or not and sets a recursion limit of 3000 for AST parsing. \n5. `PathArgument` class: This class inherits from the `argparse.Action` class and overrides its `__call__` method to clean up input values by removing quotes and whitespaces. It is used in the command-line arguments to ensure that paths are passed as strings without any extra characters. \n6. Variables defined in `process_single_python_file`: 'relative_path', 'file_details', 'base_name', 'parent_dir', 'model_config', and 'instruct_data'. These variables are used to generate the relative path, retrieve file details, create a base name for the output files, get the parent directory of start dir, define model configuration based on use_llm and single_process, acquire instructions from the `get_python_datasets` function. \n7. Variables defined in `py2dataset`: 'single_process', 'model_config', 'output_dir', 'start_dir', and 'questions_dict'. These variables are used to determine if a single process is used for file processing, define the output directory using `get_output_dir`, get the question dictionary from `get_questions` function, search for Python files using `Path(start_dir).rglob`, spawn child processes with `process_single_python_file` based on 'use_llm' and 'single_process', and combine all the individual datasets into a single dictionary. \n8. Variables defined in `clone_github_repo`: 'repo_name', 'path', and 'default_branch'. These variables are used to clone a repository or fetch the latest changes, set the repo path based on 'gitrepos' directory, get the parent directory of start dir, and run commands with git. \n9. Variables defined in `main`: 'args', 'bool_params', and 'cleaned_value'. These variables are used to handle command-line arguments, define boolean parameters for interactive mode, and clean up input values. \n10. Calls made in `process_single_python_file` include: logging.info, os.path.dirname, os.path.relpath, Path, .join, get_model, logging.error, get_python_datasets, save_python_data. \n11. Calls made in `py2dataset` include: logging.getLogger().setLevel, logging.getLogger, sys.setrecursionlimit, get_start_dir, get_output_dir, get_questions, Path(start_dir).rglob, Process, proc.start, and proc.join. \n12. Calls made in `clone_github_repo` include: shlex.quote, subprocess.run, url.split, os.path.join, os.getcwd, git.Repo.clone_from, repo.git.fetch, and repo.git.reset \n13. Inputs to the class 'PathArgument': \"Path name input in quotations. Default values for optional parameters\"\n14. Returns from `py2dataset` are: The combined datasets using `combine_json_files(output_dir, html)` function. \n15. Returns from `clone_github_repo` are: The path to the cloned repository. \n16. Returns from `main` are: True if input_str is 't' or 'true', False if input_str is 'f' or 'false', and retains current value for any other inputs. \n{\n    \"Code Elements\": {\n        \"Classes defined\": \"PathArgument\",\n        \"Dependencies\": \"pathlib, os, get_params, shlex, save_output, argparse, sys, get_python_file_details, logging, git, typing, subprocess, multiprocessing, get_python_datasets\",\n        \"Functions defined\": \"process_single_python_file, py2dataset, clone_github_repo, main, get_bool_from_input\",\n        \"`PathArgument` Inheritance\": \"argparse.Action\",\n        \"`PathArgument` Methods\": \"__call__\",\n        \"`PathArgument` Variables\": \"cleaned_value\",\n        \"`__call__` Calls\": \"values.strip(\\\\'\\\"\\\\').strip, values.strip, setattr\",\n        \"`__call__` Inputs\": \"self, parser, namespace, values, option_string\",\n        \"`clone_github_repo` Calls\": \"shlex.quote, subprocess.run, print, url.split, os.path.join, os.getcwd, os.makedirs, os.path.exists, git.Repo.clone_from, git.Repo, repo.git.custom_environment, repo.git.fetch, repo.head.reference.tracking_branch, repo.git.reset\",\n        \"`clone_github_repo` Inputs\": \"url\",\n        \"`clone_github_repo` Returns\": \"path\",\n        \"`clone_github_repo` Variables\": \"repo_name, path, default_branch, githubrepos_dir, command, repo\",\n        \"`get_bool_from_input` Calls\": \"input_str.lower\",\n        \"`get_bool_from_input` Inputs\": \"input_str, current_value\",\n        \"`get_bool_from_input` Returns\": \"True if input_str.lower() in ['t, true'] else False if input_str.lower() in ['f, false'] else current_value\",\n        \"`main` Calls\": \"values.strip(\\\\'\\\"\\\\').strip, values.strip, setattr, input_str.lower, argparse.ArgumentParser, parser.add_argument, parser.parse_args, print, input, getattr, get_bool_from_input, os.path.isdir, args.start.startswith, os.getcwd, clone_github_repo, py2dataset\",\n        \"`main` Returns\": \"True if input_str.lower() in ['t, true'] else False if input_str.lower() in ['f, false'] else current_value\",\n        \"`main` Variables\": \"args, bool_params, cleaned_value, pathname_params, parser\",\n        \"`process_single_python_file` Calls\": \"logging.info, os.path.dirname, os.path.relpath, Path, .'.join, get_model, get_python_file_details, logging.error, get_python_datasets, save_python_data\",\n        \"`process_single_python_file` Inputs\": \"python_file_path, start_dir, model_config_path, questions_dict, use_llm, output_dir, model_config, single_process_mode, detailed_analysis\",\n        \"`process_single_python_file` Variables\": \"relative_path, file_details, base_name, parent_dir, model_config, instruct_data\",\n        \"`py2dataset` Calls\": \"logging.getLogger().setLevel, logging.getLogger, sys.setrecursionlimit, get_start_dir, get_output_dir, get_questions, get_model, Path(start_dir).rglob, Path, process_single_python_file, Process, proc.start, proc.join, combine_json_files\",\n        \"`py2dataset` Inputs\": \"start_dir, output_dir, questions_pathname, model_config_pathname, use_llm, quiet, single_process, detailed, html\",\n        \"`py2dataset` Returns\": \"combine_json_files(output_dir, html)\",\n        \"`py2dataset` Variables\": \"single_process, model_config, output_dir, start_dir, questions_dict, proc\"\n    }\n}",
        "output": "```python\n\"\"\"\nFor each Python file within a given directory, this module is designed to generate, save, \nand return datasets that include responses to questions about the code. \nRequirements:\n[req00] The process_single_python_file function shall:\n    a. Accept parameters for Python file path, start directory, model config \n       pathname, questions dict, use of LLM, and output dir.\n    b. If 'use_llm' is True, use 'get_model' to instantiate LLM model config.\n    c. Use 'get_python_file_details' to retrieve Python file info.\n    d. Use 'get_python_datasets' to acquire instruct.json datasets.\n    e. Use 'save_python_data' to store file details and instruct.json data.\n[req01] The py2dataset function shall:\n    a. Accept parameters for start directory, output dir, questions path, model\n       config path, use of LLM, and quiet mode.\n    b. Adjust logging level based on 'quiet'.\n    c. Use current working dir if no valid start dir is provided.\n    d. Get output dir with 'get_output_dir'.\n    e. Retrieve questions dict with 'get_questions'.\n    f. Search for Python files using 'rglob', excluding those starting with \"_\".\n    g. For each Python file, spawn a child process with 'process_single_python_file'\n       to get file details and instruct.json data, if single_process is False.\n    h. Combine instruct.json files with 'combine_json_files'.\n    i. Return datasets.\n[req02] The main function shall:\n    a. Accept and process command-line args.\n    b. Determine py2dataset parameters based on processed arguments.\n    c. Call py2dataset with derived parameters.\n\"\"\"\nimport sys\nimport argparse\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom multiprocessing import Process\nimport subprocess\nimport os\nimport git\nimport shlex\n\nfrom get_python_file_details import get_python_file_details\nfrom get_python_datasets import get_python_datasets\nfrom get_params import (\n    get_questions,\n    get_model,\n    get_output_dir,\n    get_start_dir,\n)\nfrom save_output import combine_json_files, save_python_data\n\n\ndef process_single_python_file(\n    python_file_path: str,\n    start_dir: str,\n    model_config_path: str,\n    questions_dict: Dict,\n    use_llm: bool,\n    output_dir: str,\n    model_config: Dict = None,\n    single_process_mode: bool = False,\n    detailed_analysis: bool = False,\n) -> None:\n    \"\"\"\n    Processes a single Python file to generate question-answer pairs and instructions.\n\n    Args:\n        python_file_path (str): Path to the Python file.\n        start_dir (str): Starting directory to search for Python files.\n        model_config_path (str): Path to the model configuration file.\n        questions_dict (Dict): Dictionary of questions to answer about the Python file.\n        use_llm (bool): If True, use a Large Language Model for generating JSON answers.\n        output_dir (str): Directory to save the output files.\n        model_config (Dict): Configuration dictionary for the LLM.\n        single_process_mode (bool): Use a single process if True. Defaults to False.\n        detailed_analysis (bool): Perform detailed analysis if True. Defaults to False.\n    \"\"\"\n    logging.info(f\"Processing file: {python_file_path}\")\n\n    # need to define relative path as the having at least one parent directory\n    # e.g relative patch should be equal to <last start_directory directory> / file name without path from python_file_path\n    # Get the last directory in start_dir\n    parent_dir = os.path.dirname(start_dir)\n    relative_path = os.path.relpath(python_file_path, parent_dir)\n    relative_path = Path(relative_path)\n    base_name = \".\".join(relative_path.parts)\n\n    if not single_process_mode and use_llm:\n        model_config = get_model(model_config_path)\n\n    file_details = get_python_file_details(python_file_path)\n    if not file_details:\n        logging.error(f\"Failed to get file details for {python_file_path}\")\n        return\n\n    instruct_data = get_python_datasets(\n        python_file_path,\n        file_details,\n        base_name,\n        questions_dict,\n        model_config,\n        detailed_analysis,\n    )\n\n    if instruct_data:\n        save_python_data(\n            file_details, instruct_data, base_name, relative_path, output_dir\n        )\n    else:\n        logging.error(f\"Failed to get instruct data for {python_file_path}\")\n\n\ndef py2dataset(\n    start_dir: str = \"\",\n    output_dir: str = \"\",\n    questions_pathname: str = \"\",\n    model_config_pathname: str = \"\",\n    use_llm: bool = False,\n    quiet: bool = False,\n    single_process: bool = False,\n    detailed: bool = False,\n    html: bool = False,\n) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Generates datasets by processing Python files within a specified directory.\n    Args:\n        start_dir (str): Starting directory for Python files. Defaults to current directory.\n        output_dir (str): Directory to save the output files.\n        questions_pathname (str): Path and filename of the questions file.\n        model_config_pathname (str): Path and filename of the model configuration file.\n        use_llm (bool): If True, use a Large Language Model for generating answers. Defaults to False.\n        quiet_mode (bool): Reduce logging output if True. Defaults to False.\n        single_process (bool): Use a single process for file processing if use_llm. Defaults to False.\n        detailed (bool): Include detailed analysis if True. Defaults to False.\n        html (bool): Generate HTML outputs if True. Defaults to False.\n    Returns:\n        Dict[str, List[Dict]]: Dictionary of generated datasets.\n    \"\"\"\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n\n    sys.setrecursionlimit(3000)  # Set recursion limit higher for AST parsing\n\n    start_dir = get_start_dir(start_dir)\n    output_dir = get_output_dir(output_dir)\n    questions_dict = get_questions(questions_pathname)\n\n    # Load model configuration if LLM is used and single process mode is enabled\n    model_config = (\n        get_model(model_config_pathname) if use_llm and single_process else None\n    )\n\n    if not use_llm:\n        single_process = True\n\n    # Process each Python file in the directory\n    for python_file_path in Path(start_dir).rglob(\"[!_]*.py\"):\n        # if use_llm is false or single_process is true then process the file within the current process\n\n        if not use_llm and single_process:\n            process_single_python_file(\n                python_file_path,\n                start_dir,\n                model_config_pathname,\n                questions_dict,\n                use_llm,\n                output_dir,\n                model_config,\n                single_process,\n                detailed,\n            )\n        else:\n            # Spawn new process for each file to manage memory and performance\n            proc = Process(\n                target=process_single_python_file,\n                args=(\n                    python_file_path,\n                    start_dir,\n                    model_config_pathname,\n                    questions_dict,\n                    use_llm,\n                    output_dir,\n                    None,\n                    single_process,\n                    detailed,\n                ),\n            )\n            proc.start()\n            proc.join()\n\n    # Combine all the individual datasets into a single dictionary\n    return combine_json_files(output_dir, html)\n\n\ndef clone_github_repo(url: str) -> str:\n    \"\"\"\n    Clone repository or pull the latest changes and return local repository path.\n    Args:\n        url (str): The url of the github repository.\n    Returns:\n        str: The path to the cloned repository.\n    \"\"\"\n    # Check valid Git repository\n    try:\n        command = f\"git ls-remote {shlex.quote(url)}\"\n        subprocess.run(\n            command,\n            shell=True,\n            check=True,\n            stdout=subprocess.DEVNULL,\n            stderr=subprocess.DEVNULL,\n        )\n    except subprocess.CalledProcessError:\n        print(f\"Invalid or inaccessible repository: {url}\")\n        return \"\"\n\n    # Proceed with cloning or fetching\n    repo_name = url.split(\"/\")[-1]\n    githubrepos_dir = os.path.join(os.getcwd(), \"githubrepos\")\n    os.makedirs(githubrepos_dir, exist_ok=True)\n    path = os.path.join(githubrepos_dir, repo_name)\n    if not os.path.exists(path):\n        git.Repo.clone_from(url, path)\n    else:\n        repo = git.Repo(path)\n        with repo.git.custom_environment(\n            GIT_SSH_COMMAND=\"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no\"\n        ):\n            repo.git.fetch()\n            default_branch = repo.head.reference.tracking_branch().remote_head\n            repo.git.reset(\"--hard\", default_branch)\n    return path\n\n\ndef main():\n    \"\"\"\n    Command-line entry point for processing Python files and generating datasets.\n    Optional command-line arguments:\n    --start (str, optional): Starting directory for Python files or GitHub repository Python files. Defaults to current working directory.\n    --output_dir (str, optional): Directory to write the output files. Defaults to ./dataset/.\n    --questions_pathname (str, optional): Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\n    --model_config_pathname (str, optional): Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\n    --use_llm (bool, optional): Use llm for generating JSON answers. Defaults to False.\n    --quiet (bool, optional): Limit logging output. Defaults to False.\n    --single_process (bool, optional): If True, only a single process will be used to process Python files. Defaults to False.\n    --detailed (bool, optional): Include detailed analysis if True. Defaults to False.\n    --html (bool, optional): Generate HTML output if True. Defaults to False.\n    --I (str, optional): Interactive mode. Defaults to False.\n    \"\"\"\n\n    class PathArgument(argparse.Action):\n        def __call__(self, parser, namespace, values, option_string=None):\n            cleaned_value = values.strip('\"').strip(\"'\")\n            setattr(namespace, self.dest, cleaned_value)\n\n    def get_bool_from_input(input_str: str, current_value: bool) -> bool:\n        \"\"\"\n        Get a boolean value from user input or retain the current value if the input is invalid.\n        \"\"\"\n        return (\n            True\n            if input_str.lower() in [\"t\", \"true\"]\n            else False\n            if input_str.lower() in [\"f\", \"false\"]\n            else current_value\n        )\n\n    # parse each command line entry\n    parser = argparse.ArgumentParser(\n        description=\"Process Python files to generate datasets.\"\n    )\n    parser.add_argument(\n        \"--start\",\n        default=\".\",\n        action=PathArgument,\n        help=\"Starting directory for Python files. Defaults to current working directory.\",\n    )\n    parser.add_argument(\n        \"--output_dir\",\n        default=\"./dataset/\",\n        action=PathArgument,\n        help=\"Directory to write the output files. Defaults to ./dataset/.\",\n    )\n    parser.add_argument(\n        \"--questions_pathname\",\n        default=\"./py2dataset_questions.json\",\n        action=PathArgument,\n        help=\"Path and filename of the questions file. Defaults to ./py2dataset_questions.json.\",\n    )\n    parser.add_argument(\n        \"--model_config_pathname\",\n        default=\"./py2dataset_model_config.yaml\",\n        action=PathArgument,\n        help=\"Path and filename of the model configuration file. Defaults to ./py2dataset_model_config.yaml.\",\n    )\n    parser.add_argument(\n        \"--use_llm\",\n        action=\"store_true\",\n        help=\"Use LLM for generating JSON answers. Defaults to False.\",\n    )\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Limit logging output.\")\n    parser.add_argument(\n        \"--single_process\",\n        action=\"store_true\",\n        help=\"Use a single process for processing Python files. Defaults to False.\",\n    )\n    parser.add_argument(\n        \"--detailed\", action=\"store_true\", help=\"Include detailed analysis if True.\"\n    )\n    parser.add_argument(\n        \"--html\", action=\"store_true\", help=\"Generate HTML output if True.\"\n    )\n    parser.add_argument(\n        \"--I\",\n        \"--interactive\",\n        action=\"store_true\",\n        dest=\"interactive\",\n        help=\"Interactive mode.\",\n    )\n    args = parser.parse_args()\n\n    # Interactive mode adjustments\n    if args.interactive:\n        print(\"Input new value or press enter to keep current value.\")\n        pathname_params = [\n            \"start\",\n            \"output_dir\",\n            \"questions_pathname\",\n            \"model_config_pathname\",\n        ]\n        bool_params = [\"use_llm\", \"quiet\", \"single_process\", \"detailed\", \"html\"]\n        for param in pathname_params:\n            setattr(\n                args,\n                param,\n                input(f\"{param} [{getattr(args, param)}]: \") or getattr(args, param),\n            )\n        for param in bool_params:\n            setattr(\n                args,\n                param,\n                get_bool_from_input(\n                    input(f\"{param} [{getattr(args, param)}] (t or f): \"),\n                    getattr(args, param),\n                ),\n            )\n\n    # Validate the start directory\n    if not (os.path.isdir(args.start) or args.start.startswith(\"https://github.com/\")):\n        print(\n            f\"Invalid start directory '{args.start}'. Using current working directory.\"\n        )\n        args.start = os.getcwd()\n\n    # if the start directory is a github repository, clone it and change the start directory to the local repository\n    if args.start.startswith(\"https://github.com/\"):\n        args.start = clone_github_repo(args.start)\n\n    # Call py2dataset with args\n    py2dataset(\n        start_dir=args.start,\n        output_dir=args.output_dir,\n        questions_pathname=args.questions_pathname,\n        model_config_pathname=args.model_config_pathname,\n        use_llm=args.use_llm,\n        quiet=args.quiet,\n        single_process=args.single_process,\n        detailed=args.detailed,\n        html=args.html,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n\n```"
    },
    {
        "instruction": "Define a Python code file that is described as follows:\nThe purpose of the provided `save_output` python file is to handle data from extracted code elements from different modules to perform some processing actions as outlined below.\n- Processes data and stores them in different formats such as JSON, YAML, HTML, and Markdown files. \n- Handles the conversion of JSON files into HTML format. \n- Combines all JSON files in a given directory and removes duplicates from it. \n- Generates code graphs based on provided file details. \n- Saves Python file details as YAML files. \n- Saves instruction data as JSON files and generates code graphs.\n\nIn terms of Significance:\n- The `read_file` function is responsible for reading a file with a specific extension (JSON or YAML) and returning its contents as a dictionary. It takes in the path to the file as an argument and returns the content of that file as a dictionary. \n- The `write_file` function is responsible for writing data into a JSON or YAML file. It takes in two arguments, the data to be written and the path to the file. \n- The `convert_json_to_html` function converts JSON files within a given directory to HTML format. It accepts a directory as an argument and saves each converted file with a .html extension while preserving spacing and tabs for the 'input' field. \n- The `combine_json_files` function merges all JSON files in the directory, removes duplicates from the combined JSON files, writes them to 'instruct.json', and converts the merged JSON files to HTML format. It returns the 'instruct_list' datasets. \n- The `create_code_graph` function generates code graphs based on the provided file details. It accepts details of a Python file, a base name, and an output directory as arguments. \n- The `save_python_data` function saves the details of a Python file as a YAML file and saves instruction data as JSON files. It also generates and saves code graphs. \n\nIn terms of Inputs:\n- The input for the `read_file` function is a Path object to identify which file should be read based on its extension. It will read in JSON or YAML file and return its contents as a dictionary. \n- The input for the `write_file` function is two arguments, data to be written and the path to the file where it will be saved. It also includes an optional parameter `indent=4` for indentation level in case of writing JSON files. \n- The `convert_json_to_html` function takes a directory as input to identify which files should be converted into HTML format. \n- The `combine_json_files` function takes the directory path where all the JSON files are located, it will read in each file and merge them while removing duplicates from the combined dataset based on the instruction and output key/values pairs.\n- The input for `create_code_graph` includes the file details along with base name, an output directory path where to save the graph image. \n- The `save_python_data` function takes in a dictionary of Python file details, a list of instructions, and the relative path to the Python file and output directory for saving the YAML file, instruction data as JSON files, and code graphs.\n\nIn terms of Outputs:\n- The output of the `read_file` function is a dictionary containing the contents of the file based on its extension (JSON or YAML). \n- The output of the `write_file` function is None since it only writes data to a file and does not return anything. \n- The output of the `convert_json_to_html` function will create HTML files from JSON files with preserved spacing and tabs for the 'input' field. \n- The output of the `combine_json_files` function is a dictionary containing the 'instruct_list' datasets after merging all JSON files in the directory and removing duplicates based on instruction and output keys/values.\n- The `create_code_graph` function saves graphs as PNG images to an output directory using the base name specified, along with some other file details such as nodes, edges, etc. \n- The output of the `save_python_data` function is None since it only writes data and does not return anything. It also generates code graphs based on provided file details. \n{\n    \"Code Elements\": {\n        \"Dependencies\": \"logging, yaml, networkx, json, html, pathlib, matplotlib.pyplot, typing\",\n        \"Functions defined\": \"read_file, write_file, convert_json_to_html, preserve_spacing, convert_json_to_markdown, escape_markdown, combine_json_files, remove_duplicate_dataset_entries, create_code_graph, save_python_data\",\n        \"`combine_json_files` Calls\": \"logging.info, set, seen.add, result.append, Path, Path(directory).rglob, read_file, combined_data.extend, remove_duplicate_dataset_entries, combined_data.copy, item['instruction'].startswith, code_output.append, write_file, convert_json_to_html\",\n        \"`combine_json_files` Inputs\": \"directory, html\",\n        \"`combine_json_files` Returns\": \"{'instruct_list': instruct_data}, result\",\n        \"`combine_json_files` Variables\": \"result, purpose_data, file_path, seen, combined_data, instruction, instruct_data, code_output, json_file_data\",\n        \"`convert_json_to_html` Calls\": \"text.replace(, &nbsp;').replace, text.replace, Path(directory).rglob, Path, read_file, len, dataset[0].keys, escape, str, preserve_spacing, value.replace, row_parts.append, html_rows.append, .join, json_file.with_suffix, open, file.write, logging.info\",\n        \"`convert_json_to_html` Inputs\": \"directory\",\n        \"`convert_json_to_html` Returns\": \"text.replace(, &nbsp;').replace('\\\\\\\\t, &nbsp;' * tab_width)\",\n        \"`convert_json_to_html` Variables\": \"html_rows, html_file_path, dataset, column_count, row_parts, html_content, value, column_width\",\n        \"`convert_json_to_markdown` Calls\": \"text.replace, Path(directory).rglob, Path, read_file, dataset[0].keys, | '.join, len, escape_markdown, str, json_file.with_suffix, open, file.write, logging.error\",\n        \"`convert_json_to_markdown` Inputs\": \"directory\",\n        \"`convert_json_to_markdown` Returns\": \"text\",\n        \"`convert_json_to_markdown` Variables\": \"headers, text, markdown_file_path, row, dataset, markdown_content, markdown_special_chars\",\n        \"`create_code_graph` Calls\": \"nx.DiGraph, G.add_nodes_from, G.add_edge, edge.items, plt.figure, nx.spring_layout, nx.draw, G.edges, label.append, , .join, \\\\\\\\n'.join, nx.draw_networkx_edge_labels, plt.savefig, plt.close\",\n        \"`create_code_graph` Inputs\": \"file_details, base_name, output_subdir\",\n        \"`create_code_graph` Variables\": \"edge_labels, pos, graph_type, G, output_file, label\",\n        \"`escape_markdown` Calls\": \"text.replace\",\n        \"`escape_markdown` Inputs\": \"text\",\n        \"`escape_markdown` Returns\": \"text\",\n        \"`escape_markdown` Variables\": \"markdown_special_chars, text\",\n        \"`preserve_spacing` Calls\": \"text.replace(, &nbsp;').replace, text.replace\",\n        \"`preserve_spacing` Inputs\": \"text, tab_width\",\n        \"`preserve_spacing` Returns\": \"text.replace(, &nbsp;').replace('\\\\\\\\t, &nbsp;' * tab_width)\",\n        \"`read_file` Calls\": \"file_path.open, json.load, yaml.load\",\n        \"`read_file` Inputs\": \"file_path\",\n        \"`read_file` Returns\": \"{}, json.load(f), yaml.load(f, Loader=yaml.SafeLoader)\",\n        \"`read_file` Variables\": \"file_type\",\n        \"`remove_duplicate_dataset_entries` Calls\": \"set, seen.add, result.append\",\n        \"`remove_duplicate_dataset_entries` Inputs\": \"dataset, key1, key2\",\n        \"`remove_duplicate_dataset_entries` Returns\": \"result\",\n        \"`remove_duplicate_dataset_entries` Variables\": \"result, seen\",\n        \"`save_python_data` Calls\": \"Path, output_subdir.mkdir, zip, write_file, create_code_graph, logging.info\",\n        \"`save_python_data` Inputs\": \"file_details, instruct_list, base_name, relative_path, output_dir\",\n        \"`save_python_data` Variables\": \"file_names, contents, output_subdir\",\n        \"`write_file` Calls\": \"file_path.open, json.dump, yaml.dump\",\n        \"`write_file` Inputs\": \"data, file_path\",\n        \"`write_file` Variables\": \"file_type\"\n    }\n}",
        "output": "```python\n\"\"\"\nUtility functions for reading the input and saving the output of the py2dataset script.\nRequirements:\n[req01] The `read_file` function shall:\n        a. Accept a file path as an argument.\n        b. Read and return the contents of a JSON or YAML file as a dictionary.\n[req02] The `write_file` function shall:\n        a. Accept a dictionary and a file path as arguments.\n        b. Write the dictionary to a file in either JSON or YAML format.\n[req03] The `convert_json_to_html` function shall:\n        a. Convert JSON files within a given directory to HTML format.\n        b. Save each converted file with a .html extension.\n        c. Preserve spacing and tabs for the 'input' field.\n[req04] The `combine_json_files` function shall:\n        a. Accept a directory path as an argument.\n        b. Merge all JSON files in the directory.\n        c. Remove duplicates from the combined JSON files.\n        d. Write the combined data to 'instruct.json' files.\n        e. Convert the merged JSON files to HTML format.\n        f. Return the 'instruct_list' datasets.\n[req05] The `create_code_graph` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Generate code graphs based on the provided file details.\n        c. Save the graphs as PNG images in the specified output directory.\n[req06] The `save_python_data` function shall:\n        a. Accept details of a Python file, a base name, and an output directory as arguments.\n        b. Save the details of the Python file as a YAML file.\n        c. Save the instruction data as JSON files.\n        d. Generate and save code graphs.\n\"\"\"\nimport json\nimport logging\nfrom html import escape\nfrom pathlib import Path\nfrom typing import Dict, List\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport yaml\n\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == \"json\":\n            return json.load(f)\n        if file_type == \"yaml\":\n            return yaml.load(f, Loader=yaml.SafeLoader)\n        return {}\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file.\n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    Returns:\n        None\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open(\"w\") as f:\n        if file_type == \"json\":\n            json.dump(data, f, indent=4)\n        elif file_type == \"yaml\":\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef convert_json_to_html(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to HTML format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def preserve_spacing(text: str, tab_width: int = 4) -> str:\n        \"\"\"Preserve spaces and tabs in the provided text.\"\"\"\n        return text.replace(\" \", \"&nbsp;\").replace(\"\\t\", \"&nbsp;\" * tab_width)\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        try:\n            dataset = read_file(json_file)\n            if not dataset:\n                continue\n        except Exception:\n            continue\n\n        html_content = \"\"\"\n        <html>\n        <head>\n            <style>\n                table {border-collapse: collapse; width: 100%; table-layout: fixed;}\n                th, td {\n                    border: 1px solid black;\n                    padding: 8px;\n                    text-align: left;\n                    white-space: pre-line;\n                    vertical-align: top;\n                    word-wrap: break-word;\n                }\n            </style>\n        </head>\n        <body>\n            <table>\n                <thead>\n                    <tr>\n        \"\"\"\n        column_count = len(dataset[0].keys())\n        column_width = (\n            100 / column_count\n        )  # Calculate the width for each column based on the number of columns\n        for key in dataset[0].keys():\n            html_content += f\"<th style='width: {column_width}%;'>{key}</th>\"\n        html_content += \"\"\"\n                    </tr>\n                </thead>\n                <tbody>\n        \"\"\"\n        html_rows = []  # List to store each row's HTML content\n        for entry in dataset:\n            row_parts = [\"<tr>\"]\n            for key in entry:\n                # Convert \\n to HTML line breaks\n                value = escape(str(entry[key]))\n                value = preserve_spacing(value)\n                value = value.replace(\"\\n\", \"<br/>\")\n                row_parts.append(f\"<td>{value}</td>\")\n            row_parts.append(\"</tr>\")\n            html_rows.append(\"\".join(row_parts))  # Join all parts of the row and append to the list\n\n        # After the loop, join all rows\n        html_content += \"\".join(html_rows)\n\n        html_content += \"\"\"\n                </tbody>\n            </table>\n        </body>\n        </html>\n        \"\"\"\n        html_file_path = json_file.with_suffix(\".html\")\n        try:\n            with open(html_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(html_content)\n        except Exception:\n            logging.info(f\"Failed saving: {html_file_path}\")\n\n\ndef convert_json_to_markdown(directory: str) -> None:\n    \"\"\"\n    Convert JSON files within a given directory to Markdown format.\n    Args:\n        directory (str): The directory where the JSON files are located.\n    Returns:\n        None\n    \"\"\"\n\n    def escape_markdown(text: str) -> str:\n        \"\"\"Escape Markdown special characters in the provided text.\"\"\"\n        markdown_special_chars = \"\\\\`*_{}[]()#+-.!\"\n        for char in markdown_special_chars:\n            text = text.replace(char, f\"\\\\{char}\")\n        return text\n\n    for json_file in Path(directory).rglob(\"*.json\"):\n        dataset = read_file(json_file)\n        if not dataset:\n            continue\n\n        markdown_content = \"# Data Report\\n\\n\"\n        # Create Markdown table headers\n        headers = dataset[0].keys()\n        markdown_content += \"| \" + \" | \".join(headers) + \" |\\n\"\n        markdown_content += \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n\n        # Create Markdown table rows\n        for entry in dataset:\n            row = \"| \" + \" | \".join(escape_markdown(str(entry[key])) for key in headers) + \" |\\n\"\n            markdown_content += row\n\n        markdown_file_path = json_file.with_suffix(\".md\")\n        try:\n            with open(markdown_file_path, \"w\", encoding=\"utf-8\") as file:\n                file.write(markdown_content)\n        except Exception as e:\n            logging.error(f\"Failed to save Markdown file {markdown_file_path}: {e}\")\n            \n\ndef combine_json_files(directory: str, html: bool = False) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Combine all JSON files in the output directory into 'instruct.json', and\n    then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    Returns:\n        A dictionary containing the 'instruct_list' datasets.\n    \"\"\"\n    logging.info(f\"Combining JSON files in {directory}\")\n\n    def remove_duplicate_dataset_entries(\n        dataset: List[Dict], key1: str, key2: str\n    ) -> List[Dict]:\n        \"\"\"\n        Remove duplicate entries from the provided dataset based on the provided keys.\n        Args:\n            dataset (List[Dict]): The dataset to remove duplicates from.\n            key1 (str): The first key to check for duplicates.\n            key2 (str): The second key to check for duplicates.\n        Returns:\n            A dataset without duplicate entries.\n        \"\"\"\n        seen = set()\n        result = []\n        for item in dataset:\n            if (item[key1], item[key2]) not in seen:\n                seen.add((item[key1], item[key2]))\n                result.append(item)\n        return result\n\n    instruct_data = []\n    for file_name in [\"instruct.json\"]:\n        file_path = Path(directory) / file_name\n        combined_data = []\n        for json_file in Path(directory).rglob(f\"*.{file_name}\"):\n            try:\n                json_file_data = read_file(json_file)\n                combined_data.extend(json_file_data)\n                combined_data = remove_duplicate_dataset_entries(\n                    combined_data, \"instruction\", \"output\"\n                )\n                instruct_data = combined_data.copy()\n                purpose_data = [\n                    item\n                    for item in combined_data\n                    if item[\"instruction\"].startswith(\"1) Describe the Purpose\")\n                ]\n\n                code_output = []\n                for item in purpose_data:\n                    instruction = (\n                        \"Define a Python code file that is described as follows:\\n\"\n                        + item[\"output\"]\n                    )\n                    code_output.append(\n                        {\"instruction\": instruction, \"output\": item[\"input\"]}\n                    )\n                write_file(code_output, Path(directory) / \"training.json\")\n            except Exception as e:\n                logging.info(f\"Error processing: {json_file}: {e}\")\n\n        write_file(combined_data, file_path)\n\n    # Save html / markdown file for each json file in the output directory\n    # convert_json_to_markdown(directory)\n    if html:\n        logging.info(\"Converting JSON files to HTML\")\n        convert_json_to_html(directory)\n    \n    return {\"instruct_list\": instruct_data}\n    \n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    graph_type = \"entire_code_graph\"\n    output_file = output_subdir / f\"{base_name}.{graph_type}.png\"\n\n    # Create graphs, add nodes, and add edges\n    G = nx.DiGraph()\n    G.add_nodes_from(file_details[\"file_info\"][graph_type][\"nodes\"])\n    for edge in file_details[\"file_info\"][graph_type][\"edges\"]:\n        source, target = edge[\"source\"], edge[\"target\"]\n        if source in G.nodes and target in G.nodes:\n            G.add_edge(\n                source,\n                target,\n                **{\n                    k: v\n                    for k, v in edge.items()\n                    if k in [\"target_inputs\", \"target_returns\"]\n                },\n            )\n    # Draw graphs\n    plt.figure(figsize=(20, 20))\n    pos = nx.spring_layout(G)\n    nx.draw(\n        G,\n        pos,\n        with_labels=True,\n        font_weight=\"bold\",\n        font_size=8,\n        node_shape=\"s\",\n        node_size=500,\n        width=1,\n        arrowsize=12,\n    )\n    edge_labels = {}\n    for edge in G.edges(data=True):\n        label = []\n        if \"target_inputs\" in edge[2] and edge[2][\"target_inputs\"]:\n            label.append(f\"Inputs: {', '.join(edge[2]['target_inputs'])}\")\n        if \"target_returns\" in edge[2] and edge[2][\"target_returns\"]:\n            label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n        edge_labels[(edge[0], edge[1])] = \"\\n\".join(label)\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n    plt.savefig(output_file)  # Save the figure\n    plt.close()  # Close the figure\n\n\ndef save_python_data(\n    file_details: dict, instruct_list: list, base_name: str, relative_path: Path, output_dir: str\n) -> None:\n    \"\"\"\n    Save Python file details as a YAML file, the instruction data as a JSON file, and code graphs.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        instruct_list (list): The instruction data extracted from the Python file.\n        relative_path (Path): The relative path to the Python file.\n        output_dir (str): The directory where the output files will be saved.\n    Returns:\n        None\n    \"\"\"\n    # want output subdir to include the absolute path to the output dir + the directories between the output dir and the file\n    output_subdir = Path(output_dir) / relative_path.parent\n    output_subdir.mkdir(parents=True, exist_ok=True)\n\n    # write instrunct.json files\n    file_names = [f\"{base_name}.instruct.json\", f\"{base_name}.details.yaml\"]\n    contents = [instruct_list, file_details]\n\n    for file_name, content in zip(file_names, contents):\n        write_file(content, output_subdir / file_name)\n\n    try:\n        create_code_graph(file_details, base_name, output_subdir)\n    except Exception as e:\n        logging.info(f\"Error creating graph for {base_name}: {e}\")\n\n```"
    },
    {
        "instruction": "Define a Python code file that is described as follows:\nThe purpose of this Python file is to set up a package for distribution. It defines the metadata required by `setuptools` to create an installable package from the source code. The `setup()` function is used to specify information such as the name of the project, its version number, and any necessary dependencies or packages required for it to run correctly.\r\n\nIn this case, we can see that `py2dataset` is being set up with some requirements such as `matplotlib`, `networkx`, `ctransformers`, `PyYaml`, and `GitPython`. This Python file specifies what will be imported and the packages or modules used when `pip` installs `py2dataset` package on another computer. \n{\n    \"Code Elements\": {\n        \"Dependencies\": \"setuptools\"\n    }\n}",
        "output": "```python\nfrom setuptools import setup\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\"py2dataset\",\n    version=\"0.4\",\n    author=\"Jeff Meloy\",\n    author_email=\"jeffmeloy@gmail.com\",\n    description=\"A tool to generate structured datasets from Python source code\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/jeffmeloy/py2dataset\",\n    py_modules=[\"get_code_graph\", \"get_params\", \"get_python_datasets\", \"get_python_file_details\", \"py2dataset\", \"save_output\"],\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n    ],\n    python_requires=\">=3.8\",\n    install_requires=[\"matplotlib\", \"networkx\", \"ctransformers\", \"PyYaml\", \"GitPython\", ],\n    entry_points={\"console_scripts\": [\"py2dataset = py2dataset:main\"]},\n    packages=[\"py2dataset\"],\n    package_dir={\"py2dataset\": \".\\\\\"},\n)\n\n```"
    }
]