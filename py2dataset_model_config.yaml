prompt_template: "You are a master mathematician and Python programmer. Provide a brief yet thorough answer to the given question considering the context. 
                  \n### Instruction:\nGiven this context:\n'{context}'\nAnswer the following question and provide your reasoning: {query}\n### Response:"
inference_model:
  model_import_path: "ctransformers.AutoModelForCausalLM"
  model_params:
    ## USABLE OUTPUT WITH CURRENT PROMPT TEMPLATE
    model_path: "TheBloke/WizardCoder-Guanaco-15B-V1.1-GGML"
    model_type: "starcoder"
    local_files_only: false
    #model_path: "TheBloke/Starcoderplus-Guanaco-GPT4-15B-V1.0-GGML"
    #model_type: "starcoder"
    #local_files_only: false
    #model_path: "./models/wizardcoder-guanaco-15b-v1.1.ggmlv1.q4_0.bin"
    #model_type: "starcoder"
    #local_files_only: true
    #model_path: "./models/starcoderplus-guanaco-gpt4.ggmlv1.q4_0.bin"
    #model_type: "starcoder"
    #local_files_only: true
    ## NOT USABLE OUTPUT WITH CURRENT PROMPT TEMPLATE
    #model_path: "./models/vicuna-13b-v1.5-16k.ggmlv3.q4_1.bin"
    #model_type: "llama"
    #local_files_only: true
    #model_path: "TheBloke/stablecode-instruct-alpha-3b-GGML"
    #model_type: "gpt_neox"
    #local_files_only: false
    lib: "avx2"
    threads: 30
    batch_size: 32
    max_new_tokens: 2048
    #  Currently only LLaMA, MPT and Falcon models support the gpu_layers parameters.
    gpu_layers: 64 
    reset: True