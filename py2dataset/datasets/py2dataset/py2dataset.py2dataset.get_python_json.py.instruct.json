[
    {
        "instruction": "Dependencies of file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "\"\"\"\nGenerates JSON format question-answer pairs and instructions for a Python file\nRequirements:\n[req01] The get_model function shall accept a dictionary containing the model\n        configuration as an argument. It should import the specified model \n        class using the information from the model configuration, instantiate\n        the model class with the provided parameters from the model \n        configuration, and return the instantiated model.\n[req02] The PythonJsonGenerator class shall parse a Python file, a list of \n        questions, and generate JSON-formatted question-answer pairs and\n        instructions.\n[req03] The PythonJsonGenerator class shall add the generated question-answer\n        pairs to the qa_list attribute.\n[req04] The PythonJsonGenerator class shall add the generated instructions to\n        the instruct_list attribute.\n[req05] The PythonJsonGenerator class shall use a language model to generate\n        responses if the use_llm attribute is set to True.\n[req06] The PythonJsonGenerator class shall handle exceptions that may occur\n        during loading the language model.\n[req07] The PythonJsonGenerator class shall generate and return qa_list and\n        instruct_list when the generate method is called.\n[req08] The PythonJsonGenerator class shall use the get_model function to load\n        the specified language model according to the configuration file.\n[req09] The PythonJsonGenerator class shall use the loaded language model to\n        generate responses to the questions.\n[req10] The PythonJsonGenerator class shall handle exceptions that may occur\n        during the generation of responses.\n[req11] The get_python_json function shall create an instance of\n        PythonJsonGenerator and call the generate method.\n[req12] The get_python_json function shall return qa_list and instruct_list\n        generated by the PythonJsonGenerator instance.\n[req13] The PythonJsonGenerator class shall read the configuration file to set\n        the inference model parameters.\n\"\"\"\nimport re\nimport os\nimport json\nimport logging\nimport importlib\nimport yaml\nfrom typing import List, Dict\n\n# Set up logging\nlogging.basicConfig(\n    format='%(asctime)s - %(levelname)s - %(message)s', \n    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef get_model(model_config: dict, user_config: dict = None) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): A dictionary containing the configuration for the\n            model. It should include the import path for the model class and\n            parameters for instantiation.\n        user_config (dict): A dictionary containing user-provided configurations.\n            If provided, these configurations will override the defaults.\n    Returns:\n        object: An instance of the specified model class, or None if there was\n            an error.\n    \"\"\"\n    # If a user configuration is provided, override the defaults\n    if user_config is not None:\n        model_config.update(user_config)\n\n    # The rest of the function remains the same\n    model = None\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        module = importlib.import_module(module_name)\n    except ImportError as e:\n        print(f\"Failed to import module {module_name}. Error: {e}\")\n        return model\n    try:\n        ModelClass = getattr(module, class_name)\n    except AttributeError as e:\n        print(f\"Module {module_name} does not have a class named {class_name}. Error: {e}\")\n        return model\n    model_params = model_config['model_params']\n    try:\n        model = ModelClass.from_pretrained(model_params.pop('model_path'), **model_params)\n    except Exception as e:\n        print(f\"Failed to instantiate the model with the provided parameters. Error: {e}\")\n        return model\n    return model\n\nclass PythonJsonGenerator:\n    \"\"\"\n    A class used to generate JSON formatted dictionary outputs for a Python \n    file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict): A dictionary containing details of the Python\n            file.\n        base_name (str): The base name of the Python file.\n        questions (List): A list of questions for which responses are to be\n            generated.\n        qa_list (List): A list to store the generated question-answer pairs.\n        instruct_list (List): A list to store the generated instructions.\n        question_mapping (Dict): A dictionary mapping question types to their\n            corresponding keys in the file details.\n        use_llm (bool): A flag indicating whether to use a language model for\n            generating responses.\n        llm (AutoModelForCausalLM): The language model to be used for\n            generating responses.\n    Methods:\n        clean_and_get_unique_elements(input_str: str) -> str: Cleans an input \n            string and returns a string of unique elements.\n        add_to_list(list_to_update: List[Dict], query: str, response: str,\n            additional_field=None) -> List[Dict]: Adds a response to a list.\n        get_response_from_llm(query: str, context: str) -> str: Gets a \n            response from the language model.\n        get_variable_purpose(question_id: str, question_text: str, base_name:\n            str, name: str, info: Dict, context: str, variable_type: str) -> \n                None: Processes questions related to the purpose of a variable.\n        process_question(question_id: str, query: str, context: str, info) -> \n            None: Processes a question and adds the generated response to the\n            qa_list and instruct_list.\n        process_file_question(question_id: str, question_text: str) -> None:\n            Processes questions related to a file.\n        process_func_class_question(question_type: str, question_id: str, \n            question_text: str) -> None: Processes questions related to a \n            function or class.\n        generate() -> Tuple[List[Dict], List[Dict]]: Generates responses for\n            all the questions and returns the qa_list and instruct_list.\n    \"\"\"\n    def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, config: Dict):\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.qa_list = []\n        self.instruct_list = []\n        self.question_mapping = {\n            'file': 'file',\n            'function': 'functions',\n            'class': 'classes',\n            'method': 'classes'\n        }\n        self.use_llm = use_llm\n        self.config = config\n        if self.use_llm:\n            try:\n                self.llm_config = config['inference_model']\n                self.llm = get_model(self.llm_config)\n            except (FileNotFoundError, yaml.YAMLError, ImportError, AttributeError) as e:\n                logger.error(f'Failed to load configuration file: {e}')\n                self.use_llm = False\n                self.llm_config = None\n                self.llm = None\n        else:\n            self.llm = None\n\n    @staticmethod\n    def clean_and_get_unique_elements(input_str: str) -> str:\n        cleaned_elements = set(re.sub(r'[^\\w\\-_>\\s:/.]', '', element.strip())\n                               for element in re.sub(r'\\s+', ' ', input_str).split(','))\n        return ', '.join(cleaned_elements)\n\n    @staticmethod\n    def add_to_list(list_to_update: List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:\n        if response and response.strip() and response != 'None':\n            list_to_update.append(\n                {'instruction': query, 'input' : additional_field, 'output': response}\n                if additional_field else\n                {'question': query, 'answer': response}\n            )\n        return list_to_update\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        if not self.llm:\n            logger.error('AI model not available.')\n            return ''\n        prompt = self.config[\"prompt_template\"].format(context=context, query=query)\n        logging.info(f'Query: {query}')\n        response = self.llm(prompt)\n        logging.info(f'Response: {response}')\n        return response\n\n    def process_items(self, question_id: str, question_text: str, base_name: str, name: str, info: Dict, context: str, item_type: str) -> None:\n        if info[item_type]:\n            items = [item.strip() for item in self.clean_and_get_unique_elements(str(info[item_type])).split(',') if item]\n            for item in items:\n                query = question_text.format(filename=base_name, **{f'{item_type.split(\"_\")[0]}_name': name, f'{item_type.split(\"_\")[0]}_variable': item})\n                self.process_question(question_id, query, context, info)\n\n    def process_question(self, question_id: str, query: str, context: str, info) -> None:\n        if question_id.endswith('code_graph'):\n            response = info.get(question_id, {})\n        else:\n            response = self.get_response_from_llm(query, context) if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id, '')))\n        if response and response != 'None':\n            response_str = str(response)\n            response_str = response_str.strip()\n            if response_str:\n                self.qa_list.append({'question': query, 'answer': response_str})\n                self.instruct_list.append({'instruction': query, 'input': context, 'output': response_str})\n\n    def process_file_question(self, question_id: str, question_text: str) -> None:\n        query = question_text.format(filename=self.base_name)\n        context = self.file_details['file_info']['file_code']\n        info = self.file_details['file_info']\n        self.process_question(question_id, query, context, info)\n\n    def process_func_class_question(self, question_type: str, question_id: str, question_text: str) -> None:\n        if question_type == 'method':  \n            for class_name, class_info in self.file_details['classes'].items():\n                for key, method_info in class_info.items():\n                    if key.startswith('class_method_'):\n                        method_name = key[len('class_method_'):]\n                        context = method_info['method_code']\n                        mapping = {'class_name': class_name, 'method_name': method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_id, query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n                context = info[f'{question_type}_code']\n                mapping = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_variable_purpose' and self.use_llm:\n                    self.process_items(question_id, question_text, self.base_name, name, info, context, f'{question_type}_variables')\n                elif question_id != f'{question_type}_variable_purpose':\n                    query = question_text.format(filename=self.base_name, **mapping)\n                    self.process_question(question_id, query, context, info)\n\n    def generate(self) -> tuple[List[Dict], List[Dict]]:\n        for question in self.questions:\n            question_id = question['id']\n            question_text = question['text']\n            question_type = question['type']\n            if question_type == 'file':\n                self.process_file_question(question_id, question_text)\n            elif question_type in ['function', 'class', 'method']:\n                self.process_func_class_question(question_type, question_id, question_text)\n        return self.qa_list, self.instruct_list\n\ndef get_python_json(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, model_config: dict = None) -> tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base name.\n        questions (List[Dict]): The list of questions.\n        use_llm (bool): Whether to use the language model.\n        user_config (dict): User-provided model configurations.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    # Load default configuration from the YAML file\n    with open('py2dataset_model_config.yaml', 'r') as config_file:\n        config = yaml.safe_load(config_file)\n\n    # If user-provided configurations are given, update the defaults\n    if model_config is not None:\n        config.update(model_config)\n\n    generator = PythonJsonGenerator(file_path, file_details, base_name, questions, use_llm, config)\n    return generator.generate()\n",
        "output": "typing, json, re, importlib, logging, os, yaml"
    },
    {
        "instruction": "Structural graph of the relationships between the functions and classes defined in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "\"\"\"\nGenerates JSON format question-answer pairs and instructions for a Python file\nRequirements:\n[req01] The get_model function shall accept a dictionary containing the model\n        configuration as an argument. It should import the specified model \n        class using the information from the model configuration, instantiate\n        the model class with the provided parameters from the model \n        configuration, and return the instantiated model.\n[req02] The PythonJsonGenerator class shall parse a Python file, a list of \n        questions, and generate JSON-formatted question-answer pairs and\n        instructions.\n[req03] The PythonJsonGenerator class shall add the generated question-answer\n        pairs to the qa_list attribute.\n[req04] The PythonJsonGenerator class shall add the generated instructions to\n        the instruct_list attribute.\n[req05] The PythonJsonGenerator class shall use a language model to generate\n        responses if the use_llm attribute is set to True.\n[req06] The PythonJsonGenerator class shall handle exceptions that may occur\n        during loading the language model.\n[req07] The PythonJsonGenerator class shall generate and return qa_list and\n        instruct_list when the generate method is called.\n[req08] The PythonJsonGenerator class shall use the get_model function to load\n        the specified language model according to the configuration file.\n[req09] The PythonJsonGenerator class shall use the loaded language model to\n        generate responses to the questions.\n[req10] The PythonJsonGenerator class shall handle exceptions that may occur\n        during the generation of responses.\n[req11] The get_python_json function shall create an instance of\n        PythonJsonGenerator and call the generate method.\n[req12] The get_python_json function shall return qa_list and instruct_list\n        generated by the PythonJsonGenerator instance.\n[req13] The PythonJsonGenerator class shall read the configuration file to set\n        the inference model parameters.\n\"\"\"\nimport re\nimport os\nimport json\nimport logging\nimport importlib\nimport yaml\nfrom typing import List, Dict\n\n# Set up logging\nlogging.basicConfig(\n    format='%(asctime)s - %(levelname)s - %(message)s', \n    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef get_model(model_config: dict, user_config: dict = None) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): A dictionary containing the configuration for the\n            model. It should include the import path for the model class and\n            parameters for instantiation.\n        user_config (dict): A dictionary containing user-provided configurations.\n            If provided, these configurations will override the defaults.\n    Returns:\n        object: An instance of the specified model class, or None if there was\n            an error.\n    \"\"\"\n    # If a user configuration is provided, override the defaults\n    if user_config is not None:\n        model_config.update(user_config)\n\n    # The rest of the function remains the same\n    model = None\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        module = importlib.import_module(module_name)\n    except ImportError as e:\n        print(f\"Failed to import module {module_name}. Error: {e}\")\n        return model\n    try:\n        ModelClass = getattr(module, class_name)\n    except AttributeError as e:\n        print(f\"Module {module_name} does not have a class named {class_name}. Error: {e}\")\n        return model\n    model_params = model_config['model_params']\n    try:\n        model = ModelClass.from_pretrained(model_params.pop('model_path'), **model_params)\n    except Exception as e:\n        print(f\"Failed to instantiate the model with the provided parameters. Error: {e}\")\n        return model\n    return model\n\nclass PythonJsonGenerator:\n    \"\"\"\n    A class used to generate JSON formatted dictionary outputs for a Python \n    file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict): A dictionary containing details of the Python\n            file.\n        base_name (str): The base name of the Python file.\n        questions (List): A list of questions for which responses are to be\n            generated.\n        qa_list (List): A list to store the generated question-answer pairs.\n        instruct_list (List): A list to store the generated instructions.\n        question_mapping (Dict): A dictionary mapping question types to their\n            corresponding keys in the file details.\n        use_llm (bool): A flag indicating whether to use a language model for\n            generating responses.\n        llm (AutoModelForCausalLM): The language model to be used for\n            generating responses.\n    Methods:\n        clean_and_get_unique_elements(input_str: str) -> str: Cleans an input \n            string and returns a string of unique elements.\n        add_to_list(list_to_update: List[Dict], query: str, response: str,\n            additional_field=None) -> List[Dict]: Adds a response to a list.\n        get_response_from_llm(query: str, context: str) -> str: Gets a \n            response from the language model.\n        get_variable_purpose(question_id: str, question_text: str, base_name:\n            str, name: str, info: Dict, context: str, variable_type: str) -> \n                None: Processes questions related to the purpose of a variable.\n        process_question(question_id: str, query: str, context: str, info) -> \n            None: Processes a question and adds the generated response to the\n            qa_list and instruct_list.\n        process_file_question(question_id: str, question_text: str) -> None:\n            Processes questions related to a file.\n        process_func_class_question(question_type: str, question_id: str, \n            question_text: str) -> None: Processes questions related to a \n            function or class.\n        generate() -> Tuple[List[Dict], List[Dict]]: Generates responses for\n            all the questions and returns the qa_list and instruct_list.\n    \"\"\"\n    def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, config: Dict):\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.qa_list = []\n        self.instruct_list = []\n        self.question_mapping = {\n            'file': 'file',\n            'function': 'functions',\n            'class': 'classes',\n            'method': 'classes'\n        }\n        self.use_llm = use_llm\n        self.config = config\n        if self.use_llm:\n            try:\n                self.llm_config = config['inference_model']\n                self.llm = get_model(self.llm_config)\n            except (FileNotFoundError, yaml.YAMLError, ImportError, AttributeError) as e:\n                logger.error(f'Failed to load configuration file: {e}')\n                self.use_llm = False\n                self.llm_config = None\n                self.llm = None\n        else:\n            self.llm = None\n\n    @staticmethod\n    def clean_and_get_unique_elements(input_str: str) -> str:\n        cleaned_elements = set(re.sub(r'[^\\w\\-_>\\s:/.]', '', element.strip())\n                               for element in re.sub(r'\\s+', ' ', input_str).split(','))\n        return ', '.join(cleaned_elements)\n\n    @staticmethod\n    def add_to_list(list_to_update: List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:\n        if response and response.strip() and response != 'None':\n            list_to_update.append(\n                {'instruction': query, 'input' : additional_field, 'output': response}\n                if additional_field else\n                {'question': query, 'answer': response}\n            )\n        return list_to_update\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        if not self.llm:\n            logger.error('AI model not available.')\n            return ''\n        prompt = self.config[\"prompt_template\"].format(context=context, query=query)\n        logging.info(f'Query: {query}')\n        response = self.llm(prompt)\n        logging.info(f'Response: {response}')\n        return response\n\n    def process_items(self, question_id: str, question_text: str, base_name: str, name: str, info: Dict, context: str, item_type: str) -> None:\n        if info[item_type]:\n            items = [item.strip() for item in self.clean_and_get_unique_elements(str(info[item_type])).split(',') if item]\n            for item in items:\n                query = question_text.format(filename=base_name, **{f'{item_type.split(\"_\")[0]}_name': name, f'{item_type.split(\"_\")[0]}_variable': item})\n                self.process_question(question_id, query, context, info)\n\n    def process_question(self, question_id: str, query: str, context: str, info) -> None:\n        if question_id.endswith('code_graph'):\n            response = info.get(question_id, {})\n        else:\n            response = self.get_response_from_llm(query, context) if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id, '')))\n        if response and response != 'None':\n            response_str = str(response)\n            response_str = response_str.strip()\n            if response_str:\n                self.qa_list.append({'question': query, 'answer': response_str})\n                self.instruct_list.append({'instruction': query, 'input': context, 'output': response_str})\n\n    def process_file_question(self, question_id: str, question_text: str) -> None:\n        query = question_text.format(filename=self.base_name)\n        context = self.file_details['file_info']['file_code']\n        info = self.file_details['file_info']\n        self.process_question(question_id, query, context, info)\n\n    def process_func_class_question(self, question_type: str, question_id: str, question_text: str) -> None:\n        if question_type == 'method':  \n            for class_name, class_info in self.file_details['classes'].items():\n                for key, method_info in class_info.items():\n                    if key.startswith('class_method_'):\n                        method_name = key[len('class_method_'):]\n                        context = method_info['method_code']\n                        mapping = {'class_name': class_name, 'method_name': method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_id, query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n                context = info[f'{question_type}_code']\n                mapping = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_variable_purpose' and self.use_llm:\n                    self.process_items(question_id, question_text, self.base_name, name, info, context, f'{question_type}_variables')\n                elif question_id != f'{question_type}_variable_purpose':\n                    query = question_text.format(filename=self.base_name, **mapping)\n                    self.process_question(question_id, query, context, info)\n\n    def generate(self) -> tuple[List[Dict], List[Dict]]:\n        for question in self.questions:\n            question_id = question['id']\n            question_text = question['text']\n            question_type = question['type']\n            if question_type == 'file':\n                self.process_file_question(question_id, question_text)\n            elif question_type in ['function', 'class', 'method']:\n                self.process_func_class_question(question_type, question_id, question_text)\n        return self.qa_list, self.instruct_list\n\ndef get_python_json(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, model_config: dict = None) -> tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base name.\n        questions (List[Dict]): The list of questions.\n        use_llm (bool): Whether to use the language model.\n        user_config (dict): User-provided model configurations.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    # Load default configuration from the YAML file\n    with open('py2dataset_model_config.yaml', 'r') as config_file:\n        config = yaml.safe_load(config_file)\n\n    # If user-provided configurations are given, update the defaults\n    if model_config is not None:\n        config.update(model_config)\n\n    generator = PythonJsonGenerator(file_path, file_details, base_name, questions, use_llm, config)\n    return generator.generate()\n",
        "output": "{'nodes': ['get_model', 'get_python_json', 'PythonJsonGenerator', 'PythonJsonGenerator.__init__', 'PythonJsonGenerator.clean_and_get_unique_elements', 'PythonJsonGenerator.add_to_list', 'PythonJsonGenerator.get_response_from_llm', 'PythonJsonGenerator.process_items', 'PythonJsonGenerator.process_question', 'PythonJsonGenerator.process_file_question', 'PythonJsonGenerator.process_func_class_question', 'PythonJsonGenerator.generate'], 'edges': [{'source': 'get_python_json', 'target': 'PythonJsonGenerator'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.__init__'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.clean_and_get_unique_elements'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.add_to_list'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.get_response_from_llm'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.process_items'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.process_question'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.process_file_question'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.process_func_class_question'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.generate'}, {'source': 'PythonJsonGenerator.__init__', 'target': 'get_model', 'target_input': ['model_config', 'user_config'], 'target_returns': ['model', 'model', 'model', 'model']}]}"
    },
    {
        "instruction": "Structural graph of the relationships between the functions and classes defined and used in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "\"\"\"\nGenerates JSON format question-answer pairs and instructions for a Python file\nRequirements:\n[req01] The get_model function shall accept a dictionary containing the model\n        configuration as an argument. It should import the specified model \n        class using the information from the model configuration, instantiate\n        the model class with the provided parameters from the model \n        configuration, and return the instantiated model.\n[req02] The PythonJsonGenerator class shall parse a Python file, a list of \n        questions, and generate JSON-formatted question-answer pairs and\n        instructions.\n[req03] The PythonJsonGenerator class shall add the generated question-answer\n        pairs to the qa_list attribute.\n[req04] The PythonJsonGenerator class shall add the generated instructions to\n        the instruct_list attribute.\n[req05] The PythonJsonGenerator class shall use a language model to generate\n        responses if the use_llm attribute is set to True.\n[req06] The PythonJsonGenerator class shall handle exceptions that may occur\n        during loading the language model.\n[req07] The PythonJsonGenerator class shall generate and return qa_list and\n        instruct_list when the generate method is called.\n[req08] The PythonJsonGenerator class shall use the get_model function to load\n        the specified language model according to the configuration file.\n[req09] The PythonJsonGenerator class shall use the loaded language model to\n        generate responses to the questions.\n[req10] The PythonJsonGenerator class shall handle exceptions that may occur\n        during the generation of responses.\n[req11] The get_python_json function shall create an instance of\n        PythonJsonGenerator and call the generate method.\n[req12] The get_python_json function shall return qa_list and instruct_list\n        generated by the PythonJsonGenerator instance.\n[req13] The PythonJsonGenerator class shall read the configuration file to set\n        the inference model parameters.\n\"\"\"\nimport re\nimport os\nimport json\nimport logging\nimport importlib\nimport yaml\nfrom typing import List, Dict\n\n# Set up logging\nlogging.basicConfig(\n    format='%(asctime)s - %(levelname)s - %(message)s', \n    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef get_model(model_config: dict, user_config: dict = None) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): A dictionary containing the configuration for the\n            model. It should include the import path for the model class and\n            parameters for instantiation.\n        user_config (dict): A dictionary containing user-provided configurations.\n            If provided, these configurations will override the defaults.\n    Returns:\n        object: An instance of the specified model class, or None if there was\n            an error.\n    \"\"\"\n    # If a user configuration is provided, override the defaults\n    if user_config is not None:\n        model_config.update(user_config)\n\n    # The rest of the function remains the same\n    model = None\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        module = importlib.import_module(module_name)\n    except ImportError as e:\n        print(f\"Failed to import module {module_name}. Error: {e}\")\n        return model\n    try:\n        ModelClass = getattr(module, class_name)\n    except AttributeError as e:\n        print(f\"Module {module_name} does not have a class named {class_name}. Error: {e}\")\n        return model\n    model_params = model_config['model_params']\n    try:\n        model = ModelClass.from_pretrained(model_params.pop('model_path'), **model_params)\n    except Exception as e:\n        print(f\"Failed to instantiate the model with the provided parameters. Error: {e}\")\n        return model\n    return model\n\nclass PythonJsonGenerator:\n    \"\"\"\n    A class used to generate JSON formatted dictionary outputs for a Python \n    file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict): A dictionary containing details of the Python\n            file.\n        base_name (str): The base name of the Python file.\n        questions (List): A list of questions for which responses are to be\n            generated.\n        qa_list (List): A list to store the generated question-answer pairs.\n        instruct_list (List): A list to store the generated instructions.\n        question_mapping (Dict): A dictionary mapping question types to their\n            corresponding keys in the file details.\n        use_llm (bool): A flag indicating whether to use a language model for\n            generating responses.\n        llm (AutoModelForCausalLM): The language model to be used for\n            generating responses.\n    Methods:\n        clean_and_get_unique_elements(input_str: str) -> str: Cleans an input \n            string and returns a string of unique elements.\n        add_to_list(list_to_update: List[Dict], query: str, response: str,\n            additional_field=None) -> List[Dict]: Adds a response to a list.\n        get_response_from_llm(query: str, context: str) -> str: Gets a \n            response from the language model.\n        get_variable_purpose(question_id: str, question_text: str, base_name:\n            str, name: str, info: Dict, context: str, variable_type: str) -> \n                None: Processes questions related to the purpose of a variable.\n        process_question(question_id: str, query: str, context: str, info) -> \n            None: Processes a question and adds the generated response to the\n            qa_list and instruct_list.\n        process_file_question(question_id: str, question_text: str) -> None:\n            Processes questions related to a file.\n        process_func_class_question(question_type: str, question_id: str, \n            question_text: str) -> None: Processes questions related to a \n            function or class.\n        generate() -> Tuple[List[Dict], List[Dict]]: Generates responses for\n            all the questions and returns the qa_list and instruct_list.\n    \"\"\"\n    def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, config: Dict):\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.qa_list = []\n        self.instruct_list = []\n        self.question_mapping = {\n            'file': 'file',\n            'function': 'functions',\n            'class': 'classes',\n            'method': 'classes'\n        }\n        self.use_llm = use_llm\n        self.config = config\n        if self.use_llm:\n            try:\n                self.llm_config = config['inference_model']\n                self.llm = get_model(self.llm_config)\n            except (FileNotFoundError, yaml.YAMLError, ImportError, AttributeError) as e:\n                logger.error(f'Failed to load configuration file: {e}')\n                self.use_llm = False\n                self.llm_config = None\n                self.llm = None\n        else:\n            self.llm = None\n\n    @staticmethod\n    def clean_and_get_unique_elements(input_str: str) -> str:\n        cleaned_elements = set(re.sub(r'[^\\w\\-_>\\s:/.]', '', element.strip())\n                               for element in re.sub(r'\\s+', ' ', input_str).split(','))\n        return ', '.join(cleaned_elements)\n\n    @staticmethod\n    def add_to_list(list_to_update: List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:\n        if response and response.strip() and response != 'None':\n            list_to_update.append(\n                {'instruction': query, 'input' : additional_field, 'output': response}\n                if additional_field else\n                {'question': query, 'answer': response}\n            )\n        return list_to_update\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        if not self.llm:\n            logger.error('AI model not available.')\n            return ''\n        prompt = self.config[\"prompt_template\"].format(context=context, query=query)\n        logging.info(f'Query: {query}')\n        response = self.llm(prompt)\n        logging.info(f'Response: {response}')\n        return response\n\n    def process_items(self, question_id: str, question_text: str, base_name: str, name: str, info: Dict, context: str, item_type: str) -> None:\n        if info[item_type]:\n            items = [item.strip() for item in self.clean_and_get_unique_elements(str(info[item_type])).split(',') if item]\n            for item in items:\n                query = question_text.format(filename=base_name, **{f'{item_type.split(\"_\")[0]}_name': name, f'{item_type.split(\"_\")[0]}_variable': item})\n                self.process_question(question_id, query, context, info)\n\n    def process_question(self, question_id: str, query: str, context: str, info) -> None:\n        if question_id.endswith('code_graph'):\n            response = info.get(question_id, {})\n        else:\n            response = self.get_response_from_llm(query, context) if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id, '')))\n        if response and response != 'None':\n            response_str = str(response)\n            response_str = response_str.strip()\n            if response_str:\n                self.qa_list.append({'question': query, 'answer': response_str})\n                self.instruct_list.append({'instruction': query, 'input': context, 'output': response_str})\n\n    def process_file_question(self, question_id: str, question_text: str) -> None:\n        query = question_text.format(filename=self.base_name)\n        context = self.file_details['file_info']['file_code']\n        info = self.file_details['file_info']\n        self.process_question(question_id, query, context, info)\n\n    def process_func_class_question(self, question_type: str, question_id: str, question_text: str) -> None:\n        if question_type == 'method':  \n            for class_name, class_info in self.file_details['classes'].items():\n                for key, method_info in class_info.items():\n                    if key.startswith('class_method_'):\n                        method_name = key[len('class_method_'):]\n                        context = method_info['method_code']\n                        mapping = {'class_name': class_name, 'method_name': method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_id, query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n                context = info[f'{question_type}_code']\n                mapping = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_variable_purpose' and self.use_llm:\n                    self.process_items(question_id, question_text, self.base_name, name, info, context, f'{question_type}_variables')\n                elif question_id != f'{question_type}_variable_purpose':\n                    query = question_text.format(filename=self.base_name, **mapping)\n                    self.process_question(question_id, query, context, info)\n\n    def generate(self) -> tuple[List[Dict], List[Dict]]:\n        for question in self.questions:\n            question_id = question['id']\n            question_text = question['text']\n            question_type = question['type']\n            if question_type == 'file':\n                self.process_file_question(question_id, question_text)\n            elif question_type in ['function', 'class', 'method']:\n                self.process_func_class_question(question_type, question_id, question_text)\n        return self.qa_list, self.instruct_list\n\ndef get_python_json(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, model_config: dict = None) -> tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base name.\n        questions (List[Dict]): The list of questions.\n        use_llm (bool): Whether to use the language model.\n        user_config (dict): User-provided model configurations.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    # Load default configuration from the YAML file\n    with open('py2dataset_model_config.yaml', 'r') as config_file:\n        config = yaml.safe_load(config_file)\n\n    # If user-provided configurations are given, update the defaults\n    if model_config is not None:\n        config.update(model_config)\n\n    generator = PythonJsonGenerator(file_path, file_details, base_name, questions, use_llm, config)\n    return generator.generate()\n",
        "output": "{'nodes': ['get_model', 'get_python_json', 'PythonJsonGenerator', 'PythonJsonGenerator.__init__', 'PythonJsonGenerator.clean_and_get_unique_elements', 'PythonJsonGenerator.add_to_list', 'PythonJsonGenerator.get_response_from_llm', 'PythonJsonGenerator.process_items', 'PythonJsonGenerator.process_question', 'PythonJsonGenerator.process_file_question', 'PythonJsonGenerator.process_func_class_question', 'PythonJsonGenerator.generate', 'model_config.update', 'ModelClass.from_pretrained', 'model_params.pop', 'getattr', 'print', \"model_config['model_import_path'].rsplit\", 'importlib.import_module', 'config.update', 'open', 'yaml.safe_load', 'generator.generate', 'logger.error', \"re.sub('\\\\\\\\s+', ' ', input_str).split\", 'element.strip', \"', '.join\", 're.sub', 'set', 'list_to_update.append', 'response.strip', 'self.llm', \"self.config['prompt_template'].format\", 'logging.info', 'question_text.format', 'item.strip', 'str', 'self.process_question', 'self.clean_and_get_unique_elements(str(info[item_type])).split', 'self.clean_and_get_unique_elements', 'item_type.split', 'info.get', 'self.get_response_from_llm', 'self.qa_list.append', 'response_str.strip', 'self.instruct_list.append', 'question_id.endswith', 'self.process_items', \"self.file_details['classes'].items\", 'class_info.items', 'self.file_details[self.question_mapping[question_type]].items', 'len', 'key.startswith', 'self.process_file_question', 'self.process_func_class_question'], 'edges': [{'source': 'get_model', 'target': 'model_config.update'}, {'source': 'get_model', 'target': 'ModelClass.from_pretrained'}, {'source': 'get_model', 'target': 'model_params.pop'}, {'source': 'get_model', 'target': 'getattr'}, {'source': 'get_model', 'target': 'print'}, {'source': 'get_model', 'target': \"model_config['model_import_path'].rsplit\"}, {'source': 'get_model', 'target': 'importlib.import_module'}, {'source': 'get_python_json', 'target': 'config.update'}, {'source': 'get_python_json', 'target': 'open'}, {'source': 'get_python_json', 'target': 'yaml.safe_load'}, {'source': 'get_python_json', 'target': 'PythonJsonGenerator'}, {'source': 'get_python_json', 'target': 'generator.generate'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.__init__'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.clean_and_get_unique_elements'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.add_to_list'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.get_response_from_llm'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.process_items'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.process_question'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.process_file_question'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.process_func_class_question'}, {'source': 'PythonJsonGenerator', 'target': 'PythonJsonGenerator.generate'}, {'source': 'PythonJsonGenerator.__init__', 'target': 'logger.error'}, {'source': 'PythonJsonGenerator.__init__', 'target': 'get_model', 'target_input': ['model_config', 'user_config'], 'target_returns': ['model', 'model', 'model', 'model']}, {'source': 'PythonJsonGenerator.clean_and_get_unique_elements', 'target': \"re.sub('\\\\\\\\s+', ' ', input_str).split\"}, {'source': 'PythonJsonGenerator.clean_and_get_unique_elements', 'target': 'element.strip'}, {'source': 'PythonJsonGenerator.clean_and_get_unique_elements', 'target': \"', '.join\"}, {'source': 'PythonJsonGenerator.clean_and_get_unique_elements', 'target': 're.sub'}, {'source': 'PythonJsonGenerator.clean_and_get_unique_elements', 'target': 'set'}, {'source': 'PythonJsonGenerator.add_to_list', 'target': 'list_to_update.append'}, {'source': 'PythonJsonGenerator.add_to_list', 'target': 'response.strip'}, {'source': 'PythonJsonGenerator.get_response_from_llm', 'target': 'logger.error'}, {'source': 'PythonJsonGenerator.get_response_from_llm', 'target': 'self.llm'}, {'source': 'PythonJsonGenerator.get_response_from_llm', 'target': \"self.config['prompt_template'].format\"}, {'source': 'PythonJsonGenerator.get_response_from_llm', 'target': 'logging.info'}, {'source': 'PythonJsonGenerator.process_items', 'target': 'question_text.format'}, {'source': 'PythonJsonGenerator.process_items', 'target': 'item.strip'}, {'source': 'PythonJsonGenerator.process_items', 'target': 'str'}, {'source': 'PythonJsonGenerator.process_items', 'target': 'self.process_question'}, {'source': 'PythonJsonGenerator.process_items', 'target': 'self.clean_and_get_unique_elements(str(info[item_type])).split'}, {'source': 'PythonJsonGenerator.process_items', 'target': 'self.clean_and_get_unique_elements'}, {'source': 'PythonJsonGenerator.process_items', 'target': 'item_type.split'}, {'source': 'PythonJsonGenerator.process_question', 'target': 'info.get'}, {'source': 'PythonJsonGenerator.process_question', 'target': 'self.get_response_from_llm'}, {'source': 'PythonJsonGenerator.process_question', 'target': 'str'}, {'source': 'PythonJsonGenerator.process_question', 'target': 'self.qa_list.append'}, {'source': 'PythonJsonGenerator.process_question', 'target': 'response_str.strip'}, {'source': 'PythonJsonGenerator.process_question', 'target': 'self.instruct_list.append'}, {'source': 'PythonJsonGenerator.process_question', 'target': 'self.clean_and_get_unique_elements'}, {'source': 'PythonJsonGenerator.process_question', 'target': 'question_id.endswith'}, {'source': 'PythonJsonGenerator.process_file_question', 'target': 'question_text.format'}, {'source': 'PythonJsonGenerator.process_file_question', 'target': 'self.process_question'}, {'source': 'PythonJsonGenerator.process_func_class_question', 'target': 'question_text.format'}, {'source': 'PythonJsonGenerator.process_func_class_question', 'target': 'self.process_items'}, {'source': 'PythonJsonGenerator.process_func_class_question', 'target': \"self.file_details['classes'].items\"}, {'source': 'PythonJsonGenerator.process_func_class_question', 'target': 'class_info.items'}, {'source': 'PythonJsonGenerator.process_func_class_question', 'target': 'self.file_details[self.question_mapping[question_type]].items'}, {'source': 'PythonJsonGenerator.process_func_class_question', 'target': 'self.process_question'}, {'source': 'PythonJsonGenerator.process_func_class_question', 'target': 'len'}, {'source': 'PythonJsonGenerator.process_func_class_question', 'target': 'key.startswith'}, {'source': 'PythonJsonGenerator.generate', 'target': 'self.process_file_question'}, {'source': 'PythonJsonGenerator.generate', 'target': 'self.process_func_class_question'}]}"
    },
    {
        "instruction": "Funtions in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "\"\"\"\nGenerates JSON format question-answer pairs and instructions for a Python file\nRequirements:\n[req01] The get_model function shall accept a dictionary containing the model\n        configuration as an argument. It should import the specified model \n        class using the information from the model configuration, instantiate\n        the model class with the provided parameters from the model \n        configuration, and return the instantiated model.\n[req02] The PythonJsonGenerator class shall parse a Python file, a list of \n        questions, and generate JSON-formatted question-answer pairs and\n        instructions.\n[req03] The PythonJsonGenerator class shall add the generated question-answer\n        pairs to the qa_list attribute.\n[req04] The PythonJsonGenerator class shall add the generated instructions to\n        the instruct_list attribute.\n[req05] The PythonJsonGenerator class shall use a language model to generate\n        responses if the use_llm attribute is set to True.\n[req06] The PythonJsonGenerator class shall handle exceptions that may occur\n        during loading the language model.\n[req07] The PythonJsonGenerator class shall generate and return qa_list and\n        instruct_list when the generate method is called.\n[req08] The PythonJsonGenerator class shall use the get_model function to load\n        the specified language model according to the configuration file.\n[req09] The PythonJsonGenerator class shall use the loaded language model to\n        generate responses to the questions.\n[req10] The PythonJsonGenerator class shall handle exceptions that may occur\n        during the generation of responses.\n[req11] The get_python_json function shall create an instance of\n        PythonJsonGenerator and call the generate method.\n[req12] The get_python_json function shall return qa_list and instruct_list\n        generated by the PythonJsonGenerator instance.\n[req13] The PythonJsonGenerator class shall read the configuration file to set\n        the inference model parameters.\n\"\"\"\nimport re\nimport os\nimport json\nimport logging\nimport importlib\nimport yaml\nfrom typing import List, Dict\n\n# Set up logging\nlogging.basicConfig(\n    format='%(asctime)s - %(levelname)s - %(message)s', \n    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef get_model(model_config: dict, user_config: dict = None) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): A dictionary containing the configuration for the\n            model. It should include the import path for the model class and\n            parameters for instantiation.\n        user_config (dict): A dictionary containing user-provided configurations.\n            If provided, these configurations will override the defaults.\n    Returns:\n        object: An instance of the specified model class, or None if there was\n            an error.\n    \"\"\"\n    # If a user configuration is provided, override the defaults\n    if user_config is not None:\n        model_config.update(user_config)\n\n    # The rest of the function remains the same\n    model = None\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        module = importlib.import_module(module_name)\n    except ImportError as e:\n        print(f\"Failed to import module {module_name}. Error: {e}\")\n        return model\n    try:\n        ModelClass = getattr(module, class_name)\n    except AttributeError as e:\n        print(f\"Module {module_name} does not have a class named {class_name}. Error: {e}\")\n        return model\n    model_params = model_config['model_params']\n    try:\n        model = ModelClass.from_pretrained(model_params.pop('model_path'), **model_params)\n    except Exception as e:\n        print(f\"Failed to instantiate the model with the provided parameters. Error: {e}\")\n        return model\n    return model\n\nclass PythonJsonGenerator:\n    \"\"\"\n    A class used to generate JSON formatted dictionary outputs for a Python \n    file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict): A dictionary containing details of the Python\n            file.\n        base_name (str): The base name of the Python file.\n        questions (List): A list of questions for which responses are to be\n            generated.\n        qa_list (List): A list to store the generated question-answer pairs.\n        instruct_list (List): A list to store the generated instructions.\n        question_mapping (Dict): A dictionary mapping question types to their\n            corresponding keys in the file details.\n        use_llm (bool): A flag indicating whether to use a language model for\n            generating responses.\n        llm (AutoModelForCausalLM): The language model to be used for\n            generating responses.\n    Methods:\n        clean_and_get_unique_elements(input_str: str) -> str: Cleans an input \n            string and returns a string of unique elements.\n        add_to_list(list_to_update: List[Dict], query: str, response: str,\n            additional_field=None) -> List[Dict]: Adds a response to a list.\n        get_response_from_llm(query: str, context: str) -> str: Gets a \n            response from the language model.\n        get_variable_purpose(question_id: str, question_text: str, base_name:\n            str, name: str, info: Dict, context: str, variable_type: str) -> \n                None: Processes questions related to the purpose of a variable.\n        process_question(question_id: str, query: str, context: str, info) -> \n            None: Processes a question and adds the generated response to the\n            qa_list and instruct_list.\n        process_file_question(question_id: str, question_text: str) -> None:\n            Processes questions related to a file.\n        process_func_class_question(question_type: str, question_id: str, \n            question_text: str) -> None: Processes questions related to a \n            function or class.\n        generate() -> Tuple[List[Dict], List[Dict]]: Generates responses for\n            all the questions and returns the qa_list and instruct_list.\n    \"\"\"\n    def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, config: Dict):\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.qa_list = []\n        self.instruct_list = []\n        self.question_mapping = {\n            'file': 'file',\n            'function': 'functions',\n            'class': 'classes',\n            'method': 'classes'\n        }\n        self.use_llm = use_llm\n        self.config = config\n        if self.use_llm:\n            try:\n                self.llm_config = config['inference_model']\n                self.llm = get_model(self.llm_config)\n            except (FileNotFoundError, yaml.YAMLError, ImportError, AttributeError) as e:\n                logger.error(f'Failed to load configuration file: {e}')\n                self.use_llm = False\n                self.llm_config = None\n                self.llm = None\n        else:\n            self.llm = None\n\n    @staticmethod\n    def clean_and_get_unique_elements(input_str: str) -> str:\n        cleaned_elements = set(re.sub(r'[^\\w\\-_>\\s:/.]', '', element.strip())\n                               for element in re.sub(r'\\s+', ' ', input_str).split(','))\n        return ', '.join(cleaned_elements)\n\n    @staticmethod\n    def add_to_list(list_to_update: List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:\n        if response and response.strip() and response != 'None':\n            list_to_update.append(\n                {'instruction': query, 'input' : additional_field, 'output': response}\n                if additional_field else\n                {'question': query, 'answer': response}\n            )\n        return list_to_update\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        if not self.llm:\n            logger.error('AI model not available.')\n            return ''\n        prompt = self.config[\"prompt_template\"].format(context=context, query=query)\n        logging.info(f'Query: {query}')\n        response = self.llm(prompt)\n        logging.info(f'Response: {response}')\n        return response\n\n    def process_items(self, question_id: str, question_text: str, base_name: str, name: str, info: Dict, context: str, item_type: str) -> None:\n        if info[item_type]:\n            items = [item.strip() for item in self.clean_and_get_unique_elements(str(info[item_type])).split(',') if item]\n            for item in items:\n                query = question_text.format(filename=base_name, **{f'{item_type.split(\"_\")[0]}_name': name, f'{item_type.split(\"_\")[0]}_variable': item})\n                self.process_question(question_id, query, context, info)\n\n    def process_question(self, question_id: str, query: str, context: str, info) -> None:\n        if question_id.endswith('code_graph'):\n            response = info.get(question_id, {})\n        else:\n            response = self.get_response_from_llm(query, context) if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id, '')))\n        if response and response != 'None':\n            response_str = str(response)\n            response_str = response_str.strip()\n            if response_str:\n                self.qa_list.append({'question': query, 'answer': response_str})\n                self.instruct_list.append({'instruction': query, 'input': context, 'output': response_str})\n\n    def process_file_question(self, question_id: str, question_text: str) -> None:\n        query = question_text.format(filename=self.base_name)\n        context = self.file_details['file_info']['file_code']\n        info = self.file_details['file_info']\n        self.process_question(question_id, query, context, info)\n\n    def process_func_class_question(self, question_type: str, question_id: str, question_text: str) -> None:\n        if question_type == 'method':  \n            for class_name, class_info in self.file_details['classes'].items():\n                for key, method_info in class_info.items():\n                    if key.startswith('class_method_'):\n                        method_name = key[len('class_method_'):]\n                        context = method_info['method_code']\n                        mapping = {'class_name': class_name, 'method_name': method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_id, query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n                context = info[f'{question_type}_code']\n                mapping = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_variable_purpose' and self.use_llm:\n                    self.process_items(question_id, question_text, self.base_name, name, info, context, f'{question_type}_variables')\n                elif question_id != f'{question_type}_variable_purpose':\n                    query = question_text.format(filename=self.base_name, **mapping)\n                    self.process_question(question_id, query, context, info)\n\n    def generate(self) -> tuple[List[Dict], List[Dict]]:\n        for question in self.questions:\n            question_id = question['id']\n            question_text = question['text']\n            question_type = question['type']\n            if question_type == 'file':\n                self.process_file_question(question_id, question_text)\n            elif question_type in ['function', 'class', 'method']:\n                self.process_func_class_question(question_type, question_id, question_text)\n        return self.qa_list, self.instruct_list\n\ndef get_python_json(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, model_config: dict = None) -> tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base name.\n        questions (List[Dict]): The list of questions.\n        use_llm (bool): Whether to use the language model.\n        user_config (dict): User-provided model configurations.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    # Load default configuration from the YAML file\n    with open('py2dataset_model_config.yaml', 'r') as config_file:\n        config = yaml.safe_load(config_file)\n\n    # If user-provided configurations are given, update the defaults\n    if model_config is not None:\n        config.update(model_config)\n\n    generator = PythonJsonGenerator(file_path, file_details, base_name, questions, use_llm, config)\n    return generator.generate()\n",
        "output": "get_model, get_python_json"
    },
    {
        "instruction": "Classes in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "\"\"\"\nGenerates JSON format question-answer pairs and instructions for a Python file\nRequirements:\n[req01] The get_model function shall accept a dictionary containing the model\n        configuration as an argument. It should import the specified model \n        class using the information from the model configuration, instantiate\n        the model class with the provided parameters from the model \n        configuration, and return the instantiated model.\n[req02] The PythonJsonGenerator class shall parse a Python file, a list of \n        questions, and generate JSON-formatted question-answer pairs and\n        instructions.\n[req03] The PythonJsonGenerator class shall add the generated question-answer\n        pairs to the qa_list attribute.\n[req04] The PythonJsonGenerator class shall add the generated instructions to\n        the instruct_list attribute.\n[req05] The PythonJsonGenerator class shall use a language model to generate\n        responses if the use_llm attribute is set to True.\n[req06] The PythonJsonGenerator class shall handle exceptions that may occur\n        during loading the language model.\n[req07] The PythonJsonGenerator class shall generate and return qa_list and\n        instruct_list when the generate method is called.\n[req08] The PythonJsonGenerator class shall use the get_model function to load\n        the specified language model according to the configuration file.\n[req09] The PythonJsonGenerator class shall use the loaded language model to\n        generate responses to the questions.\n[req10] The PythonJsonGenerator class shall handle exceptions that may occur\n        during the generation of responses.\n[req11] The get_python_json function shall create an instance of\n        PythonJsonGenerator and call the generate method.\n[req12] The get_python_json function shall return qa_list and instruct_list\n        generated by the PythonJsonGenerator instance.\n[req13] The PythonJsonGenerator class shall read the configuration file to set\n        the inference model parameters.\n\"\"\"\nimport re\nimport os\nimport json\nimport logging\nimport importlib\nimport yaml\nfrom typing import List, Dict\n\n# Set up logging\nlogging.basicConfig(\n    format='%(asctime)s - %(levelname)s - %(message)s', \n    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef get_model(model_config: dict, user_config: dict = None) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): A dictionary containing the configuration for the\n            model. It should include the import path for the model class and\n            parameters for instantiation.\n        user_config (dict): A dictionary containing user-provided configurations.\n            If provided, these configurations will override the defaults.\n    Returns:\n        object: An instance of the specified model class, or None if there was\n            an error.\n    \"\"\"\n    # If a user configuration is provided, override the defaults\n    if user_config is not None:\n        model_config.update(user_config)\n\n    # The rest of the function remains the same\n    model = None\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        module = importlib.import_module(module_name)\n    except ImportError as e:\n        print(f\"Failed to import module {module_name}. Error: {e}\")\n        return model\n    try:\n        ModelClass = getattr(module, class_name)\n    except AttributeError as e:\n        print(f\"Module {module_name} does not have a class named {class_name}. Error: {e}\")\n        return model\n    model_params = model_config['model_params']\n    try:\n        model = ModelClass.from_pretrained(model_params.pop('model_path'), **model_params)\n    except Exception as e:\n        print(f\"Failed to instantiate the model with the provided parameters. Error: {e}\")\n        return model\n    return model\n\nclass PythonJsonGenerator:\n    \"\"\"\n    A class used to generate JSON formatted dictionary outputs for a Python \n    file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict): A dictionary containing details of the Python\n            file.\n        base_name (str): The base name of the Python file.\n        questions (List): A list of questions for which responses are to be\n            generated.\n        qa_list (List): A list to store the generated question-answer pairs.\n        instruct_list (List): A list to store the generated instructions.\n        question_mapping (Dict): A dictionary mapping question types to their\n            corresponding keys in the file details.\n        use_llm (bool): A flag indicating whether to use a language model for\n            generating responses.\n        llm (AutoModelForCausalLM): The language model to be used for\n            generating responses.\n    Methods:\n        clean_and_get_unique_elements(input_str: str) -> str: Cleans an input \n            string and returns a string of unique elements.\n        add_to_list(list_to_update: List[Dict], query: str, response: str,\n            additional_field=None) -> List[Dict]: Adds a response to a list.\n        get_response_from_llm(query: str, context: str) -> str: Gets a \n            response from the language model.\n        get_variable_purpose(question_id: str, question_text: str, base_name:\n            str, name: str, info: Dict, context: str, variable_type: str) -> \n                None: Processes questions related to the purpose of a variable.\n        process_question(question_id: str, query: str, context: str, info) -> \n            None: Processes a question and adds the generated response to the\n            qa_list and instruct_list.\n        process_file_question(question_id: str, question_text: str) -> None:\n            Processes questions related to a file.\n        process_func_class_question(question_type: str, question_id: str, \n            question_text: str) -> None: Processes questions related to a \n            function or class.\n        generate() -> Tuple[List[Dict], List[Dict]]: Generates responses for\n            all the questions and returns the qa_list and instruct_list.\n    \"\"\"\n    def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, config: Dict):\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.qa_list = []\n        self.instruct_list = []\n        self.question_mapping = {\n            'file': 'file',\n            'function': 'functions',\n            'class': 'classes',\n            'method': 'classes'\n        }\n        self.use_llm = use_llm\n        self.config = config\n        if self.use_llm:\n            try:\n                self.llm_config = config['inference_model']\n                self.llm = get_model(self.llm_config)\n            except (FileNotFoundError, yaml.YAMLError, ImportError, AttributeError) as e:\n                logger.error(f'Failed to load configuration file: {e}')\n                self.use_llm = False\n                self.llm_config = None\n                self.llm = None\n        else:\n            self.llm = None\n\n    @staticmethod\n    def clean_and_get_unique_elements(input_str: str) -> str:\n        cleaned_elements = set(re.sub(r'[^\\w\\-_>\\s:/.]', '', element.strip())\n                               for element in re.sub(r'\\s+', ' ', input_str).split(','))\n        return ', '.join(cleaned_elements)\n\n    @staticmethod\n    def add_to_list(list_to_update: List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:\n        if response and response.strip() and response != 'None':\n            list_to_update.append(\n                {'instruction': query, 'input' : additional_field, 'output': response}\n                if additional_field else\n                {'question': query, 'answer': response}\n            )\n        return list_to_update\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        if not self.llm:\n            logger.error('AI model not available.')\n            return ''\n        prompt = self.config[\"prompt_template\"].format(context=context, query=query)\n        logging.info(f'Query: {query}')\n        response = self.llm(prompt)\n        logging.info(f'Response: {response}')\n        return response\n\n    def process_items(self, question_id: str, question_text: str, base_name: str, name: str, info: Dict, context: str, item_type: str) -> None:\n        if info[item_type]:\n            items = [item.strip() for item in self.clean_and_get_unique_elements(str(info[item_type])).split(',') if item]\n            for item in items:\n                query = question_text.format(filename=base_name, **{f'{item_type.split(\"_\")[0]}_name': name, f'{item_type.split(\"_\")[0]}_variable': item})\n                self.process_question(question_id, query, context, info)\n\n    def process_question(self, question_id: str, query: str, context: str, info) -> None:\n        if question_id.endswith('code_graph'):\n            response = info.get(question_id, {})\n        else:\n            response = self.get_response_from_llm(query, context) if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id, '')))\n        if response and response != 'None':\n            response_str = str(response)\n            response_str = response_str.strip()\n            if response_str:\n                self.qa_list.append({'question': query, 'answer': response_str})\n                self.instruct_list.append({'instruction': query, 'input': context, 'output': response_str})\n\n    def process_file_question(self, question_id: str, question_text: str) -> None:\n        query = question_text.format(filename=self.base_name)\n        context = self.file_details['file_info']['file_code']\n        info = self.file_details['file_info']\n        self.process_question(question_id, query, context, info)\n\n    def process_func_class_question(self, question_type: str, question_id: str, question_text: str) -> None:\n        if question_type == 'method':  \n            for class_name, class_info in self.file_details['classes'].items():\n                for key, method_info in class_info.items():\n                    if key.startswith('class_method_'):\n                        method_name = key[len('class_method_'):]\n                        context = method_info['method_code']\n                        mapping = {'class_name': class_name, 'method_name': method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_id, query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n                context = info[f'{question_type}_code']\n                mapping = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_variable_purpose' and self.use_llm:\n                    self.process_items(question_id, question_text, self.base_name, name, info, context, f'{question_type}_variables')\n                elif question_id != f'{question_type}_variable_purpose':\n                    query = question_text.format(filename=self.base_name, **mapping)\n                    self.process_question(question_id, query, context, info)\n\n    def generate(self) -> tuple[List[Dict], List[Dict]]:\n        for question in self.questions:\n            question_id = question['id']\n            question_text = question['text']\n            question_type = question['type']\n            if question_type == 'file':\n                self.process_file_question(question_id, question_text)\n            elif question_type in ['function', 'class', 'method']:\n                self.process_func_class_question(question_type, question_id, question_text)\n        return self.qa_list, self.instruct_list\n\ndef get_python_json(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, model_config: dict = None) -> tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base name.\n        questions (List[Dict]): The list of questions.\n        use_llm (bool): Whether to use the language model.\n        user_config (dict): User-provided model configurations.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    # Load default configuration from the YAML file\n    with open('py2dataset_model_config.yaml', 'r') as config_file:\n        config = yaml.safe_load(config_file)\n\n    # If user-provided configurations are given, update the defaults\n    if model_config is not None:\n        config.update(model_config)\n\n    generator = PythonJsonGenerator(file_path, file_details, base_name, questions, use_llm, config)\n    return generator.generate()\n",
        "output": "PythonJsonGenerator"
    },
    {
        "instruction": "Control Flow in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "\"\"\"\nGenerates JSON format question-answer pairs and instructions for a Python file\nRequirements:\n[req01] The get_model function shall accept a dictionary containing the model\n        configuration as an argument. It should import the specified model \n        class using the information from the model configuration, instantiate\n        the model class with the provided parameters from the model \n        configuration, and return the instantiated model.\n[req02] The PythonJsonGenerator class shall parse a Python file, a list of \n        questions, and generate JSON-formatted question-answer pairs and\n        instructions.\n[req03] The PythonJsonGenerator class shall add the generated question-answer\n        pairs to the qa_list attribute.\n[req04] The PythonJsonGenerator class shall add the generated instructions to\n        the instruct_list attribute.\n[req05] The PythonJsonGenerator class shall use a language model to generate\n        responses if the use_llm attribute is set to True.\n[req06] The PythonJsonGenerator class shall handle exceptions that may occur\n        during loading the language model.\n[req07] The PythonJsonGenerator class shall generate and return qa_list and\n        instruct_list when the generate method is called.\n[req08] The PythonJsonGenerator class shall use the get_model function to load\n        the specified language model according to the configuration file.\n[req09] The PythonJsonGenerator class shall use the loaded language model to\n        generate responses to the questions.\n[req10] The PythonJsonGenerator class shall handle exceptions that may occur\n        during the generation of responses.\n[req11] The get_python_json function shall create an instance of\n        PythonJsonGenerator and call the generate method.\n[req12] The get_python_json function shall return qa_list and instruct_list\n        generated by the PythonJsonGenerator instance.\n[req13] The PythonJsonGenerator class shall read the configuration file to set\n        the inference model parameters.\n\"\"\"\nimport re\nimport os\nimport json\nimport logging\nimport importlib\nimport yaml\nfrom typing import List, Dict\n\n# Set up logging\nlogging.basicConfig(\n    format='%(asctime)s - %(levelname)s - %(message)s', \n    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef get_model(model_config: dict, user_config: dict = None) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): A dictionary containing the configuration for the\n            model. It should include the import path for the model class and\n            parameters for instantiation.\n        user_config (dict): A dictionary containing user-provided configurations.\n            If provided, these configurations will override the defaults.\n    Returns:\n        object: An instance of the specified model class, or None if there was\n            an error.\n    \"\"\"\n    # If a user configuration is provided, override the defaults\n    if user_config is not None:\n        model_config.update(user_config)\n\n    # The rest of the function remains the same\n    model = None\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        module = importlib.import_module(module_name)\n    except ImportError as e:\n        print(f\"Failed to import module {module_name}. Error: {e}\")\n        return model\n    try:\n        ModelClass = getattr(module, class_name)\n    except AttributeError as e:\n        print(f\"Module {module_name} does not have a class named {class_name}. Error: {e}\")\n        return model\n    model_params = model_config['model_params']\n    try:\n        model = ModelClass.from_pretrained(model_params.pop('model_path'), **model_params)\n    except Exception as e:\n        print(f\"Failed to instantiate the model with the provided parameters. Error: {e}\")\n        return model\n    return model\n\nclass PythonJsonGenerator:\n    \"\"\"\n    A class used to generate JSON formatted dictionary outputs for a Python \n    file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict): A dictionary containing details of the Python\n            file.\n        base_name (str): The base name of the Python file.\n        questions (List): A list of questions for which responses are to be\n            generated.\n        qa_list (List): A list to store the generated question-answer pairs.\n        instruct_list (List): A list to store the generated instructions.\n        question_mapping (Dict): A dictionary mapping question types to their\n            corresponding keys in the file details.\n        use_llm (bool): A flag indicating whether to use a language model for\n            generating responses.\n        llm (AutoModelForCausalLM): The language model to be used for\n            generating responses.\n    Methods:\n        clean_and_get_unique_elements(input_str: str) -> str: Cleans an input \n            string and returns a string of unique elements.\n        add_to_list(list_to_update: List[Dict], query: str, response: str,\n            additional_field=None) -> List[Dict]: Adds a response to a list.\n        get_response_from_llm(query: str, context: str) -> str: Gets a \n            response from the language model.\n        get_variable_purpose(question_id: str, question_text: str, base_name:\n            str, name: str, info: Dict, context: str, variable_type: str) -> \n                None: Processes questions related to the purpose of a variable.\n        process_question(question_id: str, query: str, context: str, info) -> \n            None: Processes a question and adds the generated response to the\n            qa_list and instruct_list.\n        process_file_question(question_id: str, question_text: str) -> None:\n            Processes questions related to a file.\n        process_func_class_question(question_type: str, question_id: str, \n            question_text: str) -> None: Processes questions related to a \n            function or class.\n        generate() -> Tuple[List[Dict], List[Dict]]: Generates responses for\n            all the questions and returns the qa_list and instruct_list.\n    \"\"\"\n    def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, config: Dict):\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.qa_list = []\n        self.instruct_list = []\n        self.question_mapping = {\n            'file': 'file',\n            'function': 'functions',\n            'class': 'classes',\n            'method': 'classes'\n        }\n        self.use_llm = use_llm\n        self.config = config\n        if self.use_llm:\n            try:\n                self.llm_config = config['inference_model']\n                self.llm = get_model(self.llm_config)\n            except (FileNotFoundError, yaml.YAMLError, ImportError, AttributeError) as e:\n                logger.error(f'Failed to load configuration file: {e}')\n                self.use_llm = False\n                self.llm_config = None\n                self.llm = None\n        else:\n            self.llm = None\n\n    @staticmethod\n    def clean_and_get_unique_elements(input_str: str) -> str:\n        cleaned_elements = set(re.sub(r'[^\\w\\-_>\\s:/.]', '', element.strip())\n                               for element in re.sub(r'\\s+', ' ', input_str).split(','))\n        return ', '.join(cleaned_elements)\n\n    @staticmethod\n    def add_to_list(list_to_update: List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:\n        if response and response.strip() and response != 'None':\n            list_to_update.append(\n                {'instruction': query, 'input' : additional_field, 'output': response}\n                if additional_field else\n                {'question': query, 'answer': response}\n            )\n        return list_to_update\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        if not self.llm:\n            logger.error('AI model not available.')\n            return ''\n        prompt = self.config[\"prompt_template\"].format(context=context, query=query)\n        logging.info(f'Query: {query}')\n        response = self.llm(prompt)\n        logging.info(f'Response: {response}')\n        return response\n\n    def process_items(self, question_id: str, question_text: str, base_name: str, name: str, info: Dict, context: str, item_type: str) -> None:\n        if info[item_type]:\n            items = [item.strip() for item in self.clean_and_get_unique_elements(str(info[item_type])).split(',') if item]\n            for item in items:\n                query = question_text.format(filename=base_name, **{f'{item_type.split(\"_\")[0]}_name': name, f'{item_type.split(\"_\")[0]}_variable': item})\n                self.process_question(question_id, query, context, info)\n\n    def process_question(self, question_id: str, query: str, context: str, info) -> None:\n        if question_id.endswith('code_graph'):\n            response = info.get(question_id, {})\n        else:\n            response = self.get_response_from_llm(query, context) if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id, '')))\n        if response and response != 'None':\n            response_str = str(response)\n            response_str = response_str.strip()\n            if response_str:\n                self.qa_list.append({'question': query, 'answer': response_str})\n                self.instruct_list.append({'instruction': query, 'input': context, 'output': response_str})\n\n    def process_file_question(self, question_id: str, question_text: str) -> None:\n        query = question_text.format(filename=self.base_name)\n        context = self.file_details['file_info']['file_code']\n        info = self.file_details['file_info']\n        self.process_question(question_id, query, context, info)\n\n    def process_func_class_question(self, question_type: str, question_id: str, question_text: str) -> None:\n        if question_type == 'method':  \n            for class_name, class_info in self.file_details['classes'].items():\n                for key, method_info in class_info.items():\n                    if key.startswith('class_method_'):\n                        method_name = key[len('class_method_'):]\n                        context = method_info['method_code']\n                        mapping = {'class_name': class_name, 'method_name': method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_id, query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n                context = info[f'{question_type}_code']\n                mapping = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_variable_purpose' and self.use_llm:\n                    self.process_items(question_id, question_text, self.base_name, name, info, context, f'{question_type}_variables')\n                elif question_id != f'{question_type}_variable_purpose':\n                    query = question_text.format(filename=self.base_name, **mapping)\n                    self.process_question(question_id, query, context, info)\n\n    def generate(self) -> tuple[List[Dict], List[Dict]]:\n        for question in self.questions:\n            question_id = question['id']\n            question_text = question['text']\n            question_type = question['type']\n            if question_type == 'file':\n                self.process_file_question(question_id, question_text)\n            elif question_type in ['function', 'class', 'method']:\n                self.process_func_class_question(question_type, question_id, question_text)\n        return self.qa_list, self.instruct_list\n\ndef get_python_json(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, model_config: dict = None) -> tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base name.\n        questions (List[Dict]): The list of questions.\n        use_llm (bool): Whether to use the language model.\n        user_config (dict): User-provided model configurations.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    # Load default configuration from the YAML file\n    with open('py2dataset_model_config.yaml', 'r') as config_file:\n        config = yaml.safe_load(config_file)\n\n    # If user-provided configurations are given, update the defaults\n    if model_config is not None:\n        config.update(model_config)\n\n    generator = PythonJsonGenerator(file_path, file_details, base_name, questions, use_llm, config)\n    return generator.generate()\n",
        "output": "module -> def get_model -> if -> try -> except -> try -> except -> try -> except -> class -> def __init__ -> if -> try -> except -> def clean_and_get_unique_elements -> def add_to_list -> if -> def get_response_from_llm -> if -> def process_items -> if -> for -> def process_question -> if -> if -> if -> def process_file_question -> def process_func_class_question -> if -> for -> for -> if -> for -> if -> if -> def generate -> for -> if -> if -> def get_python_json -> with -> if"
    },
    {
        "instruction": "Inputs to function: (get_model) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def get_model(model_config: dict, user_config: dict=None) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): A dictionary containing the configuration for the\n            model. It should include the import path for the model class and\n            parameters for instantiation.\n        user_config (dict): A dictionary containing user-provided configurations.\n            If provided, these configurations will override the defaults.\n    Returns:\n        object: An instance of the specified model class, or None if there was\n            an error.\n    \"\"\"\n    if user_config is not None:\n        model_config.update(user_config)\n    model = None\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        module = importlib.import_module(module_name)\n    except ImportError as e:\n        print(f'Failed to import module {module_name}. Error: {e}')\n        return model\n    try:\n        ModelClass = getattr(module, class_name)\n    except AttributeError as e:\n        print(f'Module {module_name} does not have a class named {class_name}. Error: {e}')\n        return model\n    model_params = model_config['model_params']\n    try:\n        model = ModelClass.from_pretrained(model_params.pop('model_path'), **model_params)\n    except Exception as e:\n        print(f'Failed to instantiate the model with the provided parameters. Error: {e}')\n        return model\n    return model",
        "output": "model_config, user_config"
    },
    {
        "instruction": "Inputs to function: (get_python_json) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def get_python_json(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, model_config: dict=None) -> tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base name.\n        questions (List[Dict]): The list of questions.\n        use_llm (bool): Whether to use the language model.\n        user_config (dict): User-provided model configurations.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    with open('py2dataset_model_config.yaml', 'r') as config_file:\n        config = yaml.safe_load(config_file)\n    if model_config is not None:\n        config.update(model_config)\n    generator = PythonJsonGenerator(file_path, file_details, base_name, questions, use_llm, config)\n    return generator.generate()",
        "output": "file_path, model_config, use_llm, file_details, base_name, questions"
    },
    {
        "instruction": "Docstring of function: (get_model) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def get_model(model_config: dict, user_config: dict=None) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): A dictionary containing the configuration for the\n            model. It should include the import path for the model class and\n            parameters for instantiation.\n        user_config (dict): A dictionary containing user-provided configurations.\n            If provided, these configurations will override the defaults.\n    Returns:\n        object: An instance of the specified model class, or None if there was\n            an error.\n    \"\"\"\n    if user_config is not None:\n        model_config.update(user_config)\n    model = None\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        module = importlib.import_module(module_name)\n    except ImportError as e:\n        print(f'Failed to import module {module_name}. Error: {e}')\n        return model\n    try:\n        ModelClass = getattr(module, class_name)\n    except AttributeError as e:\n        print(f'Module {module_name} does not have a class named {class_name}. Error: {e}')\n        return model\n    model_params = model_config['model_params']\n    try:\n        model = ModelClass.from_pretrained(model_params.pop('model_path'), **model_params)\n    except Exception as e:\n        print(f'Failed to instantiate the model with the provided parameters. Error: {e}')\n        return model\n    return model",
        "output": "these configurations will override the defaults. Returns: object: An instance of the specified model class, Imports and instantiates a model based on the provided configuration. Args: model_config dict: A dictionary containing the configuration for the model. It should include the import path for the model class and parameters for instantiation. user_config dict: A dictionary containing user-provided configurations. If provided, or None if there was an error."
    },
    {
        "instruction": "Docstring of function: (get_python_json) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def get_python_json(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, model_config: dict=None) -> tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base name.\n        questions (List[Dict]): The list of questions.\n        use_llm (bool): Whether to use the language model.\n        user_config (dict): User-provided model configurations.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    with open('py2dataset_model_config.yaml', 'r') as config_file:\n        config = yaml.safe_load(config_file)\n    if model_config is not None:\n        config.update(model_config)\n    generator = PythonJsonGenerator(file_path, file_details, base_name, questions, use_llm, config)\n    return generator.generate()",
        "output": "Extract information from a Python file and return it in JSON format. Args: file_path str: The path to the Python file. file_details Dict: The details of the file. base_name str: The base name. questions ListDict: The list of questions. use_llm bool: Whether to use the language model. user_config dict: User-provided model configurations. Returns: TupleListDict, ListDict: Extracted information in JSON format."
    },
    {
        "instruction": "Calls in function: (get_model) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def get_model(model_config: dict, user_config: dict=None) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): A dictionary containing the configuration for the\n            model. It should include the import path for the model class and\n            parameters for instantiation.\n        user_config (dict): A dictionary containing user-provided configurations.\n            If provided, these configurations will override the defaults.\n    Returns:\n        object: An instance of the specified model class, or None if there was\n            an error.\n    \"\"\"\n    if user_config is not None:\n        model_config.update(user_config)\n    model = None\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        module = importlib.import_module(module_name)\n    except ImportError as e:\n        print(f'Failed to import module {module_name}. Error: {e}')\n        return model\n    try:\n        ModelClass = getattr(module, class_name)\n    except AttributeError as e:\n        print(f'Module {module_name} does not have a class named {class_name}. Error: {e}')\n        return model\n    model_params = model_config['model_params']\n    try:\n        model = ModelClass.from_pretrained(model_params.pop('model_path'), **model_params)\n    except Exception as e:\n        print(f'Failed to instantiate the model with the provided parameters. Error: {e}')\n        return model\n    return model",
        "output": "model_config.update, model_configmodel_import_path.rsplit, ModelClass.from_pretrained, model_params.pop, getattr, print, importlib.import_module"
    },
    {
        "instruction": "Calls in function: (get_python_json) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def get_python_json(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, model_config: dict=None) -> tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base name.\n        questions (List[Dict]): The list of questions.\n        use_llm (bool): Whether to use the language model.\n        user_config (dict): User-provided model configurations.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    with open('py2dataset_model_config.yaml', 'r') as config_file:\n        config = yaml.safe_load(config_file)\n    if model_config is not None:\n        config.update(model_config)\n    generator = PythonJsonGenerator(file_path, file_details, base_name, questions, use_llm, config)\n    return generator.generate()",
        "output": "config.update, open, yaml.safe_load, PythonJsonGenerator, generator.generate"
    },
    {
        "instruction": "Variables in function: (get_model) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def get_model(model_config: dict, user_config: dict=None) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): A dictionary containing the configuration for the\n            model. It should include the import path for the model class and\n            parameters for instantiation.\n        user_config (dict): A dictionary containing user-provided configurations.\n            If provided, these configurations will override the defaults.\n    Returns:\n        object: An instance of the specified model class, or None if there was\n            an error.\n    \"\"\"\n    if user_config is not None:\n        model_config.update(user_config)\n    model = None\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        module = importlib.import_module(module_name)\n    except ImportError as e:\n        print(f'Failed to import module {module_name}. Error: {e}')\n        return model\n    try:\n        ModelClass = getattr(module, class_name)\n    except AttributeError as e:\n        print(f'Module {module_name} does not have a class named {class_name}. Error: {e}')\n        return model\n    model_params = model_config['model_params']\n    try:\n        model = ModelClass.from_pretrained(model_params.pop('model_path'), **model_params)\n    except Exception as e:\n        print(f'Failed to instantiate the model with the provided parameters. Error: {e}')\n        return model\n    return model",
        "output": "model_params, module, ModelClass, model"
    },
    {
        "instruction": "Variables in function: (get_python_json) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def get_python_json(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, model_config: dict=None) -> tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base name.\n        questions (List[Dict]): The list of questions.\n        use_llm (bool): Whether to use the language model.\n        user_config (dict): User-provided model configurations.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    with open('py2dataset_model_config.yaml', 'r') as config_file:\n        config = yaml.safe_load(config_file)\n    if model_config is not None:\n        config.update(model_config)\n    generator = PythonJsonGenerator(file_path, file_details, base_name, questions, use_llm, config)\n    return generator.generate()",
        "output": "generator, config"
    },
    {
        "instruction": "Returns from function: (get_model) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def get_model(model_config: dict, user_config: dict=None) -> object:\n    \"\"\"\n    Imports and instantiates a model based on the provided configuration.\n    Args:\n        model_config (dict): A dictionary containing the configuration for the\n            model. It should include the import path for the model class and\n            parameters for instantiation.\n        user_config (dict): A dictionary containing user-provided configurations.\n            If provided, these configurations will override the defaults.\n    Returns:\n        object: An instance of the specified model class, or None if there was\n            an error.\n    \"\"\"\n    if user_config is not None:\n        model_config.update(user_config)\n    model = None\n    try:\n        module_name, class_name = model_config['model_import_path'].rsplit('.', 1)\n        module = importlib.import_module(module_name)\n    except ImportError as e:\n        print(f'Failed to import module {module_name}. Error: {e}')\n        return model\n    try:\n        ModelClass = getattr(module, class_name)\n    except AttributeError as e:\n        print(f'Module {module_name} does not have a class named {class_name}. Error: {e}')\n        return model\n    model_params = model_config['model_params']\n    try:\n        model = ModelClass.from_pretrained(model_params.pop('model_path'), **model_params)\n    except Exception as e:\n        print(f'Failed to instantiate the model with the provided parameters. Error: {e}')\n        return model\n    return model",
        "output": "model"
    },
    {
        "instruction": "Returns from function: (get_python_json) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def get_python_json(file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, model_config: dict=None) -> tuple[List[Dict], List[Dict]]:\n    \"\"\"\n    Extract information from a Python file and return it in JSON format.\n    Args:\n        file_path (str): The path to the Python file.\n        file_details (Dict): The details of the file.\n        base_name (str): The base name.\n        questions (List[Dict]): The list of questions.\n        use_llm (bool): Whether to use the language model.\n        user_config (dict): User-provided model configurations.\n    Returns:\n        Tuple[List[Dict], List[Dict]]: Extracted information in JSON format.\n    \"\"\"\n    with open('py2dataset_model_config.yaml', 'r') as config_file:\n        config = yaml.safe_load(config_file)\n    if model_config is not None:\n        config.update(model_config)\n    generator = PythonJsonGenerator(file_path, file_details, base_name, questions, use_llm, config)\n    return generator.generate()",
        "output": "generator.generate"
    },
    {
        "instruction": "Methods in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "class PythonJsonGenerator:\n    \"\"\"\n    A class used to generate JSON formatted dictionary outputs for a Python \n    file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict): A dictionary containing details of the Python\n            file.\n        base_name (str): The base name of the Python file.\n        questions (List): A list of questions for which responses are to be\n            generated.\n        qa_list (List): A list to store the generated question-answer pairs.\n        instruct_list (List): A list to store the generated instructions.\n        question_mapping (Dict): A dictionary mapping question types to their\n            corresponding keys in the file details.\n        use_llm (bool): A flag indicating whether to use a language model for\n            generating responses.\n        llm (AutoModelForCausalLM): The language model to be used for\n            generating responses.\n    Methods:\n        clean_and_get_unique_elements(input_str: str) -> str: Cleans an input \n            string and returns a string of unique elements.\n        add_to_list(list_to_update: List[Dict], query: str, response: str,\n            additional_field=None) -> List[Dict]: Adds a response to a list.\n        get_response_from_llm(query: str, context: str) -> str: Gets a \n            response from the language model.\n        get_variable_purpose(question_id: str, question_text: str, base_name:\n            str, name: str, info: Dict, context: str, variable_type: str) -> \n                None: Processes questions related to the purpose of a variable.\n        process_question(question_id: str, query: str, context: str, info) -> \n            None: Processes a question and adds the generated response to the\n            qa_list and instruct_list.\n        process_file_question(question_id: str, question_text: str) -> None:\n            Processes questions related to a file.\n        process_func_class_question(question_type: str, question_id: str, \n            question_text: str) -> None: Processes questions related to a \n            function or class.\n        generate() -> Tuple[List[Dict], List[Dict]]: Generates responses for\n            all the questions and returns the qa_list and instruct_list.\n    \"\"\"\n\n    def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, config: Dict):\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.qa_list = []\n        self.instruct_list = []\n        self.question_mapping = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n        self.use_llm = use_llm\n        self.config = config\n        if self.use_llm:\n            try:\n                self.llm_config = config['inference_model']\n                self.llm = get_model(self.llm_config)\n            except (FileNotFoundError, yaml.YAMLError, ImportError, AttributeError) as e:\n                logger.error(f'Failed to load configuration file: {e}')\n                self.use_llm = False\n                self.llm_config = None\n                self.llm = None\n        else:\n            self.llm = None\n\n    @staticmethod\n    def clean_and_get_unique_elements(input_str: str) -> str:\n        cleaned_elements = set((re.sub('[^\\\\w\\\\-_>\\\\s:/.]', '', element.strip()) for element in re.sub('\\\\s+', ' ', input_str).split(',')))\n        return ', '.join(cleaned_elements)\n\n    @staticmethod\n    def add_to_list(list_to_update: List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:\n        if response and response.strip() and (response != 'None'):\n            list_to_update.append({'instruction': query, 'input': additional_field, 'output': response} if additional_field else {'question': query, 'answer': response})\n        return list_to_update\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        if not self.llm:\n            logger.error('AI model not available.')\n            return ''\n        prompt = self.config['prompt_template'].format(context=context, query=query)\n        logging.info(f'Query: {query}')\n        response = self.llm(prompt)\n        logging.info(f'Response: {response}')\n        return response\n\n    def process_items(self, question_id: str, question_text: str, base_name: str, name: str, info: Dict, context: str, item_type: str) -> None:\n        if info[item_type]:\n            items = [item.strip() for item in self.clean_and_get_unique_elements(str(info[item_type])).split(',') if item]\n            for item in items:\n                query = question_text.format(filename=base_name, **{f\"{item_type.split('_')[0]}_name\": name, f\"{item_type.split('_')[0]}_variable\": item})\n                self.process_question(question_id, query, context, info)\n\n    def process_question(self, question_id: str, query: str, context: str, info) -> None:\n        if question_id.endswith('code_graph'):\n            response = info.get(question_id, {})\n        else:\n            response = self.get_response_from_llm(query, context) if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id, '')))\n        if response and response != 'None':\n            response_str = str(response)\n            response_str = response_str.strip()\n            if response_str:\n                self.qa_list.append({'question': query, 'answer': response_str})\n                self.instruct_list.append({'instruction': query, 'input': context, 'output': response_str})\n\n    def process_file_question(self, question_id: str, question_text: str) -> None:\n        query = question_text.format(filename=self.base_name)\n        context = self.file_details['file_info']['file_code']\n        info = self.file_details['file_info']\n        self.process_question(question_id, query, context, info)\n\n    def process_func_class_question(self, question_type: str, question_id: str, question_text: str) -> None:\n        if question_type == 'method':\n            for class_name, class_info in self.file_details['classes'].items():\n                for key, method_info in class_info.items():\n                    if key.startswith('class_method_'):\n                        method_name = key[len('class_method_'):]\n                        context = method_info['method_code']\n                        mapping = {'class_name': class_name, 'method_name': method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_id, query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n                context = info[f'{question_type}_code']\n                mapping = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_variable_purpose' and self.use_llm:\n                    self.process_items(question_id, question_text, self.base_name, name, info, context, f'{question_type}_variables')\n                elif question_id != f'{question_type}_variable_purpose':\n                    query = question_text.format(filename=self.base_name, **mapping)\n                    self.process_question(question_id, query, context, info)\n\n    def generate(self) -> tuple[List[Dict], List[Dict]]:\n        for question in self.questions:\n            question_id = question['id']\n            question_text = question['text']\n            question_type = question['type']\n            if question_type == 'file':\n                self.process_file_question(question_id, question_text)\n            elif question_type in ['function', 'class', 'method']:\n                self.process_func_class_question(question_type, question_id, question_text)\n        return (self.qa_list, self.instruct_list)",
        "output": "clean_and_get_unique_elements, process_func_class_question, add_to_list, get_response_from_llm, process_file_question, process_question, process_items, generate"
    },
    {
        "instruction": "Docstring of class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "class PythonJsonGenerator:\n    \"\"\"\n    A class used to generate JSON formatted dictionary outputs for a Python \n    file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict): A dictionary containing details of the Python\n            file.\n        base_name (str): The base name of the Python file.\n        questions (List): A list of questions for which responses are to be\n            generated.\n        qa_list (List): A list to store the generated question-answer pairs.\n        instruct_list (List): A list to store the generated instructions.\n        question_mapping (Dict): A dictionary mapping question types to their\n            corresponding keys in the file details.\n        use_llm (bool): A flag indicating whether to use a language model for\n            generating responses.\n        llm (AutoModelForCausalLM): The language model to be used for\n            generating responses.\n    Methods:\n        clean_and_get_unique_elements(input_str: str) -> str: Cleans an input \n            string and returns a string of unique elements.\n        add_to_list(list_to_update: List[Dict], query: str, response: str,\n            additional_field=None) -> List[Dict]: Adds a response to a list.\n        get_response_from_llm(query: str, context: str) -> str: Gets a \n            response from the language model.\n        get_variable_purpose(question_id: str, question_text: str, base_name:\n            str, name: str, info: Dict, context: str, variable_type: str) -> \n                None: Processes questions related to the purpose of a variable.\n        process_question(question_id: str, query: str, context: str, info) -> \n            None: Processes a question and adds the generated response to the\n            qa_list and instruct_list.\n        process_file_question(question_id: str, question_text: str) -> None:\n            Processes questions related to a file.\n        process_func_class_question(question_type: str, question_id: str, \n            question_text: str) -> None: Processes questions related to a \n            function or class.\n        generate() -> Tuple[List[Dict], List[Dict]]: Generates responses for\n            all the questions and returns the qa_list and instruct_list.\n    \"\"\"\n\n    def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, config: Dict):\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.qa_list = []\n        self.instruct_list = []\n        self.question_mapping = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n        self.use_llm = use_llm\n        self.config = config\n        if self.use_llm:\n            try:\n                self.llm_config = config['inference_model']\n                self.llm = get_model(self.llm_config)\n            except (FileNotFoundError, yaml.YAMLError, ImportError, AttributeError) as e:\n                logger.error(f'Failed to load configuration file: {e}')\n                self.use_llm = False\n                self.llm_config = None\n                self.llm = None\n        else:\n            self.llm = None\n\n    @staticmethod\n    def clean_and_get_unique_elements(input_str: str) -> str:\n        cleaned_elements = set((re.sub('[^\\\\w\\\\-_>\\\\s:/.]', '', element.strip()) for element in re.sub('\\\\s+', ' ', input_str).split(',')))\n        return ', '.join(cleaned_elements)\n\n    @staticmethod\n    def add_to_list(list_to_update: List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:\n        if response and response.strip() and (response != 'None'):\n            list_to_update.append({'instruction': query, 'input': additional_field, 'output': response} if additional_field else {'question': query, 'answer': response})\n        return list_to_update\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        if not self.llm:\n            logger.error('AI model not available.')\n            return ''\n        prompt = self.config['prompt_template'].format(context=context, query=query)\n        logging.info(f'Query: {query}')\n        response = self.llm(prompt)\n        logging.info(f'Response: {response}')\n        return response\n\n    def process_items(self, question_id: str, question_text: str, base_name: str, name: str, info: Dict, context: str, item_type: str) -> None:\n        if info[item_type]:\n            items = [item.strip() for item in self.clean_and_get_unique_elements(str(info[item_type])).split(',') if item]\n            for item in items:\n                query = question_text.format(filename=base_name, **{f\"{item_type.split('_')[0]}_name\": name, f\"{item_type.split('_')[0]}_variable\": item})\n                self.process_question(question_id, query, context, info)\n\n    def process_question(self, question_id: str, query: str, context: str, info) -> None:\n        if question_id.endswith('code_graph'):\n            response = info.get(question_id, {})\n        else:\n            response = self.get_response_from_llm(query, context) if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id, '')))\n        if response and response != 'None':\n            response_str = str(response)\n            response_str = response_str.strip()\n            if response_str:\n                self.qa_list.append({'question': query, 'answer': response_str})\n                self.instruct_list.append({'instruction': query, 'input': context, 'output': response_str})\n\n    def process_file_question(self, question_id: str, question_text: str) -> None:\n        query = question_text.format(filename=self.base_name)\n        context = self.file_details['file_info']['file_code']\n        info = self.file_details['file_info']\n        self.process_question(question_id, query, context, info)\n\n    def process_func_class_question(self, question_type: str, question_id: str, question_text: str) -> None:\n        if question_type == 'method':\n            for class_name, class_info in self.file_details['classes'].items():\n                for key, method_info in class_info.items():\n                    if key.startswith('class_method_'):\n                        method_name = key[len('class_method_'):]\n                        context = method_info['method_code']\n                        mapping = {'class_name': class_name, 'method_name': method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_id, query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n                context = info[f'{question_type}_code']\n                mapping = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_variable_purpose' and self.use_llm:\n                    self.process_items(question_id, question_text, self.base_name, name, info, context, f'{question_type}_variables')\n                elif question_id != f'{question_type}_variable_purpose':\n                    query = question_text.format(filename=self.base_name, **mapping)\n                    self.process_question(question_id, query, context, info)\n\n    def generate(self) -> tuple[List[Dict], List[Dict]]:\n        for question in self.questions:\n            question_id = question['id']\n            question_text = question['text']\n            question_type = question['type']\n            if question_type == 'file':\n                self.process_file_question(question_id, question_text)\n            elif question_type in ['function', 'class', 'method']:\n                self.process_func_class_question(question_type, question_id, question_text)\n        return (self.qa_list, self.instruct_list)",
        "output": "question_text: str, info -> None: Processes a question and adds the generated response to the qa_list and instruct_list. process_file_questionquestion_id: str, query: str, variable_type: str -> None: Processes questions related to the purpose of a variable. process_questionquestion_id: str, ListDict: Generates responses for all the questions and returns the qa_list and instruct_list., question_text: str -> None: Processes questions related to a file. process_func_class_questionquestion_type: str, additional_fieldNone -> ListDict: Adds a response to a list. get_response_from_llmquery: str, context: str -> str: Gets a response from the language model. get_variable_purposequestion_id: str, A class used to generate JSON formatted dictionary outputs for a Python file. Attributes: file_path str: The path to the Python file. file_details Dict: A dictionary containing details of the Python file. base_name str: The base name of the Python file. questions List: A list of questions for which responses are to be generated. qa_list List: A list to store the generated question-answer pairs. instruct_list List: A list to store the generated instructions. question_mapping Dict: A dictionary mapping question types to their corresponding keys in the file details. use_llm bool: A flag indicating whether to use a language model for generating responses. llm AutoModelForCausalLM: The language model to be used for generating responses. Methods: clean_and_get_unique_elementsinput_str: str -> str: Cleans an input string and returns a string of unique elements. add_to_listlist_to_update: ListDict, name: str, context: str, info: Dict, question_text: str -> None: Processes questions related to a function or class. generate -> TupleListDict, response: str, base_name: str, question_id: str"
    },
    {
        "instruction": "Attributes of class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "class PythonJsonGenerator:\n    \"\"\"\n    A class used to generate JSON formatted dictionary outputs for a Python \n    file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict): A dictionary containing details of the Python\n            file.\n        base_name (str): The base name of the Python file.\n        questions (List): A list of questions for which responses are to be\n            generated.\n        qa_list (List): A list to store the generated question-answer pairs.\n        instruct_list (List): A list to store the generated instructions.\n        question_mapping (Dict): A dictionary mapping question types to their\n            corresponding keys in the file details.\n        use_llm (bool): A flag indicating whether to use a language model for\n            generating responses.\n        llm (AutoModelForCausalLM): The language model to be used for\n            generating responses.\n    Methods:\n        clean_and_get_unique_elements(input_str: str) -> str: Cleans an input \n            string and returns a string of unique elements.\n        add_to_list(list_to_update: List[Dict], query: str, response: str,\n            additional_field=None) -> List[Dict]: Adds a response to a list.\n        get_response_from_llm(query: str, context: str) -> str: Gets a \n            response from the language model.\n        get_variable_purpose(question_id: str, question_text: str, base_name:\n            str, name: str, info: Dict, context: str, variable_type: str) -> \n                None: Processes questions related to the purpose of a variable.\n        process_question(question_id: str, query: str, context: str, info) -> \n            None: Processes a question and adds the generated response to the\n            qa_list and instruct_list.\n        process_file_question(question_id: str, question_text: str) -> None:\n            Processes questions related to a file.\n        process_func_class_question(question_type: str, question_id: str, \n            question_text: str) -> None: Processes questions related to a \n            function or class.\n        generate() -> Tuple[List[Dict], List[Dict]]: Generates responses for\n            all the questions and returns the qa_list and instruct_list.\n    \"\"\"\n\n    def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, config: Dict):\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.qa_list = []\n        self.instruct_list = []\n        self.question_mapping = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n        self.use_llm = use_llm\n        self.config = config\n        if self.use_llm:\n            try:\n                self.llm_config = config['inference_model']\n                self.llm = get_model(self.llm_config)\n            except (FileNotFoundError, yaml.YAMLError, ImportError, AttributeError) as e:\n                logger.error(f'Failed to load configuration file: {e}')\n                self.use_llm = False\n                self.llm_config = None\n                self.llm = None\n        else:\n            self.llm = None\n\n    @staticmethod\n    def clean_and_get_unique_elements(input_str: str) -> str:\n        cleaned_elements = set((re.sub('[^\\\\w\\\\-_>\\\\s:/.]', '', element.strip()) for element in re.sub('\\\\s+', ' ', input_str).split(',')))\n        return ', '.join(cleaned_elements)\n\n    @staticmethod\n    def add_to_list(list_to_update: List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:\n        if response and response.strip() and (response != 'None'):\n            list_to_update.append({'instruction': query, 'input': additional_field, 'output': response} if additional_field else {'question': query, 'answer': response})\n        return list_to_update\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        if not self.llm:\n            logger.error('AI model not available.')\n            return ''\n        prompt = self.config['prompt_template'].format(context=context, query=query)\n        logging.info(f'Query: {query}')\n        response = self.llm(prompt)\n        logging.info(f'Response: {response}')\n        return response\n\n    def process_items(self, question_id: str, question_text: str, base_name: str, name: str, info: Dict, context: str, item_type: str) -> None:\n        if info[item_type]:\n            items = [item.strip() for item in self.clean_and_get_unique_elements(str(info[item_type])).split(',') if item]\n            for item in items:\n                query = question_text.format(filename=base_name, **{f\"{item_type.split('_')[0]}_name\": name, f\"{item_type.split('_')[0]}_variable\": item})\n                self.process_question(question_id, query, context, info)\n\n    def process_question(self, question_id: str, query: str, context: str, info) -> None:\n        if question_id.endswith('code_graph'):\n            response = info.get(question_id, {})\n        else:\n            response = self.get_response_from_llm(query, context) if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id, '')))\n        if response and response != 'None':\n            response_str = str(response)\n            response_str = response_str.strip()\n            if response_str:\n                self.qa_list.append({'question': query, 'answer': response_str})\n                self.instruct_list.append({'instruction': query, 'input': context, 'output': response_str})\n\n    def process_file_question(self, question_id: str, question_text: str) -> None:\n        query = question_text.format(filename=self.base_name)\n        context = self.file_details['file_info']['file_code']\n        info = self.file_details['file_info']\n        self.process_question(question_id, query, context, info)\n\n    def process_func_class_question(self, question_type: str, question_id: str, question_text: str) -> None:\n        if question_type == 'method':\n            for class_name, class_info in self.file_details['classes'].items():\n                for key, method_info in class_info.items():\n                    if key.startswith('class_method_'):\n                        method_name = key[len('class_method_'):]\n                        context = method_info['method_code']\n                        mapping = {'class_name': class_name, 'method_name': method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_id, query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n                context = info[f'{question_type}_code']\n                mapping = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_variable_purpose' and self.use_llm:\n                    self.process_items(question_id, question_text, self.base_name, name, info, context, f'{question_type}_variables')\n                elif question_id != f'{question_type}_variable_purpose':\n                    query = question_text.format(filename=self.base_name, **mapping)\n                    self.process_question(question_id, query, context, info)\n\n    def generate(self) -> tuple[List[Dict], List[Dict]]:\n        for question in self.questions:\n            question_id = question['id']\n            question_text = question['text']\n            question_type = question['type']\n            if question_type == 'file':\n                self.process_file_question(question_id, question_text)\n            elif question_type in ['function', 'class', 'method']:\n                self.process_func_class_question(question_type, question_id, question_text)\n        return (self.qa_list, self.instruct_list)",
        "output": "file_path, llm_config, config, instruct_list, question_mapping, llm, use_llm, file_details, qa_list, base_name, questions"
    },
    {
        "instruction": "Variables in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "class PythonJsonGenerator:\n    \"\"\"\n    A class used to generate JSON formatted dictionary outputs for a Python \n    file.\n    Attributes:\n        file_path (str): The path to the Python file.\n        file_details (Dict): A dictionary containing details of the Python\n            file.\n        base_name (str): The base name of the Python file.\n        questions (List): A list of questions for which responses are to be\n            generated.\n        qa_list (List): A list to store the generated question-answer pairs.\n        instruct_list (List): A list to store the generated instructions.\n        question_mapping (Dict): A dictionary mapping question types to their\n            corresponding keys in the file details.\n        use_llm (bool): A flag indicating whether to use a language model for\n            generating responses.\n        llm (AutoModelForCausalLM): The language model to be used for\n            generating responses.\n    Methods:\n        clean_and_get_unique_elements(input_str: str) -> str: Cleans an input \n            string and returns a string of unique elements.\n        add_to_list(list_to_update: List[Dict], query: str, response: str,\n            additional_field=None) -> List[Dict]: Adds a response to a list.\n        get_response_from_llm(query: str, context: str) -> str: Gets a \n            response from the language model.\n        get_variable_purpose(question_id: str, question_text: str, base_name:\n            str, name: str, info: Dict, context: str, variable_type: str) -> \n                None: Processes questions related to the purpose of a variable.\n        process_question(question_id: str, query: str, context: str, info) -> \n            None: Processes a question and adds the generated response to the\n            qa_list and instruct_list.\n        process_file_question(question_id: str, question_text: str) -> None:\n            Processes questions related to a file.\n        process_func_class_question(question_type: str, question_id: str, \n            question_text: str) -> None: Processes questions related to a \n            function or class.\n        generate() -> Tuple[List[Dict], List[Dict]]: Generates responses for\n            all the questions and returns the qa_list and instruct_list.\n    \"\"\"\n\n    def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, config: Dict):\n        self.file_path = file_path\n        self.file_details = file_details\n        self.base_name = base_name\n        self.questions = questions\n        self.qa_list = []\n        self.instruct_list = []\n        self.question_mapping = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n        self.use_llm = use_llm\n        self.config = config\n        if self.use_llm:\n            try:\n                self.llm_config = config['inference_model']\n                self.llm = get_model(self.llm_config)\n            except (FileNotFoundError, yaml.YAMLError, ImportError, AttributeError) as e:\n                logger.error(f'Failed to load configuration file: {e}')\n                self.use_llm = False\n                self.llm_config = None\n                self.llm = None\n        else:\n            self.llm = None\n\n    @staticmethod\n    def clean_and_get_unique_elements(input_str: str) -> str:\n        cleaned_elements = set((re.sub('[^\\\\w\\\\-_>\\\\s:/.]', '', element.strip()) for element in re.sub('\\\\s+', ' ', input_str).split(',')))\n        return ', '.join(cleaned_elements)\n\n    @staticmethod\n    def add_to_list(list_to_update: List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:\n        if response and response.strip() and (response != 'None'):\n            list_to_update.append({'instruction': query, 'input': additional_field, 'output': response} if additional_field else {'question': query, 'answer': response})\n        return list_to_update\n\n    def get_response_from_llm(self, query: str, context: str) -> str:\n        if not self.llm:\n            logger.error('AI model not available.')\n            return ''\n        prompt = self.config['prompt_template'].format(context=context, query=query)\n        logging.info(f'Query: {query}')\n        response = self.llm(prompt)\n        logging.info(f'Response: {response}')\n        return response\n\n    def process_items(self, question_id: str, question_text: str, base_name: str, name: str, info: Dict, context: str, item_type: str) -> None:\n        if info[item_type]:\n            items = [item.strip() for item in self.clean_and_get_unique_elements(str(info[item_type])).split(',') if item]\n            for item in items:\n                query = question_text.format(filename=base_name, **{f\"{item_type.split('_')[0]}_name\": name, f\"{item_type.split('_')[0]}_variable\": item})\n                self.process_question(question_id, query, context, info)\n\n    def process_question(self, question_id: str, query: str, context: str, info) -> None:\n        if question_id.endswith('code_graph'):\n            response = info.get(question_id, {})\n        else:\n            response = self.get_response_from_llm(query, context) if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id, '')))\n        if response and response != 'None':\n            response_str = str(response)\n            response_str = response_str.strip()\n            if response_str:\n                self.qa_list.append({'question': query, 'answer': response_str})\n                self.instruct_list.append({'instruction': query, 'input': context, 'output': response_str})\n\n    def process_file_question(self, question_id: str, question_text: str) -> None:\n        query = question_text.format(filename=self.base_name)\n        context = self.file_details['file_info']['file_code']\n        info = self.file_details['file_info']\n        self.process_question(question_id, query, context, info)\n\n    def process_func_class_question(self, question_type: str, question_id: str, question_text: str) -> None:\n        if question_type == 'method':\n            for class_name, class_info in self.file_details['classes'].items():\n                for key, method_info in class_info.items():\n                    if key.startswith('class_method_'):\n                        method_name = key[len('class_method_'):]\n                        context = method_info['method_code']\n                        mapping = {'class_name': class_name, 'method_name': method_name}\n                        query = question_text.format(filename=self.base_name, **mapping)\n                        self.process_question(question_id, query, context, method_info)\n        else:\n            for name, info in self.file_details[self.question_mapping[question_type]].items():\n                context = info[f'{question_type}_code']\n                mapping = {f'{question_type}_name': name}\n                if question_id == f'{question_type}_variable_purpose' and self.use_llm:\n                    self.process_items(question_id, question_text, self.base_name, name, info, context, f'{question_type}_variables')\n                elif question_id != f'{question_type}_variable_purpose':\n                    query = question_text.format(filename=self.base_name, **mapping)\n                    self.process_question(question_id, query, context, info)\n\n    def generate(self) -> tuple[List[Dict], List[Dict]]:\n        for question in self.questions:\n            question_id = question['id']\n            question_text = question['text']\n            question_type = question['type']\n            if question_type == 'file':\n                self.process_file_question(question_id, question_text)\n            elif question_type in ['function', 'class', 'method']:\n                self.process_func_class_question(question_type, question_id, question_text)\n        return (self.qa_list, self.instruct_list)",
        "output": "info, question_id, question_type, method_name, question_text, query, response, prompt, cleaned_elements, items, mapping, context, response_str"
    },
    {
        "instruction": "Inputs to method: (__init__) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, config: Dict):\n    self.file_path = file_path\n    self.file_details = file_details\n    self.base_name = base_name\n    self.questions = questions\n    self.qa_list = []\n    self.instruct_list = []\n    self.question_mapping = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n    self.use_llm = use_llm\n    self.config = config\n    if self.use_llm:\n        try:\n            self.llm_config = config['inference_model']\n            self.llm = get_model(self.llm_config)\n        except (FileNotFoundError, yaml.YAMLError, ImportError, AttributeError) as e:\n            logger.error(f'Failed to load configuration file: {e}')\n            self.use_llm = False\n            self.llm_config = None\n            self.llm = None\n    else:\n        self.llm = None",
        "output": "file_path, config, use_llm, file_details, self, base_name, questions"
    },
    {
        "instruction": "Inputs to method: (clean_and_get_unique_elements) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "@staticmethod\ndef clean_and_get_unique_elements(input_str: str) -> str:\n    cleaned_elements = set((re.sub('[^\\\\w\\\\-_>\\\\s:/.]', '', element.strip()) for element in re.sub('\\\\s+', ' ', input_str).split(',')))\n    return ', '.join(cleaned_elements)",
        "output": "input_str"
    },
    {
        "instruction": "Inputs to method: (add_to_list) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "@staticmethod\ndef add_to_list(list_to_update: List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:\n    if response and response.strip() and (response != 'None'):\n        list_to_update.append({'instruction': query, 'input': additional_field, 'output': response} if additional_field else {'question': query, 'answer': response})\n    return list_to_update",
        "output": "response, additional_field, query, list_to_update"
    },
    {
        "instruction": "Inputs to method: (get_response_from_llm) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def get_response_from_llm(self, query: str, context: str) -> str:\n    if not self.llm:\n        logger.error('AI model not available.')\n        return ''\n    prompt = self.config['prompt_template'].format(context=context, query=query)\n    logging.info(f'Query: {query}')\n    response = self.llm(prompt)\n    logging.info(f'Response: {response}')\n    return response",
        "output": "context, self, query"
    },
    {
        "instruction": "Inputs to method: (process_items) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def process_items(self, question_id: str, question_text: str, base_name: str, name: str, info: Dict, context: str, item_type: str) -> None:\n    if info[item_type]:\n        items = [item.strip() for item in self.clean_and_get_unique_elements(str(info[item_type])).split(',') if item]\n        for item in items:\n            query = question_text.format(filename=base_name, **{f\"{item_type.split('_')[0]}_name\": name, f\"{item_type.split('_')[0]}_variable\": item})\n            self.process_question(question_id, query, context, info)",
        "output": "info, question_id, question_text, name, item_type, context, self, base_name"
    },
    {
        "instruction": "Inputs to method: (process_question) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def process_question(self, question_id: str, query: str, context: str, info) -> None:\n    if question_id.endswith('code_graph'):\n        response = info.get(question_id, {})\n    else:\n        response = self.get_response_from_llm(query, context) if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id, '')))\n    if response and response != 'None':\n        response_str = str(response)\n        response_str = response_str.strip()\n        if response_str:\n            self.qa_list.append({'question': query, 'answer': response_str})\n            self.instruct_list.append({'instruction': query, 'input': context, 'output': response_str})",
        "output": "info, question_id, query, context, self"
    },
    {
        "instruction": "Inputs to method: (process_file_question) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def process_file_question(self, question_id: str, question_text: str) -> None:\n    query = question_text.format(filename=self.base_name)\n    context = self.file_details['file_info']['file_code']\n    info = self.file_details['file_info']\n    self.process_question(question_id, query, context, info)",
        "output": "question_id, self, question_text"
    },
    {
        "instruction": "Inputs to method: (process_func_class_question) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def process_func_class_question(self, question_type: str, question_id: str, question_text: str) -> None:\n    if question_type == 'method':\n        for class_name, class_info in self.file_details['classes'].items():\n            for key, method_info in class_info.items():\n                if key.startswith('class_method_'):\n                    method_name = key[len('class_method_'):]\n                    context = method_info['method_code']\n                    mapping = {'class_name': class_name, 'method_name': method_name}\n                    query = question_text.format(filename=self.base_name, **mapping)\n                    self.process_question(question_id, query, context, method_info)\n    else:\n        for name, info in self.file_details[self.question_mapping[question_type]].items():\n            context = info[f'{question_type}_code']\n            mapping = {f'{question_type}_name': name}\n            if question_id == f'{question_type}_variable_purpose' and self.use_llm:\n                self.process_items(question_id, question_text, self.base_name, name, info, context, f'{question_type}_variables')\n            elif question_id != f'{question_type}_variable_purpose':\n                query = question_text.format(filename=self.base_name, **mapping)\n                self.process_question(question_id, query, context, info)",
        "output": "question_id, self, question_type, question_text"
    },
    {
        "instruction": "Inputs to method: (generate) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def generate(self) -> tuple[List[Dict], List[Dict]]:\n    for question in self.questions:\n        question_id = question['id']\n        question_text = question['text']\n        question_type = question['type']\n        if question_type == 'file':\n            self.process_file_question(question_id, question_text)\n        elif question_type in ['function', 'class', 'method']:\n            self.process_func_class_question(question_type, question_id, question_text)\n    return (self.qa_list, self.instruct_list)",
        "output": "self"
    },
    {
        "instruction": "Calls in method: (__init__) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def __init__(self, file_path: str, file_details: Dict, base_name: str, questions: List[Dict], use_llm: bool, config: Dict):\n    self.file_path = file_path\n    self.file_details = file_details\n    self.base_name = base_name\n    self.questions = questions\n    self.qa_list = []\n    self.instruct_list = []\n    self.question_mapping = {'file': 'file', 'function': 'functions', 'class': 'classes', 'method': 'classes'}\n    self.use_llm = use_llm\n    self.config = config\n    if self.use_llm:\n        try:\n            self.llm_config = config['inference_model']\n            self.llm = get_model(self.llm_config)\n        except (FileNotFoundError, yaml.YAMLError, ImportError, AttributeError) as e:\n            logger.error(f'Failed to load configuration file: {e}')\n            self.use_llm = False\n            self.llm_config = None\n            self.llm = None\n    else:\n        self.llm = None",
        "output": "logger.error, get_model"
    },
    {
        "instruction": "Calls in method: (clean_and_get_unique_elements) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "@staticmethod\ndef clean_and_get_unique_elements(input_str: str) -> str:\n    cleaned_elements = set((re.sub('[^\\\\w\\\\-_>\\\\s:/.]', '', element.strip()) for element in re.sub('\\\\s+', ' ', input_str).split(',')))\n    return ', '.join(cleaned_elements)",
        "output": ", element.strip, re.subs, re.sub, set, .join,  , input_str.split"
    },
    {
        "instruction": "Calls in method: (add_to_list) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "@staticmethod\ndef add_to_list(list_to_update: List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:\n    if response and response.strip() and (response != 'None'):\n        list_to_update.append({'instruction': query, 'input': additional_field, 'output': response} if additional_field else {'question': query, 'answer': response})\n    return list_to_update",
        "output": "list_to_update.append, response.strip"
    },
    {
        "instruction": "Calls in method: (get_response_from_llm) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def get_response_from_llm(self, query: str, context: str) -> str:\n    if not self.llm:\n        logger.error('AI model not available.')\n        return ''\n    prompt = self.config['prompt_template'].format(context=context, query=query)\n    logging.info(f'Query: {query}')\n    response = self.llm(prompt)\n    logging.info(f'Response: {response}')\n    return response",
        "output": "logger.error, self.llm, self.configprompt_template.format, logging.info"
    },
    {
        "instruction": "Calls in method: (process_items) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def process_items(self, question_id: str, question_text: str, base_name: str, name: str, info: Dict, context: str, item_type: str) -> None:\n    if info[item_type]:\n        items = [item.strip() for item in self.clean_and_get_unique_elements(str(info[item_type])).split(',') if item]\n        for item in items:\n            query = question_text.format(filename=base_name, **{f\"{item_type.split('_')[0]}_name\": name, f\"{item_type.split('_')[0]}_variable\": item})\n            self.process_question(question_id, query, context, info)",
        "output": "question_text.format, self.clean_and_get_unique_elementsstrinfoitem_type.split, item.strip, str, self.process_question, self.clean_and_get_unique_elements, item_type.split"
    },
    {
        "instruction": "Calls in method: (process_question) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def process_question(self, question_id: str, query: str, context: str, info) -> None:\n    if question_id.endswith('code_graph'):\n        response = info.get(question_id, {})\n    else:\n        response = self.get_response_from_llm(query, context) if self.use_llm and question_id.endswith('purpose') else self.clean_and_get_unique_elements(str(info.get(question_id, '')))\n    if response and response != 'None':\n        response_str = str(response)\n        response_str = response_str.strip()\n        if response_str:\n            self.qa_list.append({'question': query, 'answer': response_str})\n            self.instruct_list.append({'instruction': query, 'input': context, 'output': response_str})",
        "output": "info.get, question_id.endswith, self.get_response_from_llm, str, self.qa_list.append, response_str.strip, self.clean_and_get_unique_elements, self.instruct_list.append"
    },
    {
        "instruction": "Calls in method: (process_file_question) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def process_file_question(self, question_id: str, question_text: str) -> None:\n    query = question_text.format(filename=self.base_name)\n    context = self.file_details['file_info']['file_code']\n    info = self.file_details['file_info']\n    self.process_question(question_id, query, context, info)",
        "output": "question_text.format, self.process_question"
    },
    {
        "instruction": "Calls in method: (process_func_class_question) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def process_func_class_question(self, question_type: str, question_id: str, question_text: str) -> None:\n    if question_type == 'method':\n        for class_name, class_info in self.file_details['classes'].items():\n            for key, method_info in class_info.items():\n                if key.startswith('class_method_'):\n                    method_name = key[len('class_method_'):]\n                    context = method_info['method_code']\n                    mapping = {'class_name': class_name, 'method_name': method_name}\n                    query = question_text.format(filename=self.base_name, **mapping)\n                    self.process_question(question_id, query, context, method_info)\n    else:\n        for name, info in self.file_details[self.question_mapping[question_type]].items():\n            context = info[f'{question_type}_code']\n            mapping = {f'{question_type}_name': name}\n            if question_id == f'{question_type}_variable_purpose' and self.use_llm:\n                self.process_items(question_id, question_text, self.base_name, name, info, context, f'{question_type}_variables')\n            elif question_id != f'{question_type}_variable_purpose':\n                query = question_text.format(filename=self.base_name, **mapping)\n                self.process_question(question_id, query, context, info)",
        "output": "question_text.format, self.file_detailsself.question_mappingquestion_type.items, self.file_detailsclasses.items, self.process_items, class_info.items, self.process_question, len, key.startswith"
    },
    {
        "instruction": "Calls in method: (generate) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def generate(self) -> tuple[List[Dict], List[Dict]]:\n    for question in self.questions:\n        question_id = question['id']\n        question_text = question['text']\n        question_type = question['type']\n        if question_type == 'file':\n            self.process_file_question(question_id, question_text)\n        elif question_type in ['function', 'class', 'method']:\n            self.process_func_class_question(question_type, question_id, question_text)\n    return (self.qa_list, self.instruct_list)",
        "output": "self.process_file_question, self.process_func_class_question"
    },
    {
        "instruction": "Returns from method: (clean_and_get_unique_elements) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "@staticmethod\ndef clean_and_get_unique_elements(input_str: str) -> str:\n    cleaned_elements = set((re.sub('[^\\\\w\\\\-_>\\\\s:/.]', '', element.strip()) for element in re.sub('\\\\s+', ' ', input_str).split(',')))\n    return ', '.join(cleaned_elements)",
        "output": ", .joincleaned_elements"
    },
    {
        "instruction": "Returns from method: (add_to_list) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "@staticmethod\ndef add_to_list(list_to_update: List[Dict], query: str, response: str, additional_field=None) -> List[Dict]:\n    if response and response.strip() and (response != 'None'):\n        list_to_update.append({'instruction': query, 'input': additional_field, 'output': response} if additional_field else {'question': query, 'answer': response})\n    return list_to_update",
        "output": "list_to_update"
    },
    {
        "instruction": "Returns from method: (get_response_from_llm) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def get_response_from_llm(self, query: str, context: str) -> str:\n    if not self.llm:\n        logger.error('AI model not available.')\n        return ''\n    prompt = self.config['prompt_template'].format(context=context, query=query)\n    logging.info(f'Query: {query}')\n    response = self.llm(prompt)\n    logging.info(f'Response: {response}')\n    return response",
        "output": ", response"
    },
    {
        "instruction": "Returns from method: (generate) in class: (PythonJsonGenerator) in file: (py2dataset.py2dataset.get_python_json.py)?",
        "input": "def generate(self) -> tuple[List[Dict], List[Dict]]:\n    for question in self.questions:\n        question_id = question['id']\n        question_text = question['text']\n        question_type = question['type']\n        if question_type == 'file':\n            self.process_file_question(question_id, question_text)\n        elif question_type in ['function', 'class', 'method']:\n            self.process_func_class_question(question_type, question_id, question_text)\n    return (self.qa_list, self.instruct_list)",
        "output": "self.qa_list, self.instruct_list"
    }
]