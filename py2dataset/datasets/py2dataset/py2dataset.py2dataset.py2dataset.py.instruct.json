[
    {
        "instruction": "Dependencies of file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "\"\"\"\nFor each Python files within given directory, generate qa and instruct json\nfiles that address the questions in the questions.json file. Combine these to \ncreate a composite qa.json and instruct.json file that includes all of the \ndata filues stored in the output_dir (./datasets by default)\nRequirements:\n[req01] The read_file function shall accept a file path as an argument and\n        return its contents as a dictionary.\n[req02] The write_file function shall accept a dictionary and a file path as\n        arguments, and write the dictionary to the file in JSON or YAML format.\n[req03] The combine_files function shall accept a directory path as an \n        argument, merge all JSON files in the directory into 'qa.json' and \n        'instruct.json', remove duplicates, and replace duplicate inputs with\n        an empty string.\n[req04] The process_python_directories function shall accept a directory path,\n        a dictionary of questions, a boolean flag indicating whether to use a\n        large language model (LLM), and an output directory path as arguments.\n[req05] The process_python_directories function shall analyze all Python files\n        in the given directory and its subdirectories, generate a summary of\n        each Python file's contents, generate question-answer pairs and \n        instructions for each Python file, and write the summaries, \n        question-answer pairs, and instructions to JSON and YAML files in the\n        specified output directory.\n[req06] The process_python_directories function shall call the combine_files\n        function to merge all JSON files in the output directory after \n        processing all Python files in the directory.\n[req07] The python_code_to_dataset function shall accept a directory path, a\n        boolean flag indicating whether to use a large language model (LLM),\n        and an output directory path as arguments.\n[req08] The python_code_to_dataset function shall read questions from a JSON\n        file named 'questions.json', call the process_python_directories\n        function to analyze all Python files in the given directory and its\n        subdirectories, and increase the Python recursion limit to handle \n        large files.\n[req09] The command line interface of the script shall accept five arguments:\n        the directory of Python files to analyze, a flag to indicate the use \n        of the large language model (LLM), a flag to suppress info logging\n        messages, the output directory for generated files, and a flag to \n        clean input data.\n[req10] The command line interface shall prompt the user for a directory if\n        no directory argument is provided.\n[req11] The command line interface shall raise a ValueError if the provided\n        directory argument does not exist.\n[req12] The logging level shall be set to WARNING if the '--quiet' flag is set,\n        and to INFO otherwise by the command line interface.\n[req13] The python_code_to_dataset function shall be called with the directory,\n        'use_llm' flag, and output directory provided in the command line\n        arguments.\n[req14] The create_code_graph function shall accept file details, a base name,\n        an output subdirectory, and a graph type as arguments. It shall \n        generate a graph from the file details and save\n\"\"\"\nimport argparse\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom pathlib import Path\nfrom typing import Dict, List, Union\nfrom get_python_file_details import get_python_file_details\nfrom get_python_json import get_python_json\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef combine_json_files(directory) -> None:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'instruct.json':\n            instruct_combined_data = combined_data.copy()\n        combined_data = []  \n\n    # remove duplicate inputs from instruct.json to make a cleaned_instruct.json\n    seen_inputs = set()\n    for item in instruct_combined_data:\n        if item['input'] in seen_inputs:\n            item['input'] = ''\n        else:\n            seen_inputs.add(item['input'])\n    cleaned_instruct_file_path = Path(directory) / 'cleaned_instruct.json'\n    write_file(instruct_combined_data, cleaned_instruct_file_path)\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        # Create graphs\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'  \n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        # Add edges\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_input' in edge:\n                    edge_data['target_input'] = edge['target_input']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n            \n        # Save code graph as png\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_input' in edge[2] and edge[2]['target_input']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_input'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()  # Close the figure\n\n\ndef process_python_directories(start_path: str, questions: Dict[str, Union[str, Dict]], use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config: dict = None) -> None:\n    \"\"\"\n    Processes all Python files in a given directory and its subdirectories.\n    Args:\n        start_path (str): The directory to start the search for Python files.\n        questions (Dict): The set of questions to answer about each Python \n            file.\n        use_llm (bool): Whether to use the LLM model to generate answers for\n            json.\n        output_dir (str): The directory where the output files should be\n            written. If not provided, the function writes the files to the\n            'python_json_and_yaml' directory in the current working directory.\n    \"\"\"\n    python_files = list(Path(start_path).rglob('[!_]*.py'))\n    for file_path in python_files:\n        logging.info(f'Processing: {file_path}')\n        relative_path = Path(file_path).relative_to(start_path)\n        base_name = '.'.join(part for part in relative_path.parts)\n\n        # use AST to get python file details\n        file_details = get_python_file_details(file_path)\n        if file_details is None or isinstance(file_details, tuple):\n            continue\n\n        # get lists for qa.json and intruct.json for python file\n        \n        qa_list, instruct_list = get_python_json(file_path, file_details, base_name, questions, use_llm, model_config)\n        if not qa_list:\n            continue\n\n        # create output directory if needed\n        if output_dir is None:\n            output_dir = './datasets/'\n        output_subdir = Path(output_dir) / relative_path.parts[0]\n        output_subdir.mkdir(parents=True, exist_ok=True)\n\n        # write qa.json and instrunct.json files\n        file_names = [f'{base_name}.qa.json', f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n        contents = [qa_list, instruct_list, file_details]\n        for file_name, content in zip(file_names, contents):\n            write_file(content, output_subdir / file_name)\n\n        # Create code graph images\n        if graph:\n            create_code_graph(file_details, base_name, output_subdir)\n\n    # combine all of the qa.json and instruct.json files together\n    combine_json_files(output_dir)\n\n\ndef py2dataset(start_path: str, use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config_path: str=None) -> None:\n    sys.setrecursionlimit(3000)  # Increase the recursion limit for AST\n    questions = read_file(Path('questions.json'))\n    model_config = None\n    if model_config_path:\n        model_config = read_file(Path(model_config_path))\n    process_python_directories(start_path, questions, use_llm, graph, output_dir, model_config)\n\nif __name__ == \"__main__\":\n    arg_string = ' '.join(sys.argv[1:])\n    use_llm = False\n    quiet = False\n    graph = False\n    output_dir = None\n    model_config_path = None\n    if '--use_llm' in arg_string:\n        use_llm = True\n        arg_string = arg_string.replace('--use_llm', '')\n    if '--quiet' in arg_string:\n        quiet = True\n        arg_string = arg_string.replace('--quiet', '')\n    if '--graph' in arg_string:\n        graph = True\n        arg_string = arg_string.replace('--graph', '')\n    if '--output_dir' in arg_string:\n        output_dir = arg_string.split('--output_dir ')[1].split(' ')[0]\n        arg_string = arg_string.replace(f'--output_dir {output_dir}', '')\n    if '--model_config' in arg_string:\n        model_config_path = arg_string.split('--model_config ')[1].split(' ')[0]\n        arg_string = arg_string.replace(f'--model_config {model_config_path}', '')\n\n    directory = arg_string.strip()\n    if directory.endswith('\"'):\n        directory = directory[:-1]\n    if not os.path.isdir(directory):\n        raise ValueError(f\"The directory {directory} does not exist.\")\n\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n\n    py2dataset(directory, use_llm, graph, output_dir, model_config_path)",
        "output": "pathlib, typing, get_python_file_details, json, argparse, sys, get_python_json, matplotlib.pyplot, networkx, re, logging, os, yaml"
    },
    {
        "instruction": "Structural graph of the relationships between the functions and classes defined in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "\"\"\"\nFor each Python files within given directory, generate qa and instruct json\nfiles that address the questions in the questions.json file. Combine these to \ncreate a composite qa.json and instruct.json file that includes all of the \ndata filues stored in the output_dir (./datasets by default)\nRequirements:\n[req01] The read_file function shall accept a file path as an argument and\n        return its contents as a dictionary.\n[req02] The write_file function shall accept a dictionary and a file path as\n        arguments, and write the dictionary to the file in JSON or YAML format.\n[req03] The combine_files function shall accept a directory path as an \n        argument, merge all JSON files in the directory into 'qa.json' and \n        'instruct.json', remove duplicates, and replace duplicate inputs with\n        an empty string.\n[req04] The process_python_directories function shall accept a directory path,\n        a dictionary of questions, a boolean flag indicating whether to use a\n        large language model (LLM), and an output directory path as arguments.\n[req05] The process_python_directories function shall analyze all Python files\n        in the given directory and its subdirectories, generate a summary of\n        each Python file's contents, generate question-answer pairs and \n        instructions for each Python file, and write the summaries, \n        question-answer pairs, and instructions to JSON and YAML files in the\n        specified output directory.\n[req06] The process_python_directories function shall call the combine_files\n        function to merge all JSON files in the output directory after \n        processing all Python files in the directory.\n[req07] The python_code_to_dataset function shall accept a directory path, a\n        boolean flag indicating whether to use a large language model (LLM),\n        and an output directory path as arguments.\n[req08] The python_code_to_dataset function shall read questions from a JSON\n        file named 'questions.json', call the process_python_directories\n        function to analyze all Python files in the given directory and its\n        subdirectories, and increase the Python recursion limit to handle \n        large files.\n[req09] The command line interface of the script shall accept five arguments:\n        the directory of Python files to analyze, a flag to indicate the use \n        of the large language model (LLM), a flag to suppress info logging\n        messages, the output directory for generated files, and a flag to \n        clean input data.\n[req10] The command line interface shall prompt the user for a directory if\n        no directory argument is provided.\n[req11] The command line interface shall raise a ValueError if the provided\n        directory argument does not exist.\n[req12] The logging level shall be set to WARNING if the '--quiet' flag is set,\n        and to INFO otherwise by the command line interface.\n[req13] The python_code_to_dataset function shall be called with the directory,\n        'use_llm' flag, and output directory provided in the command line\n        arguments.\n[req14] The create_code_graph function shall accept file details, a base name,\n        an output subdirectory, and a graph type as arguments. It shall \n        generate a graph from the file details and save\n\"\"\"\nimport argparse\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom pathlib import Path\nfrom typing import Dict, List, Union\nfrom get_python_file_details import get_python_file_details\nfrom get_python_json import get_python_json\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef combine_json_files(directory) -> None:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'instruct.json':\n            instruct_combined_data = combined_data.copy()\n        combined_data = []  \n\n    # remove duplicate inputs from instruct.json to make a cleaned_instruct.json\n    seen_inputs = set()\n    for item in instruct_combined_data:\n        if item['input'] in seen_inputs:\n            item['input'] = ''\n        else:\n            seen_inputs.add(item['input'])\n    cleaned_instruct_file_path = Path(directory) / 'cleaned_instruct.json'\n    write_file(instruct_combined_data, cleaned_instruct_file_path)\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        # Create graphs\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'  \n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        # Add edges\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_input' in edge:\n                    edge_data['target_input'] = edge['target_input']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n            \n        # Save code graph as png\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_input' in edge[2] and edge[2]['target_input']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_input'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()  # Close the figure\n\n\ndef process_python_directories(start_path: str, questions: Dict[str, Union[str, Dict]], use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config: dict = None) -> None:\n    \"\"\"\n    Processes all Python files in a given directory and its subdirectories.\n    Args:\n        start_path (str): The directory to start the search for Python files.\n        questions (Dict): The set of questions to answer about each Python \n            file.\n        use_llm (bool): Whether to use the LLM model to generate answers for\n            json.\n        output_dir (str): The directory where the output files should be\n            written. If not provided, the function writes the files to the\n            'python_json_and_yaml' directory in the current working directory.\n    \"\"\"\n    python_files = list(Path(start_path).rglob('[!_]*.py'))\n    for file_path in python_files:\n        logging.info(f'Processing: {file_path}')\n        relative_path = Path(file_path).relative_to(start_path)\n        base_name = '.'.join(part for part in relative_path.parts)\n\n        # use AST to get python file details\n        file_details = get_python_file_details(file_path)\n        if file_details is None or isinstance(file_details, tuple):\n            continue\n\n        # get lists for qa.json and intruct.json for python file\n        \n        qa_list, instruct_list = get_python_json(file_path, file_details, base_name, questions, use_llm, model_config)\n        if not qa_list:\n            continue\n\n        # create output directory if needed\n        if output_dir is None:\n            output_dir = './datasets/'\n        output_subdir = Path(output_dir) / relative_path.parts[0]\n        output_subdir.mkdir(parents=True, exist_ok=True)\n\n        # write qa.json and instrunct.json files\n        file_names = [f'{base_name}.qa.json', f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n        contents = [qa_list, instruct_list, file_details]\n        for file_name, content in zip(file_names, contents):\n            write_file(content, output_subdir / file_name)\n\n        # Create code graph images\n        if graph:\n            create_code_graph(file_details, base_name, output_subdir)\n\n    # combine all of the qa.json and instruct.json files together\n    combine_json_files(output_dir)\n\n\ndef py2dataset(start_path: str, use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config_path: str=None) -> None:\n    sys.setrecursionlimit(3000)  # Increase the recursion limit for AST\n    questions = read_file(Path('questions.json'))\n    model_config = None\n    if model_config_path:\n        model_config = read_file(Path(model_config_path))\n    process_python_directories(start_path, questions, use_llm, graph, output_dir, model_config)\n\nif __name__ == \"__main__\":\n    arg_string = ' '.join(sys.argv[1:])\n    use_llm = False\n    quiet = False\n    graph = False\n    output_dir = None\n    model_config_path = None\n    if '--use_llm' in arg_string:\n        use_llm = True\n        arg_string = arg_string.replace('--use_llm', '')\n    if '--quiet' in arg_string:\n        quiet = True\n        arg_string = arg_string.replace('--quiet', '')\n    if '--graph' in arg_string:\n        graph = True\n        arg_string = arg_string.replace('--graph', '')\n    if '--output_dir' in arg_string:\n        output_dir = arg_string.split('--output_dir ')[1].split(' ')[0]\n        arg_string = arg_string.replace(f'--output_dir {output_dir}', '')\n    if '--model_config' in arg_string:\n        model_config_path = arg_string.split('--model_config ')[1].split(' ')[0]\n        arg_string = arg_string.replace(f'--model_config {model_config_path}', '')\n\n    directory = arg_string.strip()\n    if directory.endswith('\"'):\n        directory = directory[:-1]\n    if not os.path.isdir(directory):\n        raise ValueError(f\"The directory {directory} does not exist.\")\n\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n\n    py2dataset(directory, use_llm, graph, output_dir, model_config_path)",
        "output": "{'nodes': ['read_file', 'write_file', 'combine_json_files', 'create_code_graph', 'process_python_directories', 'py2dataset'], 'edges': [{'source': 'combine_json_files', 'target': 'write_file', 'target_input': ['data', 'file_path']}, {'source': 'combine_json_files', 'target': 'read_file', 'target_input': ['file_path'], 'target_returns': ['json.load(f)', 'yaml.load(f)']}, {'source': 'process_python_directories', 'target': 'write_file', 'target_input': ['data', 'file_path']}, {'source': 'process_python_directories', 'target': 'combine_json_files', 'target_input': ['directory']}, {'source': 'process_python_directories', 'target': 'create_code_graph', 'target_input': ['file_details', 'base_name', 'output_subdir']}, {'source': 'py2dataset', 'target': 'process_python_directories', 'target_input': ['start_path', 'questions', 'use_llm', 'graph', 'output_dir', 'model_config']}, {'source': 'py2dataset', 'target': 'read_file', 'target_input': ['file_path'], 'target_returns': ['json.load(f)', 'yaml.load(f)']}]}"
    },
    {
        "instruction": "Structural graph of the relationships between the functions and classes defined and used in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "\"\"\"\nFor each Python files within given directory, generate qa and instruct json\nfiles that address the questions in the questions.json file. Combine these to \ncreate a composite qa.json and instruct.json file that includes all of the \ndata filues stored in the output_dir (./datasets by default)\nRequirements:\n[req01] The read_file function shall accept a file path as an argument and\n        return its contents as a dictionary.\n[req02] The write_file function shall accept a dictionary and a file path as\n        arguments, and write the dictionary to the file in JSON or YAML format.\n[req03] The combine_files function shall accept a directory path as an \n        argument, merge all JSON files in the directory into 'qa.json' and \n        'instruct.json', remove duplicates, and replace duplicate inputs with\n        an empty string.\n[req04] The process_python_directories function shall accept a directory path,\n        a dictionary of questions, a boolean flag indicating whether to use a\n        large language model (LLM), and an output directory path as arguments.\n[req05] The process_python_directories function shall analyze all Python files\n        in the given directory and its subdirectories, generate a summary of\n        each Python file's contents, generate question-answer pairs and \n        instructions for each Python file, and write the summaries, \n        question-answer pairs, and instructions to JSON and YAML files in the\n        specified output directory.\n[req06] The process_python_directories function shall call the combine_files\n        function to merge all JSON files in the output directory after \n        processing all Python files in the directory.\n[req07] The python_code_to_dataset function shall accept a directory path, a\n        boolean flag indicating whether to use a large language model (LLM),\n        and an output directory path as arguments.\n[req08] The python_code_to_dataset function shall read questions from a JSON\n        file named 'questions.json', call the process_python_directories\n        function to analyze all Python files in the given directory and its\n        subdirectories, and increase the Python recursion limit to handle \n        large files.\n[req09] The command line interface of the script shall accept five arguments:\n        the directory of Python files to analyze, a flag to indicate the use \n        of the large language model (LLM), a flag to suppress info logging\n        messages, the output directory for generated files, and a flag to \n        clean input data.\n[req10] The command line interface shall prompt the user for a directory if\n        no directory argument is provided.\n[req11] The command line interface shall raise a ValueError if the provided\n        directory argument does not exist.\n[req12] The logging level shall be set to WARNING if the '--quiet' flag is set,\n        and to INFO otherwise by the command line interface.\n[req13] The python_code_to_dataset function shall be called with the directory,\n        'use_llm' flag, and output directory provided in the command line\n        arguments.\n[req14] The create_code_graph function shall accept file details, a base name,\n        an output subdirectory, and a graph type as arguments. It shall \n        generate a graph from the file details and save\n\"\"\"\nimport argparse\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom pathlib import Path\nfrom typing import Dict, List, Union\nfrom get_python_file_details import get_python_file_details\nfrom get_python_json import get_python_json\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef combine_json_files(directory) -> None:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'instruct.json':\n            instruct_combined_data = combined_data.copy()\n        combined_data = []  \n\n    # remove duplicate inputs from instruct.json to make a cleaned_instruct.json\n    seen_inputs = set()\n    for item in instruct_combined_data:\n        if item['input'] in seen_inputs:\n            item['input'] = ''\n        else:\n            seen_inputs.add(item['input'])\n    cleaned_instruct_file_path = Path(directory) / 'cleaned_instruct.json'\n    write_file(instruct_combined_data, cleaned_instruct_file_path)\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        # Create graphs\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'  \n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        # Add edges\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_input' in edge:\n                    edge_data['target_input'] = edge['target_input']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n            \n        # Save code graph as png\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_input' in edge[2] and edge[2]['target_input']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_input'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()  # Close the figure\n\n\ndef process_python_directories(start_path: str, questions: Dict[str, Union[str, Dict]], use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config: dict = None) -> None:\n    \"\"\"\n    Processes all Python files in a given directory and its subdirectories.\n    Args:\n        start_path (str): The directory to start the search for Python files.\n        questions (Dict): The set of questions to answer about each Python \n            file.\n        use_llm (bool): Whether to use the LLM model to generate answers for\n            json.\n        output_dir (str): The directory where the output files should be\n            written. If not provided, the function writes the files to the\n            'python_json_and_yaml' directory in the current working directory.\n    \"\"\"\n    python_files = list(Path(start_path).rglob('[!_]*.py'))\n    for file_path in python_files:\n        logging.info(f'Processing: {file_path}')\n        relative_path = Path(file_path).relative_to(start_path)\n        base_name = '.'.join(part for part in relative_path.parts)\n\n        # use AST to get python file details\n        file_details = get_python_file_details(file_path)\n        if file_details is None or isinstance(file_details, tuple):\n            continue\n\n        # get lists for qa.json and intruct.json for python file\n        \n        qa_list, instruct_list = get_python_json(file_path, file_details, base_name, questions, use_llm, model_config)\n        if not qa_list:\n            continue\n\n        # create output directory if needed\n        if output_dir is None:\n            output_dir = './datasets/'\n        output_subdir = Path(output_dir) / relative_path.parts[0]\n        output_subdir.mkdir(parents=True, exist_ok=True)\n\n        # write qa.json and instrunct.json files\n        file_names = [f'{base_name}.qa.json', f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n        contents = [qa_list, instruct_list, file_details]\n        for file_name, content in zip(file_names, contents):\n            write_file(content, output_subdir / file_name)\n\n        # Create code graph images\n        if graph:\n            create_code_graph(file_details, base_name, output_subdir)\n\n    # combine all of the qa.json and instruct.json files together\n    combine_json_files(output_dir)\n\n\ndef py2dataset(start_path: str, use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config_path: str=None) -> None:\n    sys.setrecursionlimit(3000)  # Increase the recursion limit for AST\n    questions = read_file(Path('questions.json'))\n    model_config = None\n    if model_config_path:\n        model_config = read_file(Path(model_config_path))\n    process_python_directories(start_path, questions, use_llm, graph, output_dir, model_config)\n\nif __name__ == \"__main__\":\n    arg_string = ' '.join(sys.argv[1:])\n    use_llm = False\n    quiet = False\n    graph = False\n    output_dir = None\n    model_config_path = None\n    if '--use_llm' in arg_string:\n        use_llm = True\n        arg_string = arg_string.replace('--use_llm', '')\n    if '--quiet' in arg_string:\n        quiet = True\n        arg_string = arg_string.replace('--quiet', '')\n    if '--graph' in arg_string:\n        graph = True\n        arg_string = arg_string.replace('--graph', '')\n    if '--output_dir' in arg_string:\n        output_dir = arg_string.split('--output_dir ')[1].split(' ')[0]\n        arg_string = arg_string.replace(f'--output_dir {output_dir}', '')\n    if '--model_config' in arg_string:\n        model_config_path = arg_string.split('--model_config ')[1].split(' ')[0]\n        arg_string = arg_string.replace(f'--model_config {model_config_path}', '')\n\n    directory = arg_string.strip()\n    if directory.endswith('\"'):\n        directory = directory[:-1]\n    if not os.path.isdir(directory):\n        raise ValueError(f\"The directory {directory} does not exist.\")\n\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n\n    py2dataset(directory, use_llm, graph, output_dir, model_config_path)",
        "output": "{'nodes': ['read_file', 'write_file', 'combine_json_files', 'create_code_graph', 'process_python_directories', 'py2dataset', 'yaml.load', 'json.load', 'file_path.open', 'yaml.dump', 'json.dump', 'combined_data.copy', 'Path(directory).rglob', 'set', 'list', '{i[keys[file_names.index(file)]]: i for i in combined_data}.values', 'Path', 'seen_inputs.add', 'combined_data.extend', 'file_names.index', 'file_path.exists', \"'\\\\n'.join\", 'plt.close', 'nx.spring_layout', 'plt.figure', \"', '.join\", 'G.add_node', 'plt.savefig', 'nx.DiGraph', 'G.edges', 'label.append', 'nx.draw', 'G.add_edge', 'nx.draw_networkx_edge_labels', 'zip', 'get_python_file_details', 'logging.info', 'Path(start_path).rglob', 'isinstance', 'output_subdir.mkdir', 'get_python_json', 'Path(file_path).relative_to', \"'.'.join\", 'sys.setrecursionlimit'], 'edges': [{'source': 'read_file', 'target': 'yaml.load'}, {'source': 'read_file', 'target': 'json.load'}, {'source': 'read_file', 'target': 'file_path.open'}, {'source': 'write_file', 'target': 'yaml.dump'}, {'source': 'write_file', 'target': 'json.dump'}, {'source': 'write_file', 'target': 'file_path.open'}, {'source': 'combine_json_files', 'target': 'combined_data.copy'}, {'source': 'combine_json_files', 'target': 'write_file', 'target_input': ['data', 'file_path']}, {'source': 'combine_json_files', 'target': 'Path(directory).rglob'}, {'source': 'combine_json_files', 'target': 'set'}, {'source': 'combine_json_files', 'target': 'list'}, {'source': 'combine_json_files', 'target': '{i[keys[file_names.index(file)]]: i for i in combined_data}.values'}, {'source': 'combine_json_files', 'target': 'Path'}, {'source': 'combine_json_files', 'target': 'seen_inputs.add'}, {'source': 'combine_json_files', 'target': 'combined_data.extend'}, {'source': 'combine_json_files', 'target': 'file_names.index'}, {'source': 'combine_json_files', 'target': 'file_path.exists'}, {'source': 'combine_json_files', 'target': 'read_file', 'target_input': ['file_path'], 'target_returns': ['json.load(f)', 'yaml.load(f)']}, {'source': 'create_code_graph', 'target': \"'\\\\n'.join\"}, {'source': 'create_code_graph', 'target': 'plt.close'}, {'source': 'create_code_graph', 'target': 'nx.spring_layout'}, {'source': 'create_code_graph', 'target': 'plt.figure'}, {'source': 'create_code_graph', 'target': \"', '.join\"}, {'source': 'create_code_graph', 'target': 'G.add_node'}, {'source': 'create_code_graph', 'target': 'plt.savefig'}, {'source': 'create_code_graph', 'target': 'nx.DiGraph'}, {'source': 'create_code_graph', 'target': 'G.edges'}, {'source': 'create_code_graph', 'target': 'label.append'}, {'source': 'create_code_graph', 'target': 'nx.draw'}, {'source': 'create_code_graph', 'target': 'G.add_edge'}, {'source': 'create_code_graph', 'target': 'nx.draw_networkx_edge_labels'}, {'source': 'process_python_directories', 'target': 'zip'}, {'source': 'process_python_directories', 'target': 'get_python_file_details'}, {'source': 'process_python_directories', 'target': 'logging.info'}, {'source': 'process_python_directories', 'target': 'write_file', 'target_input': ['data', 'file_path']}, {'source': 'process_python_directories', 'target': 'Path(start_path).rglob'}, {'source': 'process_python_directories', 'target': 'isinstance'}, {'source': 'process_python_directories', 'target': 'output_subdir.mkdir'}, {'source': 'process_python_directories', 'target': 'get_python_json'}, {'source': 'process_python_directories', 'target': 'list'}, {'source': 'process_python_directories', 'target': 'Path(file_path).relative_to'}, {'source': 'process_python_directories', 'target': 'Path'}, {'source': 'process_python_directories', 'target': \"'.'.join\"}, {'source': 'process_python_directories', 'target': 'combine_json_files', 'target_input': ['directory']}, {'source': 'process_python_directories', 'target': 'create_code_graph', 'target_input': ['file_details', 'base_name', 'output_subdir']}, {'source': 'py2dataset', 'target': 'Path'}, {'source': 'py2dataset', 'target': 'process_python_directories', 'target_input': ['start_path', 'questions', 'use_llm', 'graph', 'output_dir', 'model_config']}, {'source': 'py2dataset', 'target': 'sys.setrecursionlimit'}, {'source': 'py2dataset', 'target': 'read_file', 'target_input': ['file_path'], 'target_returns': ['json.load(f)', 'yaml.load(f)']}]}"
    },
    {
        "instruction": "Funtions in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "\"\"\"\nFor each Python files within given directory, generate qa and instruct json\nfiles that address the questions in the questions.json file. Combine these to \ncreate a composite qa.json and instruct.json file that includes all of the \ndata filues stored in the output_dir (./datasets by default)\nRequirements:\n[req01] The read_file function shall accept a file path as an argument and\n        return its contents as a dictionary.\n[req02] The write_file function shall accept a dictionary and a file path as\n        arguments, and write the dictionary to the file in JSON or YAML format.\n[req03] The combine_files function shall accept a directory path as an \n        argument, merge all JSON files in the directory into 'qa.json' and \n        'instruct.json', remove duplicates, and replace duplicate inputs with\n        an empty string.\n[req04] The process_python_directories function shall accept a directory path,\n        a dictionary of questions, a boolean flag indicating whether to use a\n        large language model (LLM), and an output directory path as arguments.\n[req05] The process_python_directories function shall analyze all Python files\n        in the given directory and its subdirectories, generate a summary of\n        each Python file's contents, generate question-answer pairs and \n        instructions for each Python file, and write the summaries, \n        question-answer pairs, and instructions to JSON and YAML files in the\n        specified output directory.\n[req06] The process_python_directories function shall call the combine_files\n        function to merge all JSON files in the output directory after \n        processing all Python files in the directory.\n[req07] The python_code_to_dataset function shall accept a directory path, a\n        boolean flag indicating whether to use a large language model (LLM),\n        and an output directory path as arguments.\n[req08] The python_code_to_dataset function shall read questions from a JSON\n        file named 'questions.json', call the process_python_directories\n        function to analyze all Python files in the given directory and its\n        subdirectories, and increase the Python recursion limit to handle \n        large files.\n[req09] The command line interface of the script shall accept five arguments:\n        the directory of Python files to analyze, a flag to indicate the use \n        of the large language model (LLM), a flag to suppress info logging\n        messages, the output directory for generated files, and a flag to \n        clean input data.\n[req10] The command line interface shall prompt the user for a directory if\n        no directory argument is provided.\n[req11] The command line interface shall raise a ValueError if the provided\n        directory argument does not exist.\n[req12] The logging level shall be set to WARNING if the '--quiet' flag is set,\n        and to INFO otherwise by the command line interface.\n[req13] The python_code_to_dataset function shall be called with the directory,\n        'use_llm' flag, and output directory provided in the command line\n        arguments.\n[req14] The create_code_graph function shall accept file details, a base name,\n        an output subdirectory, and a graph type as arguments. It shall \n        generate a graph from the file details and save\n\"\"\"\nimport argparse\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom pathlib import Path\nfrom typing import Dict, List, Union\nfrom get_python_file_details import get_python_file_details\nfrom get_python_json import get_python_json\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef combine_json_files(directory) -> None:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'instruct.json':\n            instruct_combined_data = combined_data.copy()\n        combined_data = []  \n\n    # remove duplicate inputs from instruct.json to make a cleaned_instruct.json\n    seen_inputs = set()\n    for item in instruct_combined_data:\n        if item['input'] in seen_inputs:\n            item['input'] = ''\n        else:\n            seen_inputs.add(item['input'])\n    cleaned_instruct_file_path = Path(directory) / 'cleaned_instruct.json'\n    write_file(instruct_combined_data, cleaned_instruct_file_path)\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        # Create graphs\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'  \n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        # Add edges\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_input' in edge:\n                    edge_data['target_input'] = edge['target_input']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n            \n        # Save code graph as png\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_input' in edge[2] and edge[2]['target_input']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_input'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()  # Close the figure\n\n\ndef process_python_directories(start_path: str, questions: Dict[str, Union[str, Dict]], use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config: dict = None) -> None:\n    \"\"\"\n    Processes all Python files in a given directory and its subdirectories.\n    Args:\n        start_path (str): The directory to start the search for Python files.\n        questions (Dict): The set of questions to answer about each Python \n            file.\n        use_llm (bool): Whether to use the LLM model to generate answers for\n            json.\n        output_dir (str): The directory where the output files should be\n            written. If not provided, the function writes the files to the\n            'python_json_and_yaml' directory in the current working directory.\n    \"\"\"\n    python_files = list(Path(start_path).rglob('[!_]*.py'))\n    for file_path in python_files:\n        logging.info(f'Processing: {file_path}')\n        relative_path = Path(file_path).relative_to(start_path)\n        base_name = '.'.join(part for part in relative_path.parts)\n\n        # use AST to get python file details\n        file_details = get_python_file_details(file_path)\n        if file_details is None or isinstance(file_details, tuple):\n            continue\n\n        # get lists for qa.json and intruct.json for python file\n        \n        qa_list, instruct_list = get_python_json(file_path, file_details, base_name, questions, use_llm, model_config)\n        if not qa_list:\n            continue\n\n        # create output directory if needed\n        if output_dir is None:\n            output_dir = './datasets/'\n        output_subdir = Path(output_dir) / relative_path.parts[0]\n        output_subdir.mkdir(parents=True, exist_ok=True)\n\n        # write qa.json and instrunct.json files\n        file_names = [f'{base_name}.qa.json', f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n        contents = [qa_list, instruct_list, file_details]\n        for file_name, content in zip(file_names, contents):\n            write_file(content, output_subdir / file_name)\n\n        # Create code graph images\n        if graph:\n            create_code_graph(file_details, base_name, output_subdir)\n\n    # combine all of the qa.json and instruct.json files together\n    combine_json_files(output_dir)\n\n\ndef py2dataset(start_path: str, use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config_path: str=None) -> None:\n    sys.setrecursionlimit(3000)  # Increase the recursion limit for AST\n    questions = read_file(Path('questions.json'))\n    model_config = None\n    if model_config_path:\n        model_config = read_file(Path(model_config_path))\n    process_python_directories(start_path, questions, use_llm, graph, output_dir, model_config)\n\nif __name__ == \"__main__\":\n    arg_string = ' '.join(sys.argv[1:])\n    use_llm = False\n    quiet = False\n    graph = False\n    output_dir = None\n    model_config_path = None\n    if '--use_llm' in arg_string:\n        use_llm = True\n        arg_string = arg_string.replace('--use_llm', '')\n    if '--quiet' in arg_string:\n        quiet = True\n        arg_string = arg_string.replace('--quiet', '')\n    if '--graph' in arg_string:\n        graph = True\n        arg_string = arg_string.replace('--graph', '')\n    if '--output_dir' in arg_string:\n        output_dir = arg_string.split('--output_dir ')[1].split(' ')[0]\n        arg_string = arg_string.replace(f'--output_dir {output_dir}', '')\n    if '--model_config' in arg_string:\n        model_config_path = arg_string.split('--model_config ')[1].split(' ')[0]\n        arg_string = arg_string.replace(f'--model_config {model_config_path}', '')\n\n    directory = arg_string.strip()\n    if directory.endswith('\"'):\n        directory = directory[:-1]\n    if not os.path.isdir(directory):\n        raise ValueError(f\"The directory {directory} does not exist.\")\n\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n\n    py2dataset(directory, use_llm, graph, output_dir, model_config_path)",
        "output": "process_python_directories, py2dataset, write_file, read_file, combine_json_files, create_code_graph"
    },
    {
        "instruction": "Control Flow in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "\"\"\"\nFor each Python files within given directory, generate qa and instruct json\nfiles that address the questions in the questions.json file. Combine these to \ncreate a composite qa.json and instruct.json file that includes all of the \ndata filues stored in the output_dir (./datasets by default)\nRequirements:\n[req01] The read_file function shall accept a file path as an argument and\n        return its contents as a dictionary.\n[req02] The write_file function shall accept a dictionary and a file path as\n        arguments, and write the dictionary to the file in JSON or YAML format.\n[req03] The combine_files function shall accept a directory path as an \n        argument, merge all JSON files in the directory into 'qa.json' and \n        'instruct.json', remove duplicates, and replace duplicate inputs with\n        an empty string.\n[req04] The process_python_directories function shall accept a directory path,\n        a dictionary of questions, a boolean flag indicating whether to use a\n        large language model (LLM), and an output directory path as arguments.\n[req05] The process_python_directories function shall analyze all Python files\n        in the given directory and its subdirectories, generate a summary of\n        each Python file's contents, generate question-answer pairs and \n        instructions for each Python file, and write the summaries, \n        question-answer pairs, and instructions to JSON and YAML files in the\n        specified output directory.\n[req06] The process_python_directories function shall call the combine_files\n        function to merge all JSON files in the output directory after \n        processing all Python files in the directory.\n[req07] The python_code_to_dataset function shall accept a directory path, a\n        boolean flag indicating whether to use a large language model (LLM),\n        and an output directory path as arguments.\n[req08] The python_code_to_dataset function shall read questions from a JSON\n        file named 'questions.json', call the process_python_directories\n        function to analyze all Python files in the given directory and its\n        subdirectories, and increase the Python recursion limit to handle \n        large files.\n[req09] The command line interface of the script shall accept five arguments:\n        the directory of Python files to analyze, a flag to indicate the use \n        of the large language model (LLM), a flag to suppress info logging\n        messages, the output directory for generated files, and a flag to \n        clean input data.\n[req10] The command line interface shall prompt the user for a directory if\n        no directory argument is provided.\n[req11] The command line interface shall raise a ValueError if the provided\n        directory argument does not exist.\n[req12] The logging level shall be set to WARNING if the '--quiet' flag is set,\n        and to INFO otherwise by the command line interface.\n[req13] The python_code_to_dataset function shall be called with the directory,\n        'use_llm' flag, and output directory provided in the command line\n        arguments.\n[req14] The create_code_graph function shall accept file details, a base name,\n        an output subdirectory, and a graph type as arguments. It shall \n        generate a graph from the file details and save\n\"\"\"\nimport argparse\nimport sys\nimport os\nimport re\nimport json\nimport logging\nimport yaml\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom pathlib import Path\nfrom typing import Dict, List, Union\nfrom get_python_file_details import get_python_file_details\nfrom get_python_json import get_python_json\n\ndef read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)\n\n\ndef write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)\n\n\ndef combine_json_files(directory) -> None:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'instruct.json':\n            instruct_combined_data = combined_data.copy()\n        combined_data = []  \n\n    # remove duplicate inputs from instruct.json to make a cleaned_instruct.json\n    seen_inputs = set()\n    for item in instruct_combined_data:\n        if item['input'] in seen_inputs:\n            item['input'] = ''\n        else:\n            seen_inputs.add(item['input'])\n    cleaned_instruct_file_path = Path(directory) / 'cleaned_instruct.json'\n    write_file(instruct_combined_data, cleaned_instruct_file_path)\n\n\ndef create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        # Create graphs\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'  \n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        # Add edges\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_input' in edge:\n                    edge_data['target_input'] = edge['target_input']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n            \n        # Save code graph as png\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size = 8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_input' in edge[2] and edge[2]['target_input']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_input'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[(edge[0], edge[1])] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()  # Close the figure\n\n\ndef process_python_directories(start_path: str, questions: Dict[str, Union[str, Dict]], use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config: dict = None) -> None:\n    \"\"\"\n    Processes all Python files in a given directory and its subdirectories.\n    Args:\n        start_path (str): The directory to start the search for Python files.\n        questions (Dict): The set of questions to answer about each Python \n            file.\n        use_llm (bool): Whether to use the LLM model to generate answers for\n            json.\n        output_dir (str): The directory where the output files should be\n            written. If not provided, the function writes the files to the\n            'python_json_and_yaml' directory in the current working directory.\n    \"\"\"\n    python_files = list(Path(start_path).rglob('[!_]*.py'))\n    for file_path in python_files:\n        logging.info(f'Processing: {file_path}')\n        relative_path = Path(file_path).relative_to(start_path)\n        base_name = '.'.join(part for part in relative_path.parts)\n\n        # use AST to get python file details\n        file_details = get_python_file_details(file_path)\n        if file_details is None or isinstance(file_details, tuple):\n            continue\n\n        # get lists for qa.json and intruct.json for python file\n        \n        qa_list, instruct_list = get_python_json(file_path, file_details, base_name, questions, use_llm, model_config)\n        if not qa_list:\n            continue\n\n        # create output directory if needed\n        if output_dir is None:\n            output_dir = './datasets/'\n        output_subdir = Path(output_dir) / relative_path.parts[0]\n        output_subdir.mkdir(parents=True, exist_ok=True)\n\n        # write qa.json and instrunct.json files\n        file_names = [f'{base_name}.qa.json', f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n        contents = [qa_list, instruct_list, file_details]\n        for file_name, content in zip(file_names, contents):\n            write_file(content, output_subdir / file_name)\n\n        # Create code graph images\n        if graph:\n            create_code_graph(file_details, base_name, output_subdir)\n\n    # combine all of the qa.json and instruct.json files together\n    combine_json_files(output_dir)\n\n\ndef py2dataset(start_path: str, use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config_path: str=None) -> None:\n    sys.setrecursionlimit(3000)  # Increase the recursion limit for AST\n    questions = read_file(Path('questions.json'))\n    model_config = None\n    if model_config_path:\n        model_config = read_file(Path(model_config_path))\n    process_python_directories(start_path, questions, use_llm, graph, output_dir, model_config)\n\nif __name__ == \"__main__\":\n    arg_string = ' '.join(sys.argv[1:])\n    use_llm = False\n    quiet = False\n    graph = False\n    output_dir = None\n    model_config_path = None\n    if '--use_llm' in arg_string:\n        use_llm = True\n        arg_string = arg_string.replace('--use_llm', '')\n    if '--quiet' in arg_string:\n        quiet = True\n        arg_string = arg_string.replace('--quiet', '')\n    if '--graph' in arg_string:\n        graph = True\n        arg_string = arg_string.replace('--graph', '')\n    if '--output_dir' in arg_string:\n        output_dir = arg_string.split('--output_dir ')[1].split(' ')[0]\n        arg_string = arg_string.replace(f'--output_dir {output_dir}', '')\n    if '--model_config' in arg_string:\n        model_config_path = arg_string.split('--model_config ')[1].split(' ')[0]\n        arg_string = arg_string.replace(f'--model_config {model_config_path}', '')\n\n    directory = arg_string.strip()\n    if directory.endswith('\"'):\n        directory = directory[:-1]\n    if not os.path.isdir(directory):\n        raise ValueError(f\"The directory {directory} does not exist.\")\n\n    if quiet:\n        logging.getLogger().setLevel(logging.WARNING)\n    else:\n        logging.getLogger().setLevel(logging.INFO)\n\n    py2dataset(directory, use_llm, graph, output_dir, model_config_path)",
        "output": "module -> def read_file -> with -> if -> if -> def write_file -> with -> if -> if -> def combine_json_files -> for -> if -> for -> if -> for -> if -> def create_code_graph -> for -> for -> for -> if -> if -> if -> for -> if -> if -> def process_python_directories -> for -> if -> if -> if -> for -> if -> def py2dataset -> if -> if -> if -> if -> if -> if -> if -> if -> if -> if"
    },
    {
        "instruction": "Inputs to function: (read_file) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "file_path"
    },
    {
        "instruction": "Inputs to function: (write_file) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "file_path, data"
    },
    {
        "instruction": "Inputs to function: (combine_json_files) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def combine_json_files(directory) -> None:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'instruct.json':\n            instruct_combined_data = combined_data.copy()\n        combined_data = []\n    seen_inputs = set()\n    for item in instruct_combined_data:\n        if item['input'] in seen_inputs:\n            item['input'] = ''\n        else:\n            seen_inputs.add(item['input'])\n    cleaned_instruct_file_path = Path(directory) / 'cleaned_instruct.json'\n    write_file(instruct_combined_data, cleaned_instruct_file_path)",
        "output": "directory"
    },
    {
        "instruction": "Inputs to function: (create_code_graph) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'\n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_input' in edge:\n                    edge_data['target_input'] = edge['target_input']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_input' in edge[2] and edge[2]['target_input']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_input'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()",
        "output": "file_details, base_name, output_subdir"
    },
    {
        "instruction": "Inputs to function: (process_python_directories) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def process_python_directories(start_path: str, questions: Dict[str, Union[str, Dict]], use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config: dict=None) -> None:\n    \"\"\"\n    Processes all Python files in a given directory and its subdirectories.\n    Args:\n        start_path (str): The directory to start the search for Python files.\n        questions (Dict): The set of questions to answer about each Python \n            file.\n        use_llm (bool): Whether to use the LLM model to generate answers for\n            json.\n        output_dir (str): The directory where the output files should be\n            written. If not provided, the function writes the files to the\n            'python_json_and_yaml' directory in the current working directory.\n    \"\"\"\n    python_files = list(Path(start_path).rglob('[!_]*.py'))\n    for file_path in python_files:\n        logging.info(f'Processing: {file_path}')\n        relative_path = Path(file_path).relative_to(start_path)\n        base_name = '.'.join((part for part in relative_path.parts))\n        file_details = get_python_file_details(file_path)\n        if file_details is None or isinstance(file_details, tuple):\n            continue\n        qa_list, instruct_list = get_python_json(file_path, file_details, base_name, questions, use_llm, model_config)\n        if not qa_list:\n            continue\n        if output_dir is None:\n            output_dir = './datasets/'\n        output_subdir = Path(output_dir) / relative_path.parts[0]\n        output_subdir.mkdir(parents=True, exist_ok=True)\n        file_names = [f'{base_name}.qa.json', f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n        contents = [qa_list, instruct_list, file_details]\n        for file_name, content in zip(file_names, contents):\n            write_file(content, output_subdir / file_name)\n        if graph:\n            create_code_graph(file_details, base_name, output_subdir)\n    combine_json_files(output_dir)",
        "output": "graph, model_config, use_llm, output_dir, start_path, questions"
    },
    {
        "instruction": "Inputs to function: (py2dataset) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def py2dataset(start_path: str, use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config_path: str=None) -> None:\n    sys.setrecursionlimit(3000)\n    questions = read_file(Path('questions.json'))\n    model_config = None\n    if model_config_path:\n        model_config = read_file(Path(model_config_path))\n    process_python_directories(start_path, questions, use_llm, graph, output_dir, model_config)",
        "output": "graph, model_config_path, output_dir, use_llm, start_path"
    },
    {
        "instruction": "Docstring of function: (read_file) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "Reads a JSON or YAML file and returns its contents as a dictionary. Args: file_path Path: The path to the file. Returns: The contents of the file as a dictionary."
    },
    {
        "instruction": "Docstring of function: (write_file) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "Writes a dictionary to a JSON or YAML file. Args: data Dict: The data to write to the file. file_path Path: The path to the file."
    },
    {
        "instruction": "Docstring of function: (combine_json_files) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def combine_json_files(directory) -> None:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'instruct.json':\n            instruct_combined_data = combined_data.copy()\n        combined_data = []\n    seen_inputs = set()\n    for item in instruct_combined_data:\n        if item['input'] in seen_inputs:\n            item['input'] = ''\n        else:\n            seen_inputs.add(item['input'])\n    cleaned_instruct_file_path = Path(directory) / 'cleaned_instruct.json'\n    write_file(instruct_combined_data, cleaned_instruct_file_path)",
        "output": "Combine all JSON files in the output directory into qa.json and instruct.json, and then remove duplicates. Args: directory str: The directory where the output files are located."
    },
    {
        "instruction": "Docstring of function: (create_code_graph) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'\n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_input' in edge:\n                    edge_data['target_input'] = edge['target_input']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_input' in edge[2] and edge[2]['target_input']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_input'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()",
        "output": "Generate graphs from the file_details and save them as PNG images. Args: file_details dict: The details extracted from the Python file. base_name str: The base name of the output files. output_subdir Path: The subdirectory where the output files will be saved."
    },
    {
        "instruction": "Docstring of function: (process_python_directories) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def process_python_directories(start_path: str, questions: Dict[str, Union[str, Dict]], use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config: dict=None) -> None:\n    \"\"\"\n    Processes all Python files in a given directory and its subdirectories.\n    Args:\n        start_path (str): The directory to start the search for Python files.\n        questions (Dict): The set of questions to answer about each Python \n            file.\n        use_llm (bool): Whether to use the LLM model to generate answers for\n            json.\n        output_dir (str): The directory where the output files should be\n            written. If not provided, the function writes the files to the\n            'python_json_and_yaml' directory in the current working directory.\n    \"\"\"\n    python_files = list(Path(start_path).rglob('[!_]*.py'))\n    for file_path in python_files:\n        logging.info(f'Processing: {file_path}')\n        relative_path = Path(file_path).relative_to(start_path)\n        base_name = '.'.join((part for part in relative_path.parts))\n        file_details = get_python_file_details(file_path)\n        if file_details is None or isinstance(file_details, tuple):\n            continue\n        qa_list, instruct_list = get_python_json(file_path, file_details, base_name, questions, use_llm, model_config)\n        if not qa_list:\n            continue\n        if output_dir is None:\n            output_dir = './datasets/'\n        output_subdir = Path(output_dir) / relative_path.parts[0]\n        output_subdir.mkdir(parents=True, exist_ok=True)\n        file_names = [f'{base_name}.qa.json', f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n        contents = [qa_list, instruct_list, file_details]\n        for file_name, content in zip(file_names, contents):\n            write_file(content, output_subdir / file_name)\n        if graph:\n            create_code_graph(file_details, base_name, output_subdir)\n    combine_json_files(output_dir)",
        "output": "Processes all Python files in a given directory and its subdirectories. Args: start_path str: The directory to start the search for Python files. questions Dict: The set of questions to answer about each Python file. use_llm bool: Whether to use the LLM model to generate answers for json. output_dir str: The directory where the output files should be written. If not provided, the function writes the files to the python_json_and_yaml directory in the current working directory."
    },
    {
        "instruction": "Calls in function: (read_file) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "yaml.load, json.load, file_path.open"
    },
    {
        "instruction": "Calls in function: (write_file) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "yaml.dump, json.dump, file_path.open"
    },
    {
        "instruction": "Calls in function: (combine_json_files) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def combine_json_files(directory) -> None:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'instruct.json':\n            instruct_combined_data = combined_data.copy()\n        combined_data = []\n    seen_inputs = set()\n    for item in instruct_combined_data:\n        if item['input'] in seen_inputs:\n            item['input'] = ''\n        else:\n            seen_inputs.add(item['input'])\n    cleaned_instruct_file_path = Path(directory) / 'cleaned_instruct.json'\n    write_file(instruct_combined_data, cleaned_instruct_file_path)",
        "output": "combined_data.copy, write_file, ikeysfile_names.indexfile: i for i in combined_data.values, set, list, seen_inputs.add, Path, Pathdirectory.rglob, combined_data.extend, file_names.index, file_path.exists, read_file"
    },
    {
        "instruction": "Calls in function: (create_code_graph) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'\n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_input' in edge:\n                    edge_data['target_input'] = edge['target_input']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_input' in edge[2] and edge[2]['target_input']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_input'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()",
        "output": ", plt.close, nx.spring_layout, plt.figure, G.add_node, plt.savefig, nx.DiGraph, .join, n.join, G.edges, label.append, nx.draw, G.add_edge, nx.draw_networkx_edge_labels"
    },
    {
        "instruction": "Calls in function: (process_python_directories) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def process_python_directories(start_path: str, questions: Dict[str, Union[str, Dict]], use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config: dict=None) -> None:\n    \"\"\"\n    Processes all Python files in a given directory and its subdirectories.\n    Args:\n        start_path (str): The directory to start the search for Python files.\n        questions (Dict): The set of questions to answer about each Python \n            file.\n        use_llm (bool): Whether to use the LLM model to generate answers for\n            json.\n        output_dir (str): The directory where the output files should be\n            written. If not provided, the function writes the files to the\n            'python_json_and_yaml' directory in the current working directory.\n    \"\"\"\n    python_files = list(Path(start_path).rglob('[!_]*.py'))\n    for file_path in python_files:\n        logging.info(f'Processing: {file_path}')\n        relative_path = Path(file_path).relative_to(start_path)\n        base_name = '.'.join((part for part in relative_path.parts))\n        file_details = get_python_file_details(file_path)\n        if file_details is None or isinstance(file_details, tuple):\n            continue\n        qa_list, instruct_list = get_python_json(file_path, file_details, base_name, questions, use_llm, model_config)\n        if not qa_list:\n            continue\n        if output_dir is None:\n            output_dir = './datasets/'\n        output_subdir = Path(output_dir) / relative_path.parts[0]\n        output_subdir.mkdir(parents=True, exist_ok=True)\n        file_names = [f'{base_name}.qa.json', f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n        contents = [qa_list, instruct_list, file_details]\n        for file_name, content in zip(file_names, contents):\n            write_file(content, output_subdir / file_name)\n        if graph:\n            create_code_graph(file_details, base_name, output_subdir)\n    combine_json_files(output_dir)",
        "output": "zip, get_python_file_details, ..join, write_file, logging.info, Pathfile_path.relative_to, isinstance, output_subdir.mkdir, get_python_json, list, Path, Pathstart_path.rglob, combine_json_files, create_code_graph"
    },
    {
        "instruction": "Calls in function: (py2dataset) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def py2dataset(start_path: str, use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config_path: str=None) -> None:\n    sys.setrecursionlimit(3000)\n    questions = read_file(Path('questions.json'))\n    model_config = None\n    if model_config_path:\n        model_config = read_file(Path(model_config_path))\n    process_python_directories(start_path, questions, use_llm, graph, output_dir, model_config)",
        "output": "Path, process_python_directories, sys.setrecursionlimit, read_file"
    },
    {
        "instruction": "Variables in function: (read_file) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "file_type"
    },
    {
        "instruction": "Variables in function: (write_file) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def write_file(data: Dict, file_path: Path) -> None:\n    \"\"\"\n    Writes a dictionary to a JSON or YAML file. \n    Args:\n        data (Dict): The data to write to the file.\n        file_path (Path): The path to the file.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open('w') as f:\n        if file_type == 'json':\n            json.dump(data, f, indent=4)\n        elif file_type == 'yaml':\n            yaml.SafeDumper.ignore_aliases = lambda *args: True\n            yaml.dump(data, f, Dumper=yaml.SafeDumper, sort_keys=False)",
        "output": "file_type"
    },
    {
        "instruction": "Variables in function: (combine_json_files) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def combine_json_files(directory) -> None:\n    \"\"\"\n    Combine all JSON files in the output directory into 'qa.json' and \n    'instruct.json', and then remove duplicates.\n    Args:\n        directory (str): The directory where the output files are located.\n    \"\"\"\n    file_names = ['qa.json', 'instruct.json']\n    keys = ['question', 'instruction']\n    combined_data = []\n    for file in file_names:\n        file_path = Path(directory) / file\n        if file_path.exists():\n            combined_data = read_file(file_path)\n        for json_file in Path(directory).rglob(f'*.{file}'):\n            combined_data.extend(read_file(json_file))\n        combined_data = list({i[keys[file_names.index(file)]]: i for i in combined_data}.values())\n        write_file(combined_data, file_path)\n        if file == 'instruct.json':\n            instruct_combined_data = combined_data.copy()\n        combined_data = []\n    seen_inputs = set()\n    for item in instruct_combined_data:\n        if item['input'] in seen_inputs:\n            item['input'] = ''\n        else:\n            seen_inputs.add(item['input'])\n    cleaned_instruct_file_path = Path(directory) / 'cleaned_instruct.json'\n    write_file(instruct_combined_data, cleaned_instruct_file_path)",
        "output": "file_path, cleaned_instruct_file_path, instruct_combined_data, file_names, keys, seen_inputs, combined_data"
    },
    {
        "instruction": "Variables in function: (create_code_graph) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def create_code_graph(file_details: Dict, base_name: str, output_subdir: Path) -> None:\n    \"\"\"\n    Generate graphs from the file_details and save them as PNG images.\n    Args:\n        file_details (dict): The details extracted from the Python file.\n        base_name (str): The base name of the output files.\n        output_subdir (Path): The subdirectory where the output files will be\n            saved.\n    \"\"\"\n    for graph_type in ['internal_code_graph', 'entire_code_graph']:\n        output_file = output_subdir / f'{base_name}.{graph_type}.png'\n        G = nx.DiGraph()\n        for node_name in file_details['file_info'][graph_type]['nodes']:\n            G.add_node(node_name)\n        for edge in file_details['file_info'][graph_type]['edges']:\n            source = edge['source']\n            target = edge['target']\n            if source in G.nodes and target in G.nodes:\n                edge_data = {}\n                if 'target_input' in edge:\n                    edge_data['target_input'] = edge['target_input']\n                if 'target_returns' in edge:\n                    edge_data['target_returns'] = edge['target_returns']\n                G.add_edge(source, target, **edge_data)\n        plt.figure(figsize=(20, 20))\n        pos = nx.spring_layout(G)\n        nx.draw(G, pos, with_labels=True, font_weight='bold', font_size=8, node_shape='s', node_size=500, width=1, arrowsize=12)\n        edge_labels = {}\n        for edge in G.edges(data=True):\n            label = []\n            if 'target_input' in edge[2] and edge[2]['target_input']:\n                label.append(f\"Inputs: {', '.join(edge[2]['target_input'])}\")\n            if 'target_returns' in edge[2] and edge[2]['target_returns']:\n                label.append(f\"\\nReturns: {', '.join(edge[2]['target_returns'])}\")\n            edge_labels[edge[0], edge[1]] = '\\n'.join(label)\n        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=6)\n        plt.savefig(output_file)\n        plt.close()",
        "output": "G, pos, edge_data, output_file, edge_labels, label, target, source"
    },
    {
        "instruction": "Variables in function: (process_python_directories) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def process_python_directories(start_path: str, questions: Dict[str, Union[str, Dict]], use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config: dict=None) -> None:\n    \"\"\"\n    Processes all Python files in a given directory and its subdirectories.\n    Args:\n        start_path (str): The directory to start the search for Python files.\n        questions (Dict): The set of questions to answer about each Python \n            file.\n        use_llm (bool): Whether to use the LLM model to generate answers for\n            json.\n        output_dir (str): The directory where the output files should be\n            written. If not provided, the function writes the files to the\n            'python_json_and_yaml' directory in the current working directory.\n    \"\"\"\n    python_files = list(Path(start_path).rglob('[!_]*.py'))\n    for file_path in python_files:\n        logging.info(f'Processing: {file_path}')\n        relative_path = Path(file_path).relative_to(start_path)\n        base_name = '.'.join((part for part in relative_path.parts))\n        file_details = get_python_file_details(file_path)\n        if file_details is None or isinstance(file_details, tuple):\n            continue\n        qa_list, instruct_list = get_python_json(file_path, file_details, base_name, questions, use_llm, model_config)\n        if not qa_list:\n            continue\n        if output_dir is None:\n            output_dir = './datasets/'\n        output_subdir = Path(output_dir) / relative_path.parts[0]\n        output_subdir.mkdir(parents=True, exist_ok=True)\n        file_names = [f'{base_name}.qa.json', f'{base_name}.instruct.json', f'{base_name}.details.yaml']\n        contents = [qa_list, instruct_list, file_details]\n        for file_name, content in zip(file_names, contents):\n            write_file(content, output_subdir / file_name)\n        if graph:\n            create_code_graph(file_details, base_name, output_subdir)\n    combine_json_files(output_dir)",
        "output": "python_files, file_names, output_dir, output_subdir, file_details, contents, base_name, relative_path"
    },
    {
        "instruction": "Variables in function: (py2dataset) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def py2dataset(start_path: str, use_llm: bool=False, graph: bool=False, output_dir: str=None, model_config_path: str=None) -> None:\n    sys.setrecursionlimit(3000)\n    questions = read_file(Path('questions.json'))\n    model_config = None\n    if model_config_path:\n        model_config = read_file(Path(model_config_path))\n    process_python_directories(start_path, questions, use_llm, graph, output_dir, model_config)",
        "output": "model_config, questions"
    },
    {
        "instruction": "Returns from function: (read_file) in file: (py2dataset.py2dataset.py2dataset.py)?",
        "input": "def read_file(file_path: Path) -> Dict:\n    \"\"\"\n    Reads a JSON or YAML file and returns its contents as a dictionary.\n    Args:\n        file_path (Path): The path to the file.\n    Returns:\n        The contents of the file as a dictionary.\n    \"\"\"\n    file_type = file_path.suffix[1:]\n    with file_path.open() as f:\n        if file_type == 'json':\n            return json.load(f)\n        elif file_type == 'yaml':\n            return yaml.load(f)",
        "output": "json.loadf, yaml.loadf"
    }
]